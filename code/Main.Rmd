---
title: "Prediction of psychosis treatment outcomes using machine learning on a real-world dataset"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Necessary libraries
```{r, echo=TRUE, results="hide", warning=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(mice) # for imputing 
library(VIM) # for calculating vim scores 
library(ggplot2) # for graphs 
library(ggpubr) # for graphs 
library(kableExtra) # for tables 
library(dplyr)
library(tidyr)
library(sodium) # for encryption 
library(readr) # for reading data 
library(reticulate) # for using python in R
sklearn <- import("sklearn")
pandas <- import("pandas")
numpy <- import("numpy")
np <- import("numpy")
```

## Decrypting dataset 
```{r}
# reading the encrypted data (fill in the correct paths to the data and nonce)
cipher <- readBin("path_to_data.csv", what = "raw", n = file.info("path_to_data.csv")$size)
nonce <- readBin("path_to_nonce.bin", what = "raw", n = 24)

# fill in the password that is used to decrypt the file
password <- readline(prompt = "Enter password: ")
cat("\014") # clears console so password does not stay there 
key <- hash(charToRaw(password)) 

# decrypting data 
data_decrypted <- data_decrypt(cipher, key, nonce)
data <- unserialize(data_decrypted)
```


## Cleaning dataset 
The dataset has a lot of things that are messy or unstructured. I am going to rename, add and remove columns to improve this 
<br>

```{r}
# renamning variables to match with the rest of the dataset 
data <- data %>%
  rename(
    maandafgerondgegevensafname.1 = maandafgerondgegevensafname1,
    maandafgerondgegevensafname.2 = maandafgerondgegevensafname2,
    maandafgerondgegevensafname.3 = maandafgerondgegevensafname3,
    maandafgerondgegevensafname.4 = maandafgerondgegevensafname4,
    maandafgerondgegevensafname.5 = maandafgerondgegevensafname5,
    maandafgerondgegevensafname.6 = maandafgerondgegevensafname6,
    jaarafgerondgegevensafname.1 = jaarafgerondgegevensafname1,
    jaarafgerondgegevensafname.2 = jaarafgerondgegevensafname2,
    jaarafgerondgegevensafname.3 = jaarafgerondgegevensafname3,
    jaarafgerondgegevensafname.4 = jaarafgerondgegevensafname4,
    jaarafgerondgegevensafname.5 = jaarafgerondgegevensafname5,
    jaarafgerondgegevensafname.6 = jaarafgerondgegevensafname6
  )
```

### Correcting dataset using source data 
For some reason the columns "jaarafgerond", "jaaringepland", "maandafgerond" and "maandingepland" have data swapped between their six timepoints. I am going to access the original source data to import the columns where data is not swapped. 
<br>

```{r}
# reading the source data (fill in the correct paths to the source data and nonce)
cipher <- readBin("path_to_data.csv", what = "raw", n = file.info("path_to_data.csv")$size)
nonce <- readBin("path_to_nonce.bin", what = "raw", n = 24)

# fill in the password that is used to decrypt the file
cat("\014") # clears console 
key <- hash(charToRaw(password)) 

# decrypting the data 
data_decrypted <- data_decrypt(cipher, key, nonce)
dataOUD <- unserialize(data_decrypted)

# the column jaarafgerondgegevensafname1 does not have a dot in the original dataset but should have one in the new one, so i am going to rename the column to match the rest of the dataset 
dataOUD <- dataOUD %>%
  rename(jaarafgerondgegevensafname.1 = jaarafgerondgegevensafname1)
```
```{r}
# selecting only the columns we need from the old dataset 
dataOUD_selected <- dataOUD %>%
  select(Proefpersoonnummer, jaaringeplandgegevensafname.1:jaaringeplandgegevensafname.6, maandingeplandgegevensafname.1:maandingeplandgegevensafname.6, maandafgerondgegevensafname.1:maandafgerondgegevensafname.6, jaarafgerondgegevensafname.1:jaarafgerondgegevensafname.6)

# merging our dataset with the old dataset while keeping only matching participants (this ensures that participants that are not in the new dataset do not get added)
dataNIEUW <- data %>%
  left_join(dataOUD_selected, by = "Proefpersoonnummer") %>%
  mutate(
    jaaringeplandgegevensafname.1 = coalesce(jaaringeplandgegevensafname.1.y, jaaringeplandgegevensafname.1.x),
    jaaringeplandgegevensafname.2 = coalesce(jaaringeplandgegevensafname.2.y, jaaringeplandgegevensafname.2.x),
    jaaringeplandgegevensafname.3 = coalesce(jaaringeplandgegevensafname.3.y, jaaringeplandgegevensafname.3.x),
    jaaringeplandgegevensafname.4 = coalesce(jaaringeplandgegevensafname.4.y, jaaringeplandgegevensafname.4.x),
    jaaringeplandgegevensafname.5 = coalesce(jaaringeplandgegevensafname.5.y, jaaringeplandgegevensafname.5.x),
    jaaringeplandgegevensafname.6 = coalesce(jaaringeplandgegevensafname.6.y, jaaringeplandgegevensafname.6.x),
    maandingeplandgegevensafname.1 = coalesce(maandingeplandgegevensafname.1.y, maandingeplandgegevensafname.1.x),
    maandingeplandgegevensafname.2 = coalesce(maandingeplandgegevensafname.2.y, maandingeplandgegevensafname.2.x),
    maandingeplandgegevensafname.3 = coalesce(maandingeplandgegevensafname.3.y, maandingeplandgegevensafname.3.x),
    maandingeplandgegevensafname.4 = coalesce(maandingeplandgegevensafname.4.y, maandingeplandgegevensafname.4.x),
    maandingeplandgegevensafname.5 = coalesce(maandingeplandgegevensafname.5.y, maandingeplandgegevensafname.5.x),
    maandingeplandgegevensafname.6 = coalesce(maandingeplandgegevensafname.6.y, maandingeplandgegevensafname.6.x),
    maandafgerondgegevensafname.1 = coalesce(maandafgerondgegevensafname.1.y, maandafgerondgegevensafname.1.x),
    maandafgerondgegevensafname.2 = coalesce(maandafgerondgegevensafname.2.y, maandafgerondgegevensafname.2.x),
    maandafgerondgegevensafname.3 = coalesce(maandafgerondgegevensafname.3.y, maandafgerondgegevensafname.3.x),
    maandafgerondgegevensafname.4 = coalesce(maandafgerondgegevensafname.4.y, maandafgerondgegevensafname.4.x),
    maandafgerondgegevensafname.5 = coalesce(maandafgerondgegevensafname.5.y, maandafgerondgegevensafname.5.x),
    maandafgerondgegevensafname.6 = coalesce(maandafgerondgegevensafname.6.y, maandafgerondgegevensafname.6.x),
    jaarafgerondgegevensafname.1 = coalesce(jaarafgerondgegevensafname.1.y, jaarafgerondgegevensafname.1.x),
    jaarafgerondgegevensafname.2 = coalesce(jaarafgerondgegevensafname.2.y, jaarafgerondgegevensafname.2.x),
    jaarafgerondgegevensafname.3 = coalesce(jaarafgerondgegevensafname.3.y, jaarafgerondgegevensafname.3.x),
    jaarafgerondgegevensafname.4 = coalesce(jaarafgerondgegevensafname.4.y, jaarafgerondgegevensafname.4.x),
    jaarafgerondgegevensafname.5 = coalesce(jaarafgerondgegevensafname.5.y, jaarafgerondgegevensafname.5.x),
    jaarafgerondgegevensafname.6 = coalesce(jaarafgerondgegevensafname.6.y, jaarafgerondgegevensafname.6.x)
  ) %>%
  select(Proefpersoonnummer, everything(), -ends_with(".x"), -ends_with(".y"))  # removing the duplicate columns 
```

### Changing NA numbers to NA 
```{r}
# changing 999 values to NA (leaving out Proefpersoonnummer)
dataNIEUW <- dataNIEUW %>%
  mutate(across(-Proefpersoonnummer, ~ replace(.x, .x == 999, NA)))

# changing 99 values to NA (leaving out Proefpersoonnummer)
dataNIEUW <- dataNIEUW %>%
  mutate(across(-Proefpersoonnummer, ~ replace(.x, .x == 99, NA)))
```

### recoding binary variables

there are some binary variables for which 1 = yes, 2 = no. I want to change this to 1 = yes, 0 = no 
```{r}
dataNIEUW <- dataNIEUW %>%
  mutate_at(vars(geslacht_GegevensAfname, leefsituatie_steun.1, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1), ~ ifelse(. == 2, 0, .))
```

### recoding categorical variables 
#### leefsituatie 
we recode leefsituatie in "zelfstandig" (1, stays 1), and the rest 
```{r}
dataNIEUW$leefsituatie_nieuw <- ifelse(dataNIEUW$leefsituatie.1 == 1, 1, 0)
```

#### opleiding 
we are keeping all the categories, but we make it numeric to make it easier for the model   
```{r}
dataNIEUW$opleiding_nieuw <- ifelse(dataNIEUW$opleiding.1 == 10, NA, dataNIEUW$opleiding.1)
as.numeric(dataNIEUW$opleiding.1)
```

#### burgerlijke staat 
```{r}
dataNIEUW$burgerlijkestaat_nieuw <- ifelse(dataNIEUW$burgerlijkestaat.1 %in% c(2, 4), 1, 0)
```


### Adding columns for date finished, intervals between timepoints and months since first timepoint 
```{r}
library(lubridate)

# columns for date finished for every timepoint 
dataNIEUW$date_finished.1 <- make_date(year = dataNIEUW$jaarafgerondgegevensafname.1, 
                        month = dataNIEUW$maandafgerondgegevensafname.1, 
                        day = 1) 
dataNIEUW$date_finished.2 <- make_date(year = dataNIEUW$jaarafgerondgegevensafname.2, 
                        month = dataNIEUW$maandafgerondgegevensafname.2, 
                        day = 1) 
dataNIEUW$date_finished.3 <- make_date(year = dataNIEUW$jaarafgerondgegevensafname.3, 
                        month = dataNIEUW$maandafgerondgegevensafname.3, 
                        day = 1) 
dataNIEUW$date_finished.4 <- make_date(year = dataNIEUW$jaarafgerondgegevensafname.4, 
                        month = dataNIEUW$maandafgerondgegevensafname.4, 
                        day = 1) 
dataNIEUW$date_finished.5 <- make_date(year = dataNIEUW$jaarafgerondgegevensafname.5, 
                        month = dataNIEUW$maandafgerondgegevensafname.5, 
                        day = 1) 
dataNIEUW$date_finished.6 <- make_date(year = dataNIEUW$jaarafgerondgegevensafname.6, 
                        month = dataNIEUW$maandafgerondgegevensafname.6, 
                        day = 1) 

# columns for intervals between timestamps 
dataNIEUW$month_diff_1_and_2 <- time_length(interval(dataNIEUW$date_finished.1, dataNIEUW$date_finished.2), "months")
dataNIEUW$month_diff_2_and_3 <- time_length(interval(dataNIEUW$date_finished.2, dataNIEUW$date_finished.3), "months")
dataNIEUW$month_diff_3_and_4 <- time_length(interval(dataNIEUW$date_finished.3, dataNIEUW$date_finished.4), "months")
dataNIEUW$month_diff_4_and_5 <- time_length(interval(dataNIEUW$date_finished.4, dataNIEUW$date_finished.5), "months")
dataNIEUW$month_diff_5_and_6 <- time_length(interval(dataNIEUW$date_finished.5, dataNIEUW$date_finished.6), "months")

# columns for months since first timepoint 
dataNIEUW$month_diff_1_and_3 <- dataNIEUW$month_diff_1_and_2 + dataNIEUW$month_diff_2_and_3
dataNIEUW$month_diff_1_and_4 <- dataNIEUW$month_diff_1_and_2 + dataNIEUW$month_diff_2_and_3 + dataNIEUW$month_diff_3_and_4
dataNIEUW$month_diff_1_and_5 <- dataNIEUW$month_diff_1_and_2 + dataNIEUW$month_diff_2_and_3 + dataNIEUW$month_diff_3_and_4 + dataNIEUW$month_diff_4_and_5
dataNIEUW$month_diff_1_and_6 <- dataNIEUW$month_diff_1_and_2 + dataNIEUW$month_diff_2_and_3 + dataNIEUW$month_diff_3_and_4 + dataNIEUW$month_diff_4_and_5 + dataNIEUW$month_diff_5_and_6

# removing the unnecesarry columns 
dataNIEUW <- dataNIEUW %>% select(-c(Date_finished_1, Date_finished_2, Date_finished_3, Date_finished_4, Date_finished_5, Date_finished_6, Diff_finished_1_2_Months, Diff_finished_2_3_Months, Diff_finished_3_4_Months, Diff_finished_4_5_Months, Diff_finished_5_6_Months)) 
```

### Correcting Leeftijd1eGGZ & Leeftijd1ePsyKl 
There are two problems with Leeftijd1eGGZ & Leeftijd1ePsyKl that need to be fixed:  
1. Both columns should only contain age values, but there are some birthyears in the columns too. They do not impact the data so we can remove them  
2. In both columns, for all six timepoints, sometimes there are different answers given for the same participant. This should not be posssible so I choose to first take the most noted answer (modus) and when this is an equal share I take the average of all the answers who are noted the most  

```{r}
# selecting all columns that need to be altered 
selected_columns_1eGGZ <- c("Leeftijd1eGGZ_b.1", "Leeftijd1eGGZ_b.2", "Leeftijd1eGGZ_b.3", "Leeftijd1eGGZ_b.4", "Leeftijd1eGGZ_b.5", "Leeftijd1eGGZ_b.6")
selected_columns_1ePsyKl <- c("Leeftijd1ePsyKl_b.1", "Leeftijd1ePsyKl_b.2", "Leeftijd1ePsyKl_b.3", "Leeftijd1ePsyKl_b.4", "Leeftijd1ePsyKl_b.5", "Leeftijd1ePsyKl_b.6")

# removing values above 1900 as they are birthyears and not realistic 
dataNIEUW <- dataNIEUW %>%
  mutate(across(all_of(selected_columns_1eGGZ), ~ ifelse(. > 1900, NA, .)))

dataNIEUW <- dataNIEUW %>%
  mutate(across(all_of(selected_columns_1ePsyKl), ~ ifelse(. > 1900, NA, .)))

# function that will calculate the modus and when equal the average of the most answered 
modusmean <- function(x) {
  # we do not want to count NA's 
  x <- na.omit(x)

  # checking if the vector is empty because then answer should return NA 
  if (length(x) == 0) {
    return(NA)  
  }
  
  # calculating frequencies of values 
  freq_table <- table(x)
  max_freq <- max(freq_table)
  
  modes <- as.numeric(names(freq_table[freq_table == max_freq]))
  # if there's only one mode, we return it
  if (length(modes) == 1) {
    return(modes)
  } else {
    # if there are multiple modes, we return the mean of them
    return(mean(modes))
  }
}

# applying the function to all columns 
dataNIEUW$modusmeanGGZ <- apply(dataNIEUW[, selected_columns_1eGGZ], 1, modusmean)
dataNIEUW$modusmeanPsyKl <- apply(dataNIEUW[, selected_columns_1ePsyKl], 1, modusmean)
```

### Calculating outcomes 

#### Brief Inspire-O 
We want to have a column with the total score of all 5 brief inspire-o questions. We can do this by adding the scores of the 5 questions together and then multiplying it by 4. We also want a binary score which will give 1 if the score is 55 or higher and 0 if it is lower than 55.

```{r}
# We do this calculations for all six timepoints 
dataNIEUW$Inspire_totaal.1 = (dataNIEUW$IHS_01.1 + dataNIEUW$IHS_02.1 + dataNIEUW$IHS_03.1 + dataNIEUW$IHS_04.1 + dataNIEUW$IHS_05.1) * 5
dataNIEUW$Inspire_totaal.2 = (dataNIEUW$IHS_01.2 + dataNIEUW$IHS_02.2 + dataNIEUW$IHS_03.2 + dataNIEUW$IHS_04.2 + dataNIEUW$IHS_05.2) * 5
dataNIEUW$Inspire_totaal.3 = (dataNIEUW$IHS_01.3 + dataNIEUW$IHS_02.3 + dataNIEUW$IHS_03.3 + dataNIEUW$IHS_04.3 + dataNIEUW$IHS_05.3) * 5
dataNIEUW$Inspire_totaal.4 = (dataNIEUW$IHS_01.4 + dataNIEUW$IHS_02.4 + dataNIEUW$IHS_03.4 + dataNIEUW$IHS_04.4 + dataNIEUW$IHS_05.4) * 5
dataNIEUW$Inspire_totaal.5 = (dataNIEUW$IHS_01.5 + dataNIEUW$IHS_02.5 + dataNIEUW$IHS_03.5 + dataNIEUW$IHS_04.5 + dataNIEUW$IHS_05.5) * 5
dataNIEUW$Inspire_totaal.6 = (dataNIEUW$IHS_01.6 + dataNIEUW$IHS_02.6 + dataNIEUW$IHS_03.6 + dataNIEUW$IHS_04.6 + dataNIEUW$IHS_05.6) * 5



dataNIEUW$Inspire_binair.1 <- ifelse(
  is.na(dataNIEUW$Inspire_totaal.1), NA,  # If any value is NA, set NA
  ifelse(dataNIEUW$Inspire_totaal.1 > 54, 1, 0)  # If value is 55 or higher, set value to 1. Otherwise 0
)

dataNIEUW$Inspire_binair.2 <- ifelse(
  is.na(dataNIEUW$Inspire_totaal.2), NA,  # If any value is NA, set NA
  ifelse(dataNIEUW$Inspire_totaal.2 > 54, 1, 0)  # If value is 55 or higher, set value to 1. Otherwise 0
)

dataNIEUW$Inspire_binair.3 <- ifelse(
  is.na(dataNIEUW$Inspire_totaal.3), NA,  # If any value is NA, set NA
  ifelse(dataNIEUW$Inspire_totaal.3 > 54, 1, 0)  # If value is 55 or higher, set value to 1. Otherwise 0
)

dataNIEUW$Inspire_binair.4 <- ifelse(
  is.na(dataNIEUW$Inspire_totaal.4), NA,  # If any value is NA, set NA
  ifelse(dataNIEUW$Inspire_totaal.4 > 54, 1, 0)  # If value is 55 or higher, set value to 1. Otherwise 0
)

dataNIEUW$Inspire_binair.5 <- ifelse(
  is.na(dataNIEUW$Inspire_totaal.5), NA,  # If any value is NA, set NA
  ifelse(dataNIEUW$Inspire_totaal.5 > 54, 1, 0)  # If value is 55 or higher, set value to 1. Otherwise 0
)

dataNIEUW$Inspire_binair.6 <- ifelse(
  is.na(dataNIEUW$Inspire_totaal.6), NA,  # If any value is NA, set NA
  ifelse(dataNIEUW$Inspire_totaal.6 > 54, 1, 0)  # If value is 55 or higher, set value to 1. Otherwise 0
)



```

#### FR 
Functional Remission scale:
- 
- binary outcome, 1 if all three questions have answer "0", 0 in all other cases 
```{r}
dataNIEUW$FR_totaal.1 <- ifelse(
  apply(dataNIEUW[, c("FR_1.1", "FR_2.1", "FR_3.1")], 1, anyNA), 
  NA, 
  rowSums(dataNIEUW[, c("FR_1.1", "FR_2.1", "FR_3.1")], na.rm = TRUE)
)

dataNIEUW$FR_totaal.2 <- ifelse(
  apply(dataNIEUW[, c("FR_1.2", "FR_2.2", "FR_3.2")], 1, anyNA), 
  NA, 
  rowSums(dataNIEUW[, c("FR_1.2", "FR_2.2", "FR_3.2")], na.rm = TRUE)
)


dataNIEUW$FR_binair.1 <- ifelse(
  is.na(dataNIEUW$FR_totaal.1), NA,  # If FR_totaal.1 is NA, set FR_binair.1 to NA
  ifelse(dataNIEUW$FR_totaal.1 == 0, 1, 0)  # If FR_totaal.1 == 0, set 1, else 0
)


dataNIEUW$FR_binair.2 <- ifelse(
  is.na(dataNIEUW$FR_totaal.2), NA,  # If FR_totaal.1 is NA, set FR_binair.1 to NA
  ifelse(dataNIEUW$FR_totaal.2 == 0, 1, 0)  # If FR_totaal.1 == 0, set 1, else 0
)


```

#### MANSA 
De mansa wordt berekend door de mean van alle ingevulde vragen te berekenen en die te vermenigvuldigen met het aantal vragen (12). Als een participant meer dan 2 vragen niet heeft ingevuld krijgt de totaalscore waarde NA. 
```{r}

# mansa timepoint 1 
mansa_vars.1 <- c("MANSA_PH_1.1", "MANSA_PH_2.1", "MANSA_PH_3.1", "MANSA_PH_4.1", 
                                   "MANSA_PH_5.1", "MANSA_PH_6.1", "MANSA_PH_8.1", "MANSA_PH_12.1", 
                                   "MANSA_PH_13.1", "MANSA_PH_14.1", "MANSA_PH_15.1", "MANSA_PH_16.1")
dataNIEUW[mansa_vars.1] <- lapply(dataNIEUW[mansa_vars.1], as.numeric)

mansa_missing.1 <- rowSums(is.na(dataNIEUW[, mansa_vars.1]))
mansa_gem.1 <- rowMeans(dataNIEUW[, mansa_vars.1], na.rm = TRUE)

mansa_totaal.1 <- mansa_gem.1 * 12
mansa_totaal.1[mansa_missing.1 > 2] <- NA

dataNIEUW$mansa_totaal.1 <- mansa_totaal.1

# mansa timepoint 2 
mansa_vars.2 <- c("MANSA_PH_1.2", "MANSA_PH_2.2", "MANSA_PH_3.2", "MANSA_PH_4.2", 
                                   "MANSA_PH_5.2", "MANSA_PH_6.2", "MANSA_PH_8.2", "MANSA_PH_12.2", 
                                   "MANSA_PH_13.2", "MANSA_PH_14.2", "MANSA_PH_15.2", "MANSA_PH_16.2")
dataNIEUW[mansa_vars.2] <- lapply(dataNIEUW[mansa_vars.2], as.numeric)

mansa_missing.2 <- rowSums(is.na(dataNIEUW[, mansa_vars.2]))
mansa_gem.2 <- rowMeans(dataNIEUW[, mansa_vars.2], na.rm = TRUE)

mansa_totaal.2 <- mansa_gem.2 * 12
mansa_totaal.2[mansa_missing.2 > 2] <- NA

dataNIEUW$mansa_totaal.2 <- mansa_totaal.2
```

#### HONOS 
```{r}
# honos timepoint 1 
honos_vars.1 <- c("HONOS_add_01.1", "HONOS_add_02.1", "HONOS_add_03.1",
                  "HONOS_add_04.1", "HONOS_add_05.1", "HONOS_add_06.1",
                  "HONOS_add_07.1", "HONOS_add_08b.1", "HONOS_add_09.1",
                  "HONOS_add_10.1", "HONOS_add_11.1", "HONOS_add_12.1")

dataNIEUW[honos_vars.1] <- lapply(dataNIEUW[honos_vars.1], as.numeric)

honos_missing.1 <- rowSums(is.na(dataNIEUW[, honos_vars.1]))
honos_gem.1 <- rowMeans(dataNIEUW[, honos_vars.1], na.rm = TRUE)
honos_totaal.1 <- honos_gem.1 * 12
honos_totaal.1[honos_missing.1 > 2] <- NA

dataNIEUW$honos_totaal.1 <- honos_totaal.1

# honos timepoint 2 
honos_vars.2 <- c("HONOS_add_01.2", "HONOS_add_02.2", "HONOS_add_03.2",
                  "HONOS_add_04.2", "HONOS_add_05.2", "HONOS_add_06.2",
                  "HONOS_add_07.2", "HONOS_add_08b.2", "HONOS_add_09.2",
                  "HONOS_add_10.2", "HONOS_add_11.2", "HONOS_add_12.2")

dataNIEUW[honos_vars.2] <- lapply(dataNIEUW[honos_vars.2], as.numeric)

honos_missing.2 <- rowSums(is.na(dataNIEUW[, honos_vars.2]))
honos_gem.2 <- rowMeans(dataNIEUW[, honos_vars.2], na.rm = TRUE)
honos_totaal.2 <- honos_gem.2 * 12
honos_totaal.2[honos_missing.2 > 2] <- NA

dataNIEUW$honos_totaal.2 <- honos_totaal.2

```


### recoding variable types 
```{r}
# most variables do not have the right type. I am going to change this for the variables that i am going to use: 
numerics <- c("Age", "geslacht_GegevensAfname",
                  "modusmeanGGZ", "modusmeanPsyKl", "Inspire_totaal.1", "Inspire_totaal.2", "mansa_totaal.1", "mansa_totaal.2",
                  "month_diff_1_and_2", "opleiding_nieuw", "MANSA_PH_1.1", "MANSA_PH_2.1", "MANSA_PH_3.1", "MANSA_PH_4.1", "MANSA_PH_5.1",
                  "MANSA_PH_6.1", "MANSA_PH_8.1", "MANSA_PH_12.1", "MANSA_PH_13.1", "MANSA_PH_14.1", "MANSA_PH_15.1")

categoricals <- c("burgerlijkestaat.1", "leefsituatie.1")

binarys <- c("leefsituatie_steun.1", "levenspartner.1", "Inspire_binair.1", "Inspire_binair.2", "Leeftijd1eGGZ_a.1", "Leeftijd1ePsyKl_a.1", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1", "betaaldwerk.1", "leefsituatie_nieuw", "burgerlijkestaat_nieuw")

ordinals <- c("HONOS_add_01.1", "HONOS_add_02.1", "HONOS_add_03.1", "HONOS_add_04.1",
                  "HONOS_add_05.1", "HONOS_add_06.1", "HONOS_add_07.1", "HONOS_add_08a.1",
                  "HONOS_add_08b.1", "HONOS_add_09.1", "HONOS_add_10.1", "HONOS_add_11.1",
                  "HONOS_add_12.1", "HONOS_add_13.1", "HONOS_add_14.1", "HONOS_add_15.1",
                  "IHS_01.1", "IHS_02.1", "IHS_03.1", "IHS_04.1", "IHS_05.1",
                  "FR_1.1", "FR_2.1", "FR_3.1", "FR_1.2", "FR_2.2", "FR_3.2",
                  "IHS_01.2", "IHS_02.2", "IHS_03.2", "IHS_04.2", "IHS_05.2")

dataNIEUW[numerics] <- lapply(dataNIEUW[numerics], as.numeric)
dataNIEUW[categoricals] <- lapply(dataNIEUW[categoricals], as.factor)
dataNIEUW[binarys] <- lapply(dataNIEUW[binarys], as.factor)
dataNIEUW[ordinals] <- lapply(dataNIEUW[ordinals], as.ordered)
```


## Visualizing data 

### Distributions 
#### distribution of Brief Inspire-O binary 

```{r}
ggplot(dataNIEUW) +
  geom_bar(aes(x = Inspire_binair.1),  
           fill = "#69b3a2",   
           color = "black",   
           alpha = 0.8) +    
  labs(
    title = "Distribution of Brief Inspire-O binary score (baseline)",
    x = "Brief Inspire-O binary score",
    y = "Count"
  ) +
  theme_minimal(base_size = 14) +   
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  
    axis.text = element_text(color = "black"),  
    panel.grid.major = element_line(color = "gray80")  
  )


```
#### distribution of Brief Inspire-O binary at timepoint 2


```{r}
ggplot(dataNIEUW) +
  geom_bar(aes(x = Inspire_binair.2),  
           fill = "#69b3a2",   
           color = "black",   
           alpha = 0.8) +    
  labs(
    title = "Distribution of Brief Inspire-O binary score (timepoint 2)",
    x = "Brief Inspire-O binary score",
    y = "Count"
  ) +
  theme_minimal(base_size = 14) +   
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  
    axis.text = element_text(color = "black"),  
    panel.grid.major = element_line(color = "gray80")  
  )


```

#### Distribution of Brief Inspire-O total score at timepoint 1
```{r}
ggplot(dataNIEUW) +
  geom_histogram(aes(x = Inspire_totaal.1), 
                 binwidth = 4, 
                 fill = "#69b3a2",   
                 color = "black",   
                 alpha = 0.8) +    
  labs(
    title = "Distribution of Brief Inspire-O total score (timepoint 1)",
    x = "Brief inspire-O total score",
    y = "Count"
  ) +
  theme_minimal(base_size = 14) +   
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  
    axis.text = element_text(color = "black"),  
    panel.grid.major = element_line(color = "gray80")  
  )

```

#### Distribution of Brief Inspire-O total score at timepoint 2
```{r}
ggplot(dataNIEUW) +
  geom_histogram(aes(x = Inspire_totaal.2), 
                 binwidth = 4, 
                 fill = "#69b3a2",   
                 color = "black",   
                 alpha = 0.8) +    
  labs(
    title = "Distribution of Brief Inspire-O total score (timepoint 2)",
    x = "Brief inspire-O total score",
    y = "Count"
  ) +
  theme_minimal(base_size = 14) +   
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  
    axis.text = element_text(color = "black"),  
    panel.grid.major = element_line(color = "gray80")  
  )
```

#### Distribution of time between timepoint 1 and 2

```{r}
library(ggplot2)
library(tidyr)
library(dplyr)

ggplot(dataNIEUW) +
  geom_histogram(aes(x = month_diff_1_and_2), 
                 binwidth = 1, 
                 fill = "#69b3a2",   
                 color = "black",   
                 alpha = 0.8) +    
  labs(
    title = "Distribution of time between timepoint 1 and 2",
    x = "Months",
    y = "Count"
  ) +
  scale_x_continuous(breaks = seq(0, max(dataNIEUW$month_diff_1_and_2, na.rm = TRUE), 1)) +  
  theme_minimal(base_size = 14) +   
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  
    axis.text = element_text(color = "black"),  
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major = element_line(color = "gray80")  
  )



```
### timepoints    
#### Timeline of measurements in months 

```{r}
# selecting columns and pivoting to long format
data_long <- dataNIEUW %>%
  select(Proefpersoonnummer, Age, month_diff_1_and_2, month_diff_1_and_3, 
         month_diff_1_and_4, month_diff_1_and_5, month_diff_1_and_6) %>%
  pivot_longer(cols = starts_with("month_diff"), names_to = "Measurement", values_to = "Months") %>%
  drop_na() 

# adding the first timepoint (month 0) manually for all patients
measurement_0 <- dataNIEUW %>%
  select(Proefpersoonnummer, Age) %>%
  mutate(Months = 0, Measurement = "month_diff_1_and_1")

data_long <- bind_rows(data_long, measurement_0)

# plotting the timeline 
ggplot(data_long, aes(x = Months, y = Proefpersoonnummer, group = Proefpersoonnummer)) +
  geom_line(color = "blue", alpha = 0.5, size = 0.5) + 
  geom_point(color = "black", size = 1) +  
  labs(
    x = "Months Since First Measurement", 
    y = "Patient ID", 
    title = "Timeline of Patient Measurements"
  ) +
  theme_minimal()

```

#### Timeline of measurements in months sorted by length between first and second timepoint 
```{r}
# re-ordering patients on lenght between first and second timepoint
patient_order <- dataNIEUW %>%
  arrange(desc(month_diff_1_and_2)) %>%
  pull(Proefpersoonnummer)
data_long$Proefpersoonnummer <- factor(data_long$Proefpersoonnummer, levels = patient_order)


# plotting the timeline
ggplot(data_long, aes(x = Months, y = Proefpersoonnummer, group = Proefpersoonnummer)) +
  geom_line(color = "blue", alpha = 0.7, size = 0.4) +  
  geom_point(color = "black", size = 0.8) + 
  geom_vline(xintercept = 9, color = "black", linetype = "dashed", size = 0.6) + 
  geom_vline(xintercept = 15, color = "black", linetype = "dashed", size = 0.6) + 
  scale_x_continuous(
    breaks = c(0, 9, 15, 20, 40, 60)
  ) +
  labs(
    x = "Months Since First Measurement", 
    y = "Patients", 
    title = "Timeline of Patient Measurements"
  ) +
  theme_minimal(base_size = 12) + 
  coord_cartesian(ylim = c(0, 520)) +
  theme(
    axis.title.y = element_text(margin = margin(r = -43))  # <-- This moves the y-axis label to the right
  )


```

```{r}
dataNIEUW %>%
  select(Age, month_diff_1_and_2) %>%
  filter(month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15) %>%
  summarise(
    mean_age = mean(Age, na.rm = TRUE),
    sd_age = sd(Age, na.rm = TRUE),
    min_age = min(Age, na.rm = TRUE),
    max_age = max(Age, na.rm = TRUE)
  )

```


### Correlations 
#### Correlation between inspire_totaal.1 & inspire_binair.2 

```{r}
ggplot(dataNIEUW, aes(x = Inspire_totaal.1, y = Inspire_binair.2)) +
  geom_jitter(aes(color = factor(Inspire_binair.2)), width = 0.1, height = 0.1) +
  labs(title = "Scatter Plot of Inspire_totaal.1 vs Inspire_binair.2",
       x = "Inspire_totaal.1 (Total Score at Timepoint 1)",
       y = "Inspire_binair.2 (Binary Outcome at Timepoint 2)") +
  theme_minimal()

```

We can train a logistic model to check dependence of inspire_totaal.1 on Inspire_binair.2 

```{r}
logistic_model <- glm(Inspire_binair.2 ~ Inspire_totaal.1, data = dataNIEUW, family = binomial())
summary(logistic_model)

```

As we can see, p < 0.05 and thus significant. Inspire_totaal.1 has a significant effect on Inspire_binair.2. 

#### Correlation between Inspire_totaal.1 and Inspire_totaal.2 

```{r}
ggplot(dataNIEUW, aes(x = Inspire_totaal.1, y = Inspire_totaal.2)) +
  geom_point() +
  labs(title = "Scatter Plot of Inspire_totaal.1 vs Inspire_totaal.2",
       x = "Inspire_totaal.1",
       y = "Inspire_totaal.2") +
  theme_minimal() +
  geom_point(alpha = 0.5) +  # Transparency to avoid overplotting
  geom_smooth(method = "lm", color = "red", se = FALSE)
```


```{r}
library(ggcorrplot)

cor_matrix <- cor(dataNIEUW[, c('Inspire_totaal.1', 'Inspire_totaal.2')], use = "complete.obs")
ggcorrplot(cor_matrix, method = "circle")
```

Here we see a small correlation between Inspire_totaal.1 and Inspire_totaal

```{r}
cor(dataNIEUW$Inspire_totaal.1, dataNIEUW$Inspire_totaal.2, use = "complete.obs")
```

#### Correlation between mansa_totaal.1 and mansa_totaal.2 
```{r}
ggplot(dataNIEUW, aes(x = mansa_totaal.1, y = mansa_totaal.2)) +
  geom_point() +
  labs(title = "Scatter Plot of mansa_totaal.1 vs mansa_totaal.2",
       x = "mansa_totaal.1",
       y = "mansa_totaal.2") +
  theme_minimal() +
  geom_point(alpha = 0.5) +  # Transparency to avoid overplotting
  geom_smooth(method = "lm", color = "red", se = FALSE)

```


```{r}
library(ggcorrplot)

cor_matrix <- cor(dataNIEUW[, c('mansa_totaal.1', 'mansa_totaal.2')], use = "complete.obs")
ggcorrplot(cor_matrix, method = "circle")
```

Here we see a small correlation between mansa_totaal.1 and mansa_totaal.2

```{r}
cor(dataNIEUW$mansa_totaal.1, dataNIEUW$mansa_totaal.2, use = "complete.obs")
```

#### Correlation between honos_totaal.1 and honos_totaal.2 
```{r}
ggplot(dataNIEUW, aes(x = honos_totaal.1, y = honos_totaal.2)) +
  geom_point() +
  labs(title = "Scatter Plot of honos_totaal.1 vs honos_totaal.2",
       x = "honos_totaal.1",
       y = "honos_totaal.2") +
  theme_minimal() +
  geom_point(alpha = 0.5) +  # Transparency to avoid overplotting
  geom_smooth(method = "lm", color = "red", se = FALSE)

```


```{r}
library(ggcorrplot)

cor_matrix <- cor(dataNIEUW[, c('honos_totaal.1', 'honos_totaal.2')], use = "complete.obs")
ggcorrplot(cor_matrix, method = "circle")
```

Here we see a small correlation between mansa_totaal.1 and mansa_totaal.2

```{r}
cor(dataNIEUW$honos_totaal.1, dataNIEUW$honos_totaal.2, use = "complete.obs")
```


## Building the models
### overview of possible predictors 

```{r}
# selecting possible columns that i am going to use for test model 
# I filter the data to only include non-missing values of outcome Inspire_binair.2 and to only include patients with their second timepoint between 9 and 15 months after their first timepoint 
# first i filter to only include patients with their second timepoint between 9 and 15 months: 
dataPossiblePredictors <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, Leeftijd1eGGZ_a.1, Leeftijd1ePsyKl_a.1, modusmeanGGZ, modusmeanPsyKl, burgerlijkestaat.1, leefsituatie.1, leefsituatie_steun.1, levenspartner.1, opleiding.1, betaaldwerk.1, Inspire_totaal.1, Inspire_totaal.2, Inspire_binair.1, Inspire_binair.2, month_diff_1_and_2, month_diff_1_and_3, HONOS_add_01.1, HONOS_add_02.1, HONOS_add_03.1, HONOS_add_04.1, HONOS_add_05.1, HONOS_add_06.1, HONOS_add_07.1, HONOS_add_08a.1, HONOS_add_08b.1, HONOS_add_09.1, HONOS_add_10.1, HONOS_add_11.1, HONOS_add_12.1, HONOS_add_13.1, HONOS_add_14.1, HONOS_add_15.1, MANSA_PH_1.1, MANSA_PH_2.1, MANSA_PH_3.1, MANSA_PH_4.1, MANSA_PH_5.1, MANSA_PH_6.1, MANSA_PH_7.1, MANSA_PH_8.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, MANSA_PH_12.1, MANSA_PH_13.1, MANSA_PH_14.1, MANSA_PH_15.1, IHS_01.1, IHS_02.1, IHS_03.1, IHS_04.1, IHS_05.1, FR_1.1, FR_2.1, FR_3.1, FR_1.2, FR_2.2, FR_3.2, FR_totaal.1, FR_binair.1, FR_totaal.2, FR_binair.2, IHS_01.2, IHS_02.2, IHS_03.2, IHS_04.2, IHS_05.2, vrijwilligerswerk.1, vrijwilligerswerk_2_.1, herstelHV_1.1, herstelHV_2.1, herstelHV_3.1, HerstelHV_4.1, herstelHV_4_2.1, herstelHV_5.1, HerstelHV_4_10_.1, Betrouwbaarheid_1_.1) %>% filter(month_diff_1_and_2 > 8 & month_diff_1_and_2 < 16)
print("amount of patients after filtering for timepoint:")
count(dataPossiblePredictors)

# second i filter to leave out patients with NA in outcome
dataPossiblePredictors <- dataPossiblePredictors %>% filter(!is.na(Inspire_totaal.2))
print("amount of patients after filtering for NA in outcome:")
count(dataPossiblePredictors)
```

```{r}
# checking what percentage of each variable is missing 
missing_percentage <- colSums(is.na(dataPossiblePredictors)) / nrow(dataPossiblePredictors) * 100
print(missing_percentage)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataPossiblePredictors))
cat("Total missing values: ", total_missing, "\n")
summary(dataPossiblePredictors)

md.pattern(dataPossiblePredictors)
```

to be honest this might not be better, it is pure chaos 

### model 1: Inspire_binair.2 as outcome and Inspire_totaal.1 (+ demographics) as predictor 

```{r}
# for the first model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, Inspire_totaal.1, Mansa_ph_10.1
# and the following outcome: Inpsire_binair.2
# I filter the data to only include non-missing values of outcome Inspire_binair.2 and to only include patients with their follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModel1 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, MANSA_PH_10.1, Inspire_totaal.1, Inspire_binair.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(Inspire_binair.2))
```

#### visualizing missing data 

```{r}
missing_percentageM1 <- colSums(is.na(dataModel1)) / nrow(dataModel1) * 100
print(missing_percentageM1)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel1))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel1)
```


#### imputing data 
```{r}
methods <- make.method(dataModel1)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["Inspire_totaal.1"] <- "pmm" # numeric 

# doing the imputation
imputed_dataModel1 <- mice(dataModel1, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel1)
```





```{r}
dataModel1_complete <- complete(imputed_dataModel1,1)
```

```{r}
original_data <- dataModel1$Inspire_totaal.1
imputed_data <- complete(imputed_dataModel1, 1)$Inspire_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X <- dataModel1_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "Inspire_totaal.1", "MANSA_PH_10.1")]
y <- dataModel1_complete$Inspire_binair.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
 
y <- as.integer(as.character(y))
```

#### baseline model 
```{r}
DummyClassifier <- sklearn$dummy$DummyClassifier
dummy_clf <- DummyClassifier(strategy = "most_frequent")

dummy_clf$fit(X, y)

accuracy <- dummy_clf$score(X, y)
print(accuracy)
```



#### svm using cross validation 

```{r}

scalerSVM <- sklearn$preprocessing$StandardScaler()
cross_val_predict <- sklearn$model_selection$cross_val_predict
metrics <- sklearn$metrics

# scaling X for svm 
X_scaledSVM <- scalerSVM$fit_transform(X) 

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svm1 <- sklearn$svm$SVC(C=C_val, kernel='linear', probability=TRUE)
  y_pred_svm1 <- cross_val_predict(svm1, X_scaledSVM, y, cv=5L)
  
  conf_matrix <- metrics$confusion_matrix(y, y_pred_svm1)
  tn <- conf_matrix[1, 1]  
  fp <- conf_matrix[1, 2]  
  fn <- conf_matrix[2, 1] 
  tp <- conf_matrix[2, 2] 
  
  accuracy <- (tp + tn) / sum(conf_matrix)
  precision <- tp / (tp + fp)  
  sensitivity <- tp / (tp + fn)  
  specificity <- tn / (tn + fp) 
  f1_score <- metrics$f1_score(y, y_pred_svm1)
  cat("SVM model with C =", C_val, "\n")
  cat("Confusion Matrix: \n")
  print(conf_matrix)
  cat("Accuracy:", accuracy, "\n")
  cat("Precision:", precision, "\n")
  cat("Sensitivity (Recall for positive class):", sensitivity, "\n")
  cat("Specificity (Recall for negative class):", specificity, "\n")
  cat("F1-Score:", f1_score, "\n")
  cat("\n")
  
}



```

#### knn using cross validation 
```{r}
KNeighborsClassifier <- sklearn$neighbors$KNeighborsClassifier
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
scalerSVM <- sklearn$preprocessing$StandardScaler()
set.seed(42)


scalerKNN <- MinMaxScaler()  
# scalerKNN <- sklearn$preprocessing$StandardScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)  


for (n in c(3, 5, 10, 30, 100)) {
  knn1 <- KNeighborsClassifier(n_neighbors = as.integer(n))
  y_pred_knn1 <- cross_val_predict(knn1, X_scaledKNN, y, cv=5L)

  conf_matrix <- metrics$confusion_matrix(y, y_pred_knn1)
  tn <- conf_matrix[1, 1] 
  fp <- conf_matrix[1, 2]  
  fn <- conf_matrix[2, 1] 
  tp <- conf_matrix[2, 2]  

  accuracy <- (tp + tn) / sum(conf_matrix)
  sensitivity <- tp / (tp + fn) 
  specificity <- tn / (tn + fp)  
  precision <- tp / (tp + fp)  
  f1_score <- metrics$f1_score(y, y_pred_knn1)

  cat("KNN model with n =", n, "\n")
  print(conf_matrix)
  cat("Accuracy:", accuracy, "\n")
  cat("Sensitivity:", sensitivity, "\n")
  cat("Specificity:", specificity, "\n")
  cat("Precision:", precision, "\n")
  cat("F1-Score:", f1_score, "\n")
  cat("\n")
}

# One thing i do not really understand is that when i scale the data, i get worse results. Here are the unscaled results: 
cat("unscaled results", "\n")
for (n in c(3, 5, 10, 30, 100)) {
  knn1 <- KNeighborsClassifier(n_neighbors = as.integer(n))
  y_pred_knn1 <- cross_val_predict(knn1, X, y, cv=5L)

  conf_matrix <- metrics$confusion_matrix(y, y_pred_knn1)
  tn <- conf_matrix[1, 1] 
  fp <- conf_matrix[1, 2]  
  fn <- conf_matrix[2, 1] 
  tp <- conf_matrix[2, 2]  

  accuracy <- (tp + tn) / sum(conf_matrix)
  sensitivity <- tp / (tp + fn) 
  specificity <- tn / (tn + fp)  
  precision <- tp / (tp + fp)  
  f1_score <- metrics$f1_score(y, y_pred_knn1)

  cat("KNN model with n =", n, "\n")
  print(conf_matrix)
  cat("Accuracy:", accuracy, "\n")
  cat("Sensitivity:", sensitivity, "\n")
  cat("Specificity:", specificity, "\n")
  cat("Precision:", precision, "\n")
  cat("F1-Score:", f1_score, "\n")
  cat("\n")
}

```

### model 1b: Inspire_binair.2 as outcome without Inspire_totaal.1 (+ demographics) as predictor 

```{r}
# for the second model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, Mansa_ph_10.1
# and the following outcome: Inpsire_binair.2
# I filter the data to only include non-missing values of outcome Inspire_binair.2 and to only include patients with their follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModel1b <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, MANSA_PH_10.1, Inspire_binair.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(Inspire_binair.2))
```

#### visualizing missing data 

```{r}
missing_percentageM1b <- colSums(is.na(dataModel1b)) / nrow(dataModel1b) * 100
print(missing_percentageM1b)
```

imputatation is not needed as no predictors have missing values 

#### Selecting features and outcome 
```{r}
X <- dataModel1b[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "MANSA_PH_10.1")]
y <- dataModel1b$Inspire_binair.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
 
y <- as.integer(as.character(y))
```

#### baseline model 
```{r}
DummyClassifier <- sklearn$dummy$DummyClassifier
dummy_clf <- DummyClassifier(strategy = "most_frequent")

dummy_clf$fit(X, y)

accuracy <- dummy_clf$score(X, y)
print(accuracy)
```



#### svm using cross validation 

```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
cross_val_predict <- sklearn$model_selection$cross_val_predict
metrics <- sklearn$metrics

# scaling X for svm 
X_scaledSVM <- scalerSVM$fit_transform(X) 

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svm1b <- sklearn$svm$SVC(C=C_val, kernel='linear', probability=TRUE)
  y_pred_svm1b <- cross_val_predict(svm1b, X_scaledSVM, y, cv=5L)
  
  conf_matrix <- metrics$confusion_matrix(y, y_pred_svm1b)
  tn <- conf_matrix[1, 1]  
  fp <- conf_matrix[1, 2]  
  fn <- conf_matrix[2, 1] 
  tp <- conf_matrix[2, 2] 
  
  accuracy <- (tp + tn) / sum(conf_matrix)
  precision <- tp / (tp + fp)  
  sensitivity <- tp / (tp + fn)  
  specificity <- tn / (tn + fp) 
  f1_score <- metrics$f1_score(y, y_pred_svm1b)
  cat("SVM model with C =", C_val, "\n")
  cat("Confusion Matrix: \n")
  print(conf_matrix)
  cat("Accuracy:", accuracy, "\n")
  cat("Precision:", precision, "\n")
  cat("Sensitivity (Recall for positive class):", sensitivity, "\n")
  cat("Specificity (Recall for negative class):", specificity, "\n")
  cat("F1-Score:", f1_score, "\n")
  cat("\n")
}



```

#### knn using cross validation 
```{r}
KNeighborsClassifier <- sklearn$neighbors$KNeighborsClassifier
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
scalerSVM <- sklearn$preprocessing$StandardScaler()
set.seed(42)


scalerKNN <- MinMaxScaler()  
# scalerKNN <- sklearn$preprocessing$StandardScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)  


for (n in c(3, 5, 10, 30, 100)) {
  knn1b <- KNeighborsClassifier(n_neighbors = as.integer(n))
  y_pred_knn1b <- cross_val_predict(knn1b, X_scaledKNN, y, cv=5L)

  conf_matrix <- metrics$confusion_matrix(y, y_pred_knn1b)
  tn <- conf_matrix[1, 1] 
  fp <- conf_matrix[1, 2]  
  fn <- conf_matrix[2, 1] 
  tp <- conf_matrix[2, 2]  

  accuracy <- (tp + tn) / sum(conf_matrix)
  sensitivity <- tp / (tp + fn) 
  specificity <- tn / (tn + fp)  
  precision <- tp / (tp + fp)  
  f1_score <- metrics$f1_score(y, y_pred_knn1b)

  cat("KNN model with n =", n, "\n")
  print(conf_matrix)
  cat("Accuracy:", accuracy, "\n")
  cat("Sensitivity:", sensitivity, "\n")
  cat("Specificity:", specificity, "\n")
  cat("Precision:", precision, "\n")
  cat("F1-Score:", f1_score, "\n")
  cat("\n")
}
```


### model 2: Inspire_totaal.2 as outcome and Inspire_totaal.1 (+ demographics) as predictor 
```{r}
# for model 2 I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, Inspire_totaal.1, Mansa_ph_10.1
# and the following outcome: Inpsire_totaal.2
# I filter the data to only include non-missing values of outcome Inspire_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel2 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, MANSA_PH_10.1, Inspire_totaal.1, Inspire_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(Inspire_totaal.2))
```

```{r}
# the following is to keep track of the metrics 
model2_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model2_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```


#### visualizing missing data 

```{r}
missing_percentageM2 <- colSums(is.na(dataModel2)) / nrow(dataModel2) * 100
print(missing_percentageM2)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel2))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel2)
```

#### imputing data 
```{r}
methods <- make.method(dataModel2)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["Inspire_totaal.1"] <- "pmm" # numeric 

# doing the imputation
imputed_dataModel2 <- mice(dataModel2, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel2)
```

```{r}
dataModel2_complete <- complete(imputed_dataModel2,1)
```

```{r}
original_data <- dataModel2$Inspire_totaal.1
imputed_data <- complete(imputed_dataModel2, 1)$Inspire_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X <- dataModel2_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "Inspire_totaal.1", "MANSA_PH_10.1")]
y <- dataModel2_complete$Inspire_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
library(car)
vif(lm(Inspire_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1 + Inspire_totaal.1 + MANSA_PH_10.1, data = dataModel2_complete))
```

#### baseline model that uses Inspire_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel2_complete[, c("Inspire_totaal.1")]  # Use only Inspire_totaal.1 as predictor
y_baseline <- dataModel2_complete$Inspire_totaal.2  # The outcome is Inspire_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean_baseline)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using Inspire_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model2_results <- rbind(model2_results, data.frame(
  Model = "2",
  Method = "baseline",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model2_results_normalized <- rbind(model2_results_normalized, data.frame(
  Model = "2",
  Method = "baseline",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline / r2_mean_baseline,
  R2_std = "-"
))


```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr2 <- SVR(C=C_val, kernel='linear')
  
  mse_scores <- cross_val_score(svr2, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr2, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr2, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model2_results <- rbind(model2_results, data.frame(
    Model = "2",
    Method = "SVR",
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model2_results_normalized <- rbind(model2_results_normalized, data.frame(
    Model = "2",
    Method = "SVR",
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
    
}

```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn2 <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn2, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn2, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn2, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model2_results <- rbind(model2_results, data.frame(
    Model = "2",
    Method = "KNR",
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model2_results_normalized <- rbind(model2_results_normalized, data.frame(
    Model = "2",
    Method = "KNR",
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```






### model 2b: Inspire_totaal.2 as outcome without Inpire_totaal.1 as predictor 
```{r}
# for model 2b I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, Mansa_ph_10.1
# and the following outcome: Inpsire_totaal.2
# I filter the data to only include non-missing values of outcome Inspire_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModel2b <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, MANSA_PH_10.1, Inspire_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(Inspire_totaal.2))
```

#### visualizing missing data 

```{r}
missing_percentageM2b <- colSums(is.na(dataModel2b)) / nrow(dataModel2b) * 100
print(missing_percentageM2b)
```

no missing data in predictors, so no imputation needed 

#### Selecting features and outcome 
```{r}
X <- dataModel2b[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "MANSA_PH_10.1")]
y <- dataModel2b$Inspire_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr2b <- SVR(C=C_val, kernel='linear')
  
  mse_scores <- cross_val_score(svr2b, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr2b, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr2b, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model2_results <- rbind(model2_results, data.frame(
    Model = "2b",
    Method = "SVR",
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model2_results_normalized <- rbind(model2_results_normalized, data.frame(
    Model = "2b",
    Method = "SVR",
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn2b <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn2b, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn2b, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn2b, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model2_results <- rbind(model2_results, data.frame(
  Model = "2b",
  Method = "KNR",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("neighbors = ", n),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))

model2_results_normalized <- rbind(model2_results_normalized, data.frame(
  Model = "2b",
  Method = "KNR",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("neighbors = ", n),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))
}

```


### model 3: mansa_totaal.2 as outcome and mansa_totaal.1 (+ demographics) as predictor 
```{r}
# for the third model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:
View(dataModel3)

dataModel3 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, mansa_totaal.2, month_diff_1_and_2, month_diff_1_and_3, month_diff_1_and_4, month_diff_1_and_5, month_diff_1_and_6) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
model3_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model3_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM3 <- colSums(is.na(dataModel3)) / nrow(dataModel3) * 100
print(missing_percentageM3)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel3))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel3)
```

#### imputing data 
```{r}
methods <- make.method(dataModel3)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["mansa_totaal.1"] <- "pmm" # numeric 

# doing the imputation
imputed_dataModel3 <- mice(dataModel3, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel3)
```

```{r}
dataModel3_complete <- complete(imputed_dataModel3,1)
```

```{r}
original_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel3)),
  value = c(dataModel3$mansa_totaal.1, dataModel3$levenspartner.1, dataModel3$betaaldwerk.1),
  type = "Original"
)

imputed_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel3)),
  value = c(complete(imputed_dataModel3, 1)$mansa_totaal.1, 
            complete(imputed_dataModel3, 1)$levenspartner.1, 
            complete(imputed_dataModel3, 1)$betaaldwerk.1),
  type = "Imputed"
)

plot_data <- rbind(original_data, imputed_data)

ggplot(plot_data, aes(x = value, color = type, linetype = type)) +
  geom_density() +
  facet_wrap(~variable, scales = "free") + 
  labs(title = "Density Plot of Original (Dashed) vs. Imputed (Solid) Data",
       x = "Value",
       y = "Density",
       color = "Data Type") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

```

#### Selecting features and outcome 
```{r}
X <- dataModel3_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1")]
y <- dataModel3_complete$mansa_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(mansa_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1 + mansa_totaal.1, data = dataModel3_complete))
```

#### baseline model that uses mansa_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel3_complete[, c("mansa_totaal.1")]  
y_baseline <- dataModel3_complete$mansa_totaal.2  

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model3_results <- rbind(model3_results, data.frame(
  Model = "3",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model3_results_normalized <- rbind(model3_results_normalized, data.frame(
  Model = "3",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline / r2_mean_baseline,
  R2_std = "-"
))
```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr3 <- SVR(C=C_val, kernel='linear')
  
  mse_scores <- cross_val_score(svr3, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr3, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr3, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model3_results <- rbind(model3_results, data.frame(
    Model = "3",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model3_results_normalized <- rbind(model3_results_normalized, data.frame(
    Model = "3",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}


```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn3 <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn3, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn3, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn3, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model3_results <- rbind(model3_results, data.frame(
    Model = "3",
    Method = "KNR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model3_results_normalized <- rbind(model3_results_normalized, data.frame(
    Model = "3",
    Method = "KNR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```






### model 3b: mansa_totaal.2 as outcome without mansa_totaal.1 as predictor 
```{r}
# for model 3b I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModel3b <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.2, month_diff_1_and_2, month_diff_1_and_3, month_diff_1_and_4, month_diff_1_and_5, month_diff_1_and_6) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 

```


#### visualizing missing data 

```{r}
missing_percentageM3b <- colSums(is.na(dataModel3b)) / nrow(dataModel3b) * 100
print(missing_percentageM3b)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel3b))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel3b)
```

#### imputing data 
```{r}
methods <- make.method(dataModel3b)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 

# doing the imputation
imputed_dataModel3b <- mice(dataModel3b, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel3b)
```

```{r}
dataModel3b_complete <- complete(imputed_dataModel3b,1)
```

```{r}
original_data <- data.frame(
  variable = rep(c("levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel3b)),
  value = c(dataModel3b$mansa_totaal.1, dataModel3b$levenspartner.1, dataModel3b$betaaldwerk.1),
  type = "Original"
)

imputed_data <- data.frame(
  variable = rep(c("levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel3b)),
  value = c(complete(imputed_dataModel3b, 1)$mansa_totaal.1, 
            complete(imputed_dataModel3b, 1)$levenspartner.1, 
            complete(imputed_dataModel3b, 1)$betaaldwerk.1),
  type = "Imputed"
)

plot_data <- rbind(original_data, imputed_data)

ggplot(plot_data, aes(x = value, color = type, linetype = type)) +
  geom_density() +
  facet_wrap(~variable, scales = "free") + 
  labs(title = "Density Plot of Original (Dashed) vs. Imputed (Solid) Data",
       x = "Value",
       y = "Density",
       color = "Data Type") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

```

#### Selecting features and outcome 
```{r}
X <- dataModel3b_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1")]
y <- dataModel3b_complete$mansa_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr3b <- SVR(C=C_val, kernel='linear')
  
  mse_scores <- cross_val_score(svr3b, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr3b, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr3b, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model3_results <- rbind(model3_results, data.frame(
    Model = "3b",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model3_results_normalized <- rbind(model3_results_normalized, data.frame(
    Model = "3b",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn3b <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn3b, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn3b, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn3b, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model3_results <- rbind(model3_results, data.frame(
    Model = "3b",
    Method = "KNR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model3_results_normalized <- rbind(model3_results_normalized, data.frame(
    Model = "3b",
    Method = "KNR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```






### model 4: honos_totaal.2 as outcome with honos_totaal.1 as predictor  
```{r}
# for the fourth model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, honos_totaal.1
# and the following outcome: honos_totaal.2
# I filter the data to only include non-missing values of outcome honos_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModel4 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, honos_totaal.1, honos_totaal.2, month_diff_1_and_2, month_diff_1_and_3, month_diff_1_and_4, month_diff_1_and_5, month_diff_1_and_6) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(honos_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
model4_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model4_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM4 <- colSums(is.na(dataModel4)) / nrow(dataModel4) * 100
print(missing_percentageM4)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel4))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel4)
```

#### imputing data 
```{r}
methods <- make.method(dataModel4)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 

# doing the imputation
imputed_dataModel4 <- mice(dataModel4, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel4)
```

```{r}
dataModel4_complete <- complete(imputed_dataModel4,1)
```

```{r}
original_data <- data.frame(
  variable = rep(c("honos_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel4)),
  value = c(dataModel4$honos_totaal.1, dataModel4$levenspartner.1, dataModel4$betaaldwerk.1),
  type = "Original"
)

imputed_data <- data.frame(
  variable = rep(c("honos_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel4)),
  value = c(complete(imputed_dataModel4, 1)$honos_totaal.1, 
            complete(imputed_dataModel4, 1)$levenspartner.1, 
            complete(imputed_dataModel4, 1)$betaaldwerk.1),
  type = "Imputed"
)

plot_data <- rbind(original_data, imputed_data)

ggplot(plot_data, aes(x = value, color = type, linetype = type)) +
  geom_density() +
  facet_wrap(~variable, scales = "free") + 
  labs(title = "Density Plot of Original (Dashed) vs. Imputed (Solid) Data",
       x = "Value",
       y = "Density",
       color = "Data Type") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

```

#### Selecting features and outcome 
```{r}
X <- dataModel4_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "honos_totaal.1")]
y <- dataModel4_complete$honos_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(honos_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1 + honos_totaal.1, data = dataModel4_complete))
```

#### baseline model that uses honos_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel4_complete[, c("honos_totaal.1")]  # Use only Inspire_totaal.1 as predictor
y_baseline <- dataModel4_complete$honos_totaal.2  # The outcome is Inspire_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using honos_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model4_results <- rbind(model4_results, data.frame(
  Model = "4",
  Method = "baseline",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model4_results_normalized <- rbind(model4_results_normalized, data.frame(
  Model = "4",
  Method = "baseline",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline / r2_mean_baseline,
  R2_std = "-"
))
```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr4 <- SVR(C=C_val, kernel='linear')
  
  mse_scores <- cross_val_score(svr4, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr4, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr4, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model4_results <- rbind(model4_results, data.frame(
    Model = "4",
    Method = "SVR",
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model4_results_normalized <- rbind(model4_results_normalized, data.frame(
    Model = "4",
    Method = "SVR",
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn4 <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn4, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn4, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn4, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model4_results <- rbind(model4_results, data.frame(
    Model = "4",
    Method = "KNR",
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model4_results_normalized <- rbind(model4_results_normalized, data.frame(
    Model = "4",
    Method = "KNR",
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```



### model 4b: honos_totaal.2 as outcome without honos_totaal.1 as predictor  
```{r}
# for model 4b I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk
# and the following outcome: honos_totaal.2
# I filter the data to only include non-missing values of outcome honos_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModel4b <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, honos_totaal.2, month_diff_1_and_2, month_diff_1_and_3, month_diff_1_and_4, month_diff_1_and_5, month_diff_1_and_6) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(honos_totaal.2)) 

```


#### visualizing missing data 

```{r}
missing_percentageM4b <- colSums(is.na(dataModel4b)) / nrow(dataModel4b) * 100
print(missing_percentageM4b)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel4b))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel4b)
```

#### imputing data 
```{r}
methods <- make.method(dataModel4b)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 

# doing the imputation
imputed_dataModel4b <- mice(dataModel4b, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel4b)
```

```{r}
dataModel4b_complete <- complete(imputed_dataModel4b,1)
```

```{r}
original_data <- data.frame(
  variable = rep(c("levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel4b)),
  value = c(dataModel4b$honos_totaal.1, dataModel4b$levenspartner.1, dataModel4b$betaaldwerk.1),
  type = "Original"
)

imputed_data <- data.frame(
  variable = rep(c("levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel4b)),
  value = c(complete(imputed_dataModel4b, 1)$honos_totaal.1, 
            complete(imputed_dataModel4b, 1)$levenspartner.1, 
            complete(imputed_dataModel4b, 1)$betaaldwerk.1),
  type = "Imputed"
)

plot_data <- rbind(original_data, imputed_data)

ggplot(plot_data, aes(x = value, color = type, linetype = type)) +
  geom_density() +
  facet_wrap(~variable, scales = "free") + 
  labs(title = "Density Plot of Original (Dashed) vs. Imputed (Solid) Data",
       x = "Value",
       y = "Density",
       color = "Data Type") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

```

#### Selecting features and outcome 
```{r}
X <- dataModel4b_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1")]
y <- dataModel4b_complete$honos_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(honos_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1, data = dataModel4b_complete))
```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr4b <- SVR(C=C_val, kernel='linear')
  
  mse_scores <- cross_val_score(svr4b, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr4b, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr4b, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model4_results <- rbind(model4_results, data.frame(
    Model = "4b",
    Method = "SVR",
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model4_results_normalized <- rbind(model4_results_normalized, data.frame(
    Model = "4b",
    Method = "SVR",
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn4b <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn4b, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn4b, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn4b, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model4_results <- rbind(model4_results, data.frame(
    Model = "4b",
    Method = "KNR",
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model4_results_normalized <- rbind(model4_results_normalized, data.frame(
    Model = "4b",
    Method = "KNR",
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```



### model 5: FR_totaal.2 as outcome and FR_totaal.1 as predictor
```{r}
# for the fifth model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, FR_totaal.1
# and the following outcome: FR_totaal.2
# I filter the data to only include non-missing values of outcome FR_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModel5 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, FR_totaal.1, FR_totaal.2, month_diff_1_and_2, month_diff_1_and_3, month_diff_1_and_4, month_diff_1_and_5, month_diff_1_and_6) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(FR_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
model5_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model5_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM5 <- colSums(is.na(dataModel5)) / nrow(dataModel5) * 100
print(missing_percentageM5)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel5))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel5)
```

#### imputing data 
```{r}
methods <- make.method(dataModel5)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["FR_totaal.1"] <- "pmm" # numeric 

# doing the imputation
imputed_dataModel5 <- mice(dataModel5, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel5)
```

```{r}
dataModel5_complete <- complete(imputed_dataModel5,1)
```

```{r}
original_data <- data.frame(
  variable = rep(c("FR_totaal.1"), each = nrow(dataModel5)),
  value = c(dataModel5$FR_totaal.1),
  type = "Original"
)

imputed_data <- data.frame(
  variable = rep(c("FR_totaal.1"), each = nrow(dataModel5)),
  value = c(complete(imputed_dataModel5, 1)$FR_totaal.1),
  type = "Imputed"
)

plot_data <- rbind(original_data, imputed_data)

ggplot(plot_data, aes(x = value, color = type, linetype = type)) +
  geom_density() +
  facet_wrap(~variable, scales = "free") + 
  labs(title = "Density Plot of Original (Dashed) vs. Imputed (Solid) Data",
       x = "Value",
       y = "Density",
       color = "Data Type") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

```

#### Selecting features and outcome 
```{r}
X <- dataModel5_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "FR_totaal.1")]
y <- dataModel5_complete$FR_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(FR_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1 + FR_totaal.1, data = dataModel5_complete))
```

#### baseline model that uses FR_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel5_complete[, c("FR_totaal.1")]  # Use only Inspire_totaal.1 as predictor
y_baseline <- dataModel5_complete$FR_totaal.2  # The outcome is Inspire_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using FR_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model5_results <- rbind(model5_results, data.frame(
  Model = "5",
  Method = "baseline",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model5_results_normalized <- rbind(model5_results_normalized, data.frame(
  Model = "5",
  Method = "baseline",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline / r2_mean_baseline,
  R2_std = "-"
))
```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr5 <- SVR(C=C_val, kernel='linear')
  
  mse_scores <- cross_val_score(svr5, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr5, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr5, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model5_results <- rbind(model5_results, data.frame(
    Model = "5",
    Method = "SVR",
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model5_results_normalized <- rbind(model5_results_normalized, data.frame(
    Model = "5",
    Method = "SVR",
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn5 <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn5, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn5, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn5, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model5_results <- rbind(model5_results, data.frame(
    Model = "5",
    Method = "KNR",
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model5_results_normalized <- rbind(model5_results_normalized, data.frame(
    Model = "5",
    Method = "KNR",
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```



### model 5b: honos_totaal.2 as outcome without honos_totaal.1 as predictor  
```{r}
# for model 5b I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk
# and the following outcome: FR_totaal.2
# I filter the data to only include non-missing values of outcome FR_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModel5b <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, FR_totaal.2, month_diff_1_and_2, month_diff_1_and_3, month_diff_1_and_4, month_diff_1_and_5, month_diff_1_and_6) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(FR_totaal.2)) 

```


#### visualizing missing data 

```{r}
missing_percentageM5b <- colSums(is.na(dataModel5b)) / nrow(dataModel5b) * 100
print(missing_percentageM5b)
```

#### Selecting features and outcome 
```{r}
X <- dataModel5b[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1")]
y <- dataModel5b$FR_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(FR_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1, data = dataModel5b))
```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr5b <- SVR(C=C_val, kernel='linear')
  
  mse_scores <- cross_val_score(svr5b, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr5b, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr5b, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model5_results <- rbind(model5_results, data.frame(
    Model = "5b",
    Method = "SVR",
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model5_results_normalized <- rbind(model5_results_normalized, data.frame(
    Model = "5b",
    Method = "SVR",
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn5b <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn5b, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn5b, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn5b, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model5_results <- rbind(model5_results, data.frame(
    Model = "5b",
    Method = "KNR",
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model5_results_normalized <- rbind(model5_results_normalized, data.frame(
    Model = "5b",
    Method = "KNR",
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```



### model 6: mansa_totaal.2 as outcome with mansa_totaal.1, honos_totaal.1, Inspire_totaal.1 and FR_totaal.1 as predictors
```{r}
# for the sixth model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModel6 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, mansa_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
model6_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model6_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM6 <- colSums(is.na(dataModel6)) / nrow(dataModel6) * 100
print(missing_percentageM6)
```

Inspire_totaal.1 heeft veel missende waardes, wil ik die wel meenemen? 

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel6))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel6)
```

#### imputing data 
```{r}
methods <- make.method(dataModel6)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel6 <- mice(dataModel6, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel6)
```

```{r}
dataModel6_complete <- complete(imputed_dataModel6,1)
```

```{r}
original_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "FR_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel6)),
  value = c(dataModel6$mansa_totaal.1, dataModel6$honos_totaal.1, dataModel6$Inspire_totaal.1, dataModel6$FR_totaal.1, dataModel6$levenspartner.1, dataModel6$betaaldwerk.1),
  type = "Original"
)

imputed_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "FR_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel6)),
  value = c(complete(imputed_dataModel6, 1)$mansa_totaal.1, 
            complete(imputed_dataModel6, 1)$honos_totaal.1, 
            complete(imputed_dataModel6, 1)$Inspire_totaal.1,
            complete(imputed_dataModel6, 1)$FR_totaal.1,
            complete(imputed_dataModel6, 1)$levenspartner.1, 
            complete(imputed_dataModel6, 1)$betaaldwerk.1),
  type = "Imputed"
)

plot_data <- rbind(original_data, imputed_data)

ggplot(plot_data, aes(x = value, color = type, linetype = type)) +
  geom_density() +
  facet_wrap(~variable, scales = "free") + 
  labs(title = "Density Plot of Original (Dashed) vs. Imputed (Solid) Data",
       x = "Value",
       y = "Density",
       color = "Data Type") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

```

#### Selecting features and outcome 
```{r}
X <- dataModel6_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "FR_totaal.1")]
y <- dataModel6_complete$mansa_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(mansa_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1 + mansa_totaal.1 + honos_totaal.1 + Inspire_totaal.1 + FR_totaal.1, data = dataModel6_complete))
```

#### baseline model that uses mansa_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel6_complete[, c("mansa_totaal.1")]  # Use only mansa_totaal.1 as predictor
y_baseline <- dataModel6_complete$mansa_totaal.2  # The outcome is mansa_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model6_results <- rbind(model6_results, data.frame(
  Model = "6",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model6_results_normalized <- rbind(model6_results_normalized, data.frame(
  Model = "6",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline / r2_mean_baseline,
  R2_std = "-"
))
```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr6 <- SVR(C=C_val, kernel='linear')
  
  svr6$fit(X_scaledSVM, y)
  y_pred <- svr6$predict(X_scaledSVM)

  results_df <- data.frame(
    Actual = y,
    Predicted = y_pred
  )

  # creating a plot that shows predicted versus actual values 
  plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +  # Ideal line (y = x)
    labs(
      title = paste("SVR Model Predictions vs Actual Outcomes\nC =", C_val),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(plot)
  
  mse_scores <- cross_val_score(svr6, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr6, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr6, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model6_results <- rbind(model6_results, data.frame(
    Model = "6",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model6_results_normalized <- rbind(model6_results_normalized, data.frame(
    Model = "6",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn6 <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn6, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn6, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn6, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model6_results <- rbind(model6_results, data.frame(
    Model = "6",
    Method = "KNR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model6_results_normalized <- rbind(model6_results_normalized, data.frame(
    Model = "6",
    Method = "KNR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### Nested cross validation
```{r}
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results <- data.frame()
best_params_list <- list()
outer_scores <- c()

# parameter grid
c_values <- c(0.01, 0.1, 1, 10, 30)

# outer CV loop
for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]
  
  set.seed(42 + outer_fold)  
  n_train_samples <- length(train_idx)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  
  for (c_val in c_values) {
    cv_scores <- c()

    # inner loop 
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]
      
      if (inner_fold == 1) {  
        cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
            "rows, y:", length(y_inner_train), "elements\n")
        cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
            "rows, y:", length(y_inner_val), "elements\n")
      }
      
      inner_svr <- SVR(C = c_val, kernel = 'linear')
      inner_svr$fit(X_inner_train, y_inner_train)
      
      y_pred_inner <- inner_svr$predict(X_inner_val)
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
      cv_scores <- c(cv_scores, neg_mse)
    }
    
    mean_score <- mean(cv_scores)
    
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
    }
  }
  
  cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
  
  best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
  
  best_svr <- SVR(C = best_c, kernel = 'linear')
  best_svr$fit(X_train_outer, y_train_outer)
  
  y_pred_outer <- best_svr$predict(X_test_outer)
  
  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
  
  outer_scores <- c(outer_scores, r2)
  
  nested_cv_results <- rbind(nested_cv_results, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results:\n")
print(nested_cv_results)
cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")

cat("\nBest Parameters per Fold:\n")
for (i in 1:length(best_params_list)) {
  cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# after the nested CV is complete, we evaluate the final model with cross_val_score
rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

final_svr <- SVR(C = most_common_c, kernel = 'linear')

mse_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model6_results <- rbind(model6_results, data.frame(
  Model = "6",
  Method = "SVR_nested_CV",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model6_results_normalized <- rbind(model6_results_normalized, data.frame(
  Model = "6",
  Method = "SVR_nested_CV",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


final_svr$fit(X_scaledSVM, y)
final_y_pred <- final_svr$predict(X_scaledSVM)

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)
```






### model 7: mansa_totaal.2 as output and mansa_totaal.1, honos_totaal.1 and FR_totaal.1 as predictors
```{r}
# for the seventh model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1, honos_totaal.1, FR_totaal.1. I removed Inspire_totaal.1 because it has a large amount of missing values 
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel7 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, FR_totaal.1, mansa_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
model7_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model7_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM7 <- colSums(is.na(dataModel7)) / nrow(dataModel7) * 100
print(missing_percentageM7)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel7))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel7)
```

#### imputing data 
```{r}
methods <- make.method(dataModel7)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel7 <- mice(dataModel7, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel7)
```

```{r}
dataModel7_complete <- complete(imputed_dataModel7,1)
```

```{r}
original_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "honos_totaal.1", "FR_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel7)),
  value = c(dataModel7$mansa_totaal.1, dataModel7$honos_totaal.1, dataModel7$FR_totaal.1, dataModel7$levenspartner.1, dataModel7$betaaldwerk.1),
  type = "Original"
)

imputed_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "honos_totaal.1", "FR_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModel7)),
  value = c(complete(imputed_dataModel7, 1)$mansa_totaal.1, 
            complete(imputed_dataModel7, 1)$honos_totaal.1, 
            complete(imputed_dataModel7, 1)$FR_totaal.1,
            complete(imputed_dataModel7, 1)$levenspartner.1, 
            complete(imputed_dataModel7, 1)$betaaldwerk.1),
  type = "Imputed"
)

plot_data <- rbind(original_data, imputed_data)

ggplot(plot_data, aes(x = value, color = type, linetype = type)) +
  geom_density() +
  facet_wrap(~variable, scales = "free") + 
  labs(title = "Density Plot of Original (Dashed) vs. Imputed (Solid) Data",
       x = "Value",
       y = "Density",
       color = "Data Type") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

```

#### Selecting features and outcome 
```{r}
X <- dataModel7_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "FR_totaal.1")]
y <- dataModel7_complete$mansa_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(mansa_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1 + mansa_totaal.1 + honos_totaal.1 + FR_totaal.1, data = dataModel7_complete))
```

#### baseline model that uses mansa_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel7_complete[, c("mansa_totaal.1")] 
y_baseline <- dataModel7_complete$mansa_totaal.2  

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"

cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model7_results <- rbind(model7_results, data.frame(
  Model = "7",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model7_results_normalized <- rbind(model7_results_normalized, data.frame(
  Model = "7",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline / r2_mean_baseline,
  R2_std = "-"
))
```

#### svr using cross validation 
```{r}
scalerSVM <- sklearn$preprocessing$StandardScaler()
SVR <- sklearn$svm$SVR
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

X_scaledSVM <- scalerSVM$fit_transform(X) 

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svr7 <- SVR(C=C_val, kernel='linear')
  
  svr7$fit(X_scaledSVM, y)
  y_pred <- svr7$predict(X_scaledSVM)

  results_df <- data.frame(
    Actual = y,
    Predicted = y_pred
  )

  # creating a plot that shows predicted versus actual values 
  plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +  # Ideal line (y = x)
    labs(
      title = paste("SVR Model Predictions vs Actual Outcomes\nC =", C_val),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(plot)
  
  mse_scores <- cross_val_score(svr7, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svr7, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svr7, X_scaledSVM, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model7_results <- rbind(model7_results, data.frame(
    Model = "7",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model7_results_normalized <- rbind(model7_results_normalized, data.frame(
    Model = "7",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### knr using cross validation 
```{r}
KNeighborsRegressor <- sklearn$neighbors$KNeighborsRegressor
MinMaxScaler <- sklearn$preprocessing$MinMaxScaler
RepeatedKFold <- sklearn$model_selection$RepeatedKFold
cross_val_score <- sklearn$model_selection$cross_val_score
metrics <- sklearn$metrics

scalerKNN <- MinMaxScaler()
X_scaledKNN <- scalerKNN$fit_transform(X)

rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

for (n in c(3, 5, 10, 30, 100)) {
  knn7 <- KNeighborsRegressor(n_neighbors = as.integer(n))
  
  mse_scores <- cross_val_score(knn7, X_scaledKNN, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(knn7, X_scaledKNN, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(knn7, X_scaledKNN, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r)  
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean) 
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("KNN Regressor model with n =", n, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  model7_results <- rbind(model7_results, data.frame(
    Model = "7",
    Method = "KNR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  model7_results_normalized <- rbind(model7_results_normalized, data.frame(
    Model = "7",
    Method = "KNR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("neighbors = ", n),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

#### Nested cross validation
```{r}
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results <- data.frame()
best_params_list <- list()
outer_scores <- c()

# parameter grid
c_values <- c(0.01, 0.1, 1, 10, 30)

# outer CV loop
for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]
  
  set.seed(42 + outer_fold)  
  n_train_samples <- length(train_idx)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  
  for (c_val in c_values) {
    cv_scores <- c()

    # inner loop 
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]
      
      if (inner_fold == 1) {  
        cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
            "rows, y:", length(y_inner_train), "elements\n")
        cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
            "rows, y:", length(y_inner_val), "elements\n")
      }
      
      inner_svr <- SVR(C = c_val, kernel = 'linear')
      inner_svr$fit(X_inner_train, y_inner_train)
      
      y_pred_inner <- inner_svr$predict(X_inner_val)
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
      cv_scores <- c(cv_scores, neg_mse)
    }
    
    mean_score <- mean(cv_scores)
    
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
    }
  }
  
  cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
  
  best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
  
  best_svr <- SVR(C = best_c, kernel = 'linear')
  best_svr$fit(X_train_outer, y_train_outer)
  
  y_pred_outer <- best_svr$predict(X_test_outer)
  
  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
  
  outer_scores <- c(outer_scores, r2)
  
  nested_cv_results <- rbind(nested_cv_results, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results:\n")
print(nested_cv_results)
cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")

cat("\nBest Parameters per Fold:\n")
for (i in 1:length(best_params_list)) {
  cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# after the nested CV is complete, we evaluate the final model with cross_val_score
rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

final_svr <- SVR(C = most_common_c, kernel = 'linear')

mse_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model7_results <- rbind(model7_results, data.frame(
  Model = "7",
  Method = "SVR_nested_CV",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model7_results_normalized <- rbind(model7_results_normalized, data.frame(
  Model = "7",
  Method = "SVR_nested_CV",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


final_svr$fit(X_scaledSVM, y)
final_y_pred <- final_svr$predict(X_scaledSVM)

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)
```





### Feature selection test 
```{r}
RFECV <- sklearn$feature_selection$RFECV
KFold <- sklearn$model_selection$KFold  # RepeatedKFold isn't supported by RFECV, use KFold here
```

```{r}
# for the sixth model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


dataModelFS <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, mansa_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
modelFS_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

modelFS_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageMFS <- colSums(is.na(dataModelFS)) / nrow(dataModelFS) * 100
print(missing_percentageMFS)
```

Inspire_totaal.1 heeft veel missende waardes, wil ik die wel meenemen? 

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModelFS))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModelFS)
```

#### imputing data 
```{r}
methods <- make.method(dataModelFS)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""
methods["mansa_totaal.2"] <- ""

# doing the imputation
imputed_dataModelFS <- mice(dataModel6, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModelFS)
```

```{r}
dataModelFS_complete <- complete(imputed_dataModelFS,1)
```

```{r}
original_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "FR_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModelFS)),
  value = c(dataModel6$mansa_totaal.1, dataModelFS$honos_totaal.1, dataModelFS$Inspire_totaal.1, dataModelFS$FR_totaal.1, dataModelFS$levenspartner.1, dataModelFS$betaaldwerk.1),
  type = "Original"
)

imputed_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "FR_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModelFS)),
  value = c(complete(imputed_dataModelFS, 1)$mansa_totaal.1, 
            complete(imputed_dataModelFS, 1)$honos_totaal.1, 
            complete(imputed_dataModelFS, 1)$Inspire_totaal.1,
            complete(imputed_dataModelFS, 1)$FR_totaal.1,
            complete(imputed_dataModelFS, 1)$levenspartner.1, 
            complete(imputed_dataModelFS, 1)$betaaldwerk.1),
  type = "Imputed"
)

plot_data <- rbind(original_data, imputed_data)

ggplot(plot_data, aes(x = value, color = type, linetype = type)) +
  geom_density() +
  facet_wrap(~variable, scales = "free") + 
  labs(title = "Density Plot of Original (Dashed) vs. Imputed (Solid) Data",
       x = "Value",
       y = "Density",
       color = "Data Type") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

```

#### Selecting features and outcome 
```{r}
X <- dataModelFS_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "FR_totaal.1")]
y <- dataModelFS_complete$mansa_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(mansa_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1 + mansa_totaal.1 + honos_totaal.1 + Inspire_totaal.1 + FR_totaal.1, data = dataModelFS_complete))
```

#### baseline model that uses mansa_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModelFS_complete[, c("mansa_totaal.1")]  # Use only mansa_totaal.1 as predictor
y_baseline <- dataModelFS_complete$mansa_totaal.2  # The outcome is mansa_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

modelFS_results <- rbind(modelFS_results, data.frame(
  Model = "FS",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

modelFS_results_normalized <- rbind(modelFS_results_normalized, data.frame(
  Model = "FS",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline / r2_mean_baseline,
  R2_std = "-"
))
```


```{r}
for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svrFS <- SVR(C=C_val, kernel='linear')

  # Run RFECV
  rfecv <- RFECV(estimator = svrFS)
  rfecv$set_params(
  step = as.integer(1),
  cv = KFold(n_splits=5L, shuffle=TRUE, random_state=42L),
  scoring = "neg_mean_squared_error"
)

  
  rfecv$fit(X_scaledSVM, y)

  # Get selected features and their mask
  selected_mask <- rfecv$support_
  selected_indices <- which(py_to_r(selected_mask))
  selected_feature_names <- colnames(X)[selected_indices]

  cat("SVR with C =", C_val, ": selected", length(selected_indices), "features\n")
  print(selected_feature_names)
  
  # You could now re-run cross_val_score using only the selected features:
  X_selected <- X_scaledSVM[, selected_mask, drop = FALSE]

  mse_scores <- cross_val_score(svrFS, X_selected, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svrFS, X_selected, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svrFS, X_selected, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  modelFS_results <- rbind(modelFS_results, data.frame(
    Model = "FS",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  modelFS_results_normalized <- rbind(modelFS_results_normalized, data.frame(
    Model = "FS",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

### Feature selection test 2
```{r}
RFECV <- sklearn$feature_selection$RFECV
KFold <- sklearn$model_selection$KFold  # RepeatedKFold isn't supported by RFECV, use KFold here
```

```{r}
# for the sixth model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:


# dataModelFS2 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, modusmeanPsyKl, burgerlijkestaat.1, leefsituatie.1, leefsituatie_steun.1, opleiding.1, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, mansa_totaal.2, HONOS_add_13.1, HONOS_add_14.1, HONOS_add_15.1, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, FR_1.1, FR_2.1, FR_3.1, IHS_01.1, IHS_02.1, IHS_03.1, IHS_04.1, IHS_05.1, month_diff_1_and_2, month_diff_1_and_3, month_diff_1_and_4, month_diff_1_and_5, month_diff_1_and_6) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 

# dataModelFS2 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, modusmeanPsyKl, burgerlijkestaat.1, leefsituatie.1, leefsituatie_steun.1, opleiding.1, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, mansa_totaal.2, month_diff_1_and_2, month_diff_1_and_3, month_diff_1_and_4, month_diff_1_and_5, month_diff_1_and_6) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 

dataModelFS2 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, leefsituatie_steun.1, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, mansa_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 


```

```{r}
# the following is to keep track of the metrics 
modelFS2_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

modelFS2_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageMFS2 <- colSums(is.na(dataModelFS2)) / nrow(dataModelFS2) * 100
print(missing_percentageMFS2)
```

Inspire_totaal.1 heeft veel missende waardes, wil ik die wel meenemen? 

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModelFS2))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModelFS2)
```

#### imputing data 
```{r}
methods <- make.method(dataModelFS2)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 

# methods["burgerlijkestaat.1"] <- "polyreg" # categorical
# methods["leefsituatie.1"] <- "polyreg" # categorical
methods["leefsituatie_steun.1"] <- "logreg" # binary 
# methods["opleiding.1"] <- "polyreg" # categorical
# methods["modusmeanPsyKl"] <- "pmm" # numeric 
methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""
methods["mansa_totaal.2"] <- ""

# methods["HONOS_add_13.1"] <- "polr" # ordinal 
# methods["HONOS_add_14.1"] <- "polr" # ordinal 
# methods["HONOS_add_15.1"] <- "polr" # ordinal 
# methods["MANSA_PH_7.1"] <- "logreg" # binary
# methods["MANSA_PH_9.1"] <- ""
# methods["MANSA_PH_10.1"] <- "logreg" # binary 
# methods["MANSA_PH_11.1"] <- "logreg" # binary 
# methods["FR_1.1"] <- "polyreg" # categorical
# methods["FR_2.1"] <- "polyreg" # categorical
# methods["FR_3.1"] <- "polyreg" # categorical
# methods["IHS_01.1"] <- "polr" # ordinal 
# methods["IHS_02.1"] <- "polr" # ordinal 
# methods["IHS_03.1"] <- "polr" # ordinal
# methods["IHS_04.1"] <- "polr" # ordinal 
# methods["IHS_05.1"] <- "polr" # ordinal 

# oke jeroen dit moet je nu doen: 
# spss dataset bekijken en ff die vragenlijsten doorspitten. Kijken of los toevoegen een beetje nuttig gaat zijn 
# ook kijken of alles categorical is of ordinal 

# doing the imputation
imputed_dataModelFS2 <- mice(dataModelFS2, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModelFS2)
```

```{r}
dataModelFS2_complete <- complete(imputed_dataModelFS2,1)
```

```{r}
original_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "FR_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModelFS)),
  value = c(dataModel6$mansa_totaal.1, dataModelFS$honos_totaal.1, dataModelFS$Inspire_totaal.1, dataModelFS$FR_totaal.1, dataModelFS$levenspartner.1, dataModelFS$betaaldwerk.1),
  type = "Original"
)

imputed_data <- data.frame(
  variable = rep(c("mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "FR_totaal.1", "levenspartner.1", "betaaldwerk.1"), each = nrow(dataModelFS)),
  value = c(complete(imputed_dataModelFS, 1)$mansa_totaal.1, 
            complete(imputed_dataModelFS, 1)$honos_totaal.1, 
            complete(imputed_dataModelFS, 1)$Inspire_totaal.1,
            complete(imputed_dataModelFS, 1)$FR_totaal.1,
            complete(imputed_dataModelFS, 1)$levenspartner.1, 
            complete(imputed_dataModelFS, 1)$betaaldwerk.1),
  type = "Imputed"
)

plot_data <- rbind(original_data, imputed_data)

ggplot(plot_data, aes(x = value, color = type, linetype = type)) +
  geom_density() +
  facet_wrap(~variable, scales = "free") + 
  labs(title = "Density Plot of Original (Dashed) vs. Imputed (Solid) Data",
       x = "Value",
       y = "Density",
       color = "Data Type") +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

```

#### Selecting features and outcome 
```{r}
X <- dataModelFS2_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "leefsituatie_steun.1", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "FR_totaal.1")]
y <- dataModelFS2_complete$mansa_totaal.2

# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(mansa_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1 + mansa_totaal.1 + honos_totaal.1 + Inspire_totaal.1 + FR_totaal.1, data = dataModelFS2_complete))
```

#### baseline model that uses mansa_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModelFS2_complete[, c("mansa_totaal.1")]  # Use only mansa_totaal.1 as predictor
y_baseline <- dataModelFS2_complete$mansa_totaal.2  # The outcome is mansa_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

modelFS2_results <- rbind(modelFS2_results, data.frame(
  Model = "FS2",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

modelFS2_results_normalized <- rbind(modelFS2_results_normalized, data.frame(
  Model = "FS",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline / r2_mean_baseline,
  R2_std = "-"
))
```


```{r}
for (C_val in c(1, 0.1, 0.01, 10, 30)) {
  svrFS2 <- SVR(C=C_val, kernel='linear')

  # Run RFECV
  rfecv <- RFECV(estimator = svrFS2)
  rfecv$set_params(
  step = as.integer(1),
  cv = KFold(n_splits=5L, shuffle=TRUE, random_state=42L),
  scoring = "neg_mean_squared_error"
)

  
  rfecv$fit(X_scaledSVM, y)

  # Get selected features and their mask
  selected_mask <- rfecv$support_
  selected_indices <- which(py_to_r(selected_mask))
  selected_feature_names <- colnames(X)[selected_indices]

  cat("SVR with C =", C_val, ": selected", length(selected_indices), "features\n")
  print(selected_feature_names)
  
  # You could now re-run cross_val_score using only the selected features:
  X_selected <- X_scaledSVM[, selected_mask, drop = FALSE]

  mse_scores <- cross_val_score(svrFS2, X_selected, y, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(svrFS2, X_selected, y, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(svrFS2, X_selected, y, cv=rkf, scoring='r2')

  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)

  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)

  cat("SVR model with C =", C_val, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  modelFS2_results <- rbind(modelFS2_results, data.frame(
    Model = "FS2",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))

  modelFS2_results_normalized <- rbind(modelFS2_results_normalized, data.frame(
    Model = "FS2",
    Method = "SVR",
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", C_val),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
}

```

## Fase 3: 
### model 8a: baseline for Brief Inspire-O
```{r}
# for model 8a i just take the input from Inspire_totaal.1 (baseline) for a patient and directly use it as prediction for Inspire_totaal.2
# I filter the data to only include non-missing values of outcome Inspire_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel8a <- dataNIEUW %>% select(Inspire_totaal.1, Inspire_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(Inspire_totaal.2))
```

```{r}
# the following is to keep track of the metrics 
model8a_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model8a_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```


#### visualizing missing data 

```{r}
missing_percentageM8a <- colSums(is.na(dataModel8a)) / nrow(dataModel8a) * 100
print(missing_percentageM8a)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel8a))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel8a)
```

#### imputing data 
```{r}
methods <- make.method(dataModel8a)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel8a <- mice(dataModel8a, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel8a)
```

```{r}
dataModel8a_complete <- complete(imputed_dataModel8a,1)
```

```{r}
original_data <- dataModel8a$Inspire_totaal.1
imputed_data <- complete(imputed_dataModel8a, 1)$Inspire_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X_baseline <- dataModel8a_complete$Inspire_totaal.1
y_baseline <- dataModel8a_complete$Inspire_totaal.2
```

#### baseline model that uses Inspire_totaal.1 input directly as outcome 
```{r}

y_pred_baseline <- X_baseline  

# following is for creating the plot
results_df_8a <- data.frame(
  Actual = y_baseline,
  Predicted = y_pred_baseline
)

plot_8a <- ggplot(results_df_8a, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +  # Ideal y = x line
  labs(
    title = "Baseline Model 8a Predictions vs Actual Outcomes",
    x = "Actual Outcome (Inspire_totaal.2)",
    y = "Predicted Outcome (Inspire_totaal.1)"
  ) +
  theme_minimal()

# Display the plot
print(plot_8a)


mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean_baseline)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using Inspire_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model8a_results <- rbind(model8a_results, data.frame(
  Model = "8a",
  Method = "baseline",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model8a_results_normalized <- rbind(model8a_results_normalized, data.frame(
  Model = "8a",
  Method = "baseline",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline,
  R2_std = "-"
))

```













### model 8b: baseline for Mansa
```{r}
# for model 8b i just take the input from mansa_totaal.1 (baseline) for a patient and directly use it as prediction for mansa_totaal.2
# I filter the data to only include non-missing values of outcome Inspire_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel8b <- dataNIEUW %>% select(mansa_totaal.1, mansa_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2))
```

```{r}
# the following is to keep track of the metrics 
model8b_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model8b_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```


#### visualizing missing data 

```{r}
missing_percentageM8b <- colSums(is.na(dataModel8b[ , !(names(dataModel8b) == "month_diff_1_and_2")])) / nrow(dataModel8b) * 100
print(missing_percentageM8b)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel8b))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel8b)
```

#### imputing data 
```{r}
methods <- make.method(dataModel8b)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel8b <- mice(dataModel8b, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel8b)
```

```{r}
dataModel8b_complete <- complete(imputed_dataModel8b,1)
```

```{r}
original_data <- dataModel8b$mansa_totaal.1
imputed_data <- complete(imputed_dataModel8b, 1)$mansa_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X_baseline <- dataModel8b_complete$mansa_totaal.1
y_baseline <- dataModel8b_complete$mansa_totaal.2
```

#### baseline model that uses Inspire_totaal.1 input directly as outcome 
```{r}

y_pred_baseline <- X_baseline  

# Create dataframe of actual vs predicted values
results_df_8b <- data.frame(
  Actual = y_baseline,
  Predicted = y_pred_baseline
)

# Create plot
plot_8b <- ggplot(results_df_8b, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +  # Ideal y = x line
  labs(
    title = "Baseline Model 8b Predictions vs Actual Outcomes",
    x = "Actual Outcome (mansa_totaal.2)",
    y = "Predicted Outcome (mansa_totaal.1)"
  ) +
  theme_minimal()

print(plot_8b)


mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean_baseline)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model8b_results <- rbind(model8b_results, data.frame(
  Model = "8b",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model8b_results_normalized <- rbind(model8b_results_normalized, data.frame(
  Model = "8b",
  Method = "baseline",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline,
  R2_std = "-"
))

```













### model 8c: baseline for honos 
```{r}
# for model 8c i just take the input from Inspire_totaal.1 (baseline) for a patient and directly use it as prediction for Inspire_totaal.2
# I filter the data to only include non-missing values of outcome Inspire_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel8c <- dataNIEUW %>% select(honos_totaal.1, honos_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(honos_totaal.2))
```

```{r}
# the following is to keep track of the metrics 
model8c_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model8c_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```


#### visualizing missing data 

```{r}
missing_percentageM8c <- colSums(is.na(dataModel8c)) / nrow(dataModel8c) * 100
print(missing_percentageM8c)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel8c))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel8c)
```

#### imputing data 
```{r}
methods <- make.method(dataModel8c)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["honos_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel8c <- mice(dataModel8c, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel8c)
```

```{r}
dataModel8c_complete <- complete(imputed_dataModel8c,1)
```

```{r}
original_data <- dataModel8c$honos_totaal.1
imputed_data <- complete(imputed_dataModel8c, 1)$honos_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X_baseline <- dataModel8c_complete$honos_totaal.1
y_baseline <- dataModel8c_complete$honos_totaal.2
```

#### baseline model that uses honos_totaal.1 input directly as outcome 
```{r}

y_pred_baseline <- X_baseline  

# following is for creating the plot
results_df_8c <- data.frame(
  Actual = y_baseline,
  Predicted = y_pred_baseline
)

plot_8c <- ggplot(results_df_8c, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +  # Ideal y = x line
  labs(
    title = "Baseline Model 8c Predictions vs Actual Outcomes",
    x = "Actual Outcome (honos_totaal.2)",
    y = "Predicted Outcome (honos_totaal.1)"
  ) +
  theme_minimal()

print(plot_8c)


mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean_baseline)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using honos_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model8c_results <- rbind(model8c_results, data.frame(
  Model = "8c",
  Method = "baseline",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model8c_results_normalized <- rbind(model8c_results_normalized, data.frame(
  Model = "8c",
  Method = "baseline",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline,
  R2_std = "-"
))

```


















### model 8d: baseline for FR
```{r}
# for model 8d i just take the input from FR_totaal.1 (baseline) for a patient and directly use it as prediction for FR_totaal.2
# I filter the data to only include non-missing values of outcome FR_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel8d <- dataNIEUW %>% select(FR_totaal.1, FR_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(FR_totaal.2))
```

```{r}
# the following is to keep track of the metrics 
model8d_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model8d_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```


#### visualizing missing data 

```{r}
missing_percentageM8d <- colSums(is.na(dataModel8d)) / nrow(dataModel8d) * 100
print(missing_percentageM8d)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel8d))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel8d)
```

#### imputing data 
```{r}
methods <- make.method(dataModel8d)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["FR_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel8d <- mice(dataModel8d, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel8d)
```

```{r}
dataModel8d_complete <- complete(imputed_dataModel8d,1)
```

```{r}
original_data <- dataModel8d$FR_totaal.1
imputed_data <- complete(imputed_dataModel8d, 1)$FR_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X_baseline <- dataModel8d_complete$FR_totaal.1
y_baseline <- dataModel8d_complete$FR_totaal.2
```

#### baseline model that uses FR_totaal.1 input directly as outcome 
```{r}

y_pred_baseline <- X_baseline  

# following is for creating the plot
results_df_8d <- data.frame(
  Actual = y_baseline,
  Predicted = y_pred_baseline
)

plot_8d <- ggplot(results_df_8d, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +  # Ideal y = x line
  labs(
    title = "Baseline Model 8d Predictions vs Actual Outcomes",
    x = "Actual Outcome (FR_totaal.2)",
    y = "Predicted Outcome (FR_totaal.1)"
  ) +
  theme_minimal()

# Display the plot
print(plot_8d)


mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean_baseline)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using FR_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

model8d_results <- rbind(model8d_results, data.frame(
  Model = "8d",
  Method = "baseline",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline,
  MSE_std = mse_std_baseline,
  RMSE_mean = rmse_mean_baseline,
  RMSE_std = rmse_std_baseline,
  MAE_mean = mae_mean_baseline,
  MAE_std = mae_std_baseline,
  R2_mean = r2_mean_baseline,
  R2_std = r2_std_baseline
))

model8d_results_normalized <- rbind(model8d_results_normalized, data.frame(
  Model = "8d",
  Method = "baseline",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = "-",
  MSE_mean = mse_mean_baseline / mse_mean_baseline,
  MSE_std = "-",
  RMSE_mean = rmse_mean_baseline / rmse_mean_baseline,
  RMSE_std = "-",
  MAE_mean = mae_mean_baseline / mae_mean_baseline,
  MAE_std = "-",
  R2_mean = r2_mean_baseline,
  R2_std = "-"
))

```














### model 9a: baseline plus for Brief Inspire-O
baseline plus is a better version of the normal baseline. Instead of directly using the baseline as outcome it trains a svr model with the baseline as only predictor 
```{r}
# for model 9a I only use Inspire_totaal.1 as predictor
# and the following outcome: Inpsire_totaal.2
# I filter the data to only include non-missing values of outcome Inspire_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel9a <- dataNIEUW %>% select(Inspire_totaal.1, Inspire_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(Inspire_totaal.2))
```

```{r}
# the following is to keep track of the metrics 
model9a_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model9a_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```


#### visualizing missing data 

```{r}
missing_percentageM9a <- colSums(is.na(dataModel9a)) / nrow(dataModel9a) * 100
print(missing_percentageM9a)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel9a))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel9a)
```

#### imputing data 
```{r}
methods <- make.method(dataModel9a)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel9a <- mice(dataModel9a, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel9a)
```

```{r}
dataModel9a_complete <- complete(imputed_dataModel9a,1)
```

```{r}
original_data <- dataModel9a$Inspire_totaal.1
imputed_data <- complete(imputed_dataModel9a, 1)$Inspire_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X <- dataModel9a_complete$Inspire_totaal.1
y <- dataModel9a_complete$Inspire_totaal.2

X_scaledSVM <- scalerSVM$fit_transform(r_to_py(matrix(X, ncol = 1)))
```

#### baseline model that uses Inspire_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel9a_complete[, c("Inspire_totaal.1")]  # Use only Inspire_totaal.1 as predictor
y_baseline <- dataModel9a_complete$Inspire_totaal.2  # The outcome is Inspire_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean_baseline)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using Inspire_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")
```

#### Nested cross validation
```{r}
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results <- data.frame()
best_params_list <- list()
outer_scores <- c()

# parameter grid
c_values <- c(0.01, 0.1, 1, 10, 30)

# outer CV loop
for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]
  
  set.seed(42 + outer_fold)  
  n_train_samples <- length(train_idx)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  
  for (c_val in c_values) {
    cv_scores <- c()

    # inner loop 
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]
      
      if (inner_fold == 1) {  
        cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
            "rows, y:", length(y_inner_train), "elements\n")
        cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
            "rows, y:", length(y_inner_val), "elements\n")
      }
      
      inner_svr <- SVR(C = c_val, kernel = 'linear')
      inner_svr$fit(X_inner_train, y_inner_train)
      
      y_pred_inner <- inner_svr$predict(X_inner_val)
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
      cv_scores <- c(cv_scores, neg_mse)
    }
    
    mean_score <- mean(cv_scores)
    
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
    }
  }
  
  cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
  
  best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
  
  best_svr <- SVR(C = best_c, kernel = 'linear')
  best_svr$fit(X_train_outer, y_train_outer)
  
  y_pred_outer <- best_svr$predict(X_test_outer)
  
  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
  
  outer_scores <- c(outer_scores, r2)
  
  nested_cv_results <- rbind(nested_cv_results, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results:\n")
print(nested_cv_results)
cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")

cat("\nBest Parameters per Fold:\n")
for (i in 1:length(best_params_list)) {
  cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# after the nested CV is complete, we evaluate the final model with cross_val_score
rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

final_svr <- SVR(C = most_common_c, kernel = 'linear')

mse_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model9a_results <- rbind(model9a_results, data.frame(
  Model = "9a",
  Method = "baseline-plus",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model9a_results_normalized <- rbind(model9a_results_normalized, data.frame(
  Model = "9a",
  Method = "baseline-plus",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


final_svr$fit(X_scaledSVM, y)
final_y_pred <- final_svr$predict(X_scaledSVM)

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)
```






### model 9b: baseline plus for Mansa
baseline plus is a better version of the normal baseline. Instead of directly using the baseline as outcome it trains a svr model with the baseline as only predictor 
```{r}
# for model 9b I only use mansa_totaal.1 as predictor
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel9b <- dataNIEUW %>% select(mansa_totaal.1, mansa_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2))
```

```{r}
# the following is to keep track of the metrics 
model9b_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model9b_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```


#### visualizing missing data 

```{r}
missing_percentageM9b <- colSums(is.na(dataModel9b)) / nrow(dataModel9b) * 100
print(missing_percentageM9b)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel9b))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel9b)
```

#### imputing data 
```{r}
methods <- make.method(dataModel9b)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel9b <- mice(dataModel9b, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel9b)
```

```{r}
dataModel9b_complete <- complete(imputed_dataModel9b,1)
```

```{r}
original_data <- dataModel9b$mansa_totaal.1
imputed_data <- complete(imputed_dataModel9b, 1)$mansa_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X <- dataModel9b_complete$mansa_totaal.1
y <- dataModel9b_complete$mansa_totaal.2

X_scaledSVM <- scalerSVM$fit_transform(r_to_py(matrix(X, ncol = 1)))
```

#### baseline model that uses mansa_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel9b_complete[, c("mansa_totaal.1")]  # Use only mansa_totaal.1 as predictor
y_baseline <- dataModel9b_complete$mansa_totaal.2  # The outcome is mansa_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean_baseline)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")
```

#### Nested cross validation
```{r}
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results <- data.frame()
best_params_list <- list()
outer_scores <- c()

# parameter grid
c_values <- c(0.01, 0.1, 1, 10, 30)

# outer CV loop
for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]
  
  set.seed(42 + outer_fold)  
  n_train_samples <- length(train_idx)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  
  for (c_val in c_values) {
    cv_scores <- c()

    # inner loop 
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]
      
      if (inner_fold == 1) {  
        cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
            "rows, y:", length(y_inner_train), "elements\n")
        cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
            "rows, y:", length(y_inner_val), "elements\n")
      }
      
      inner_svr <- SVR(C = c_val, kernel = 'linear')
      inner_svr$fit(X_inner_train, y_inner_train)
      
      y_pred_inner <- inner_svr$predict(X_inner_val)
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
      cv_scores <- c(cv_scores, neg_mse)
    }
    
    mean_score <- mean(cv_scores)
    
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
    }
  }
  
  cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
  
  best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
  
  best_svr <- SVR(C = best_c, kernel = 'linear')
  best_svr$fit(X_train_outer, y_train_outer)
  
  y_pred_outer <- best_svr$predict(X_test_outer)
  
  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
  
  outer_scores <- c(outer_scores, r2)
  
  nested_cv_results <- rbind(nested_cv_results, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results:\n")
print(nested_cv_results)
cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")

cat("\nBest Parameters per Fold:\n")
for (i in 1:length(best_params_list)) {
  cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# after the nested CV is complete, we evaluate the final model with cross_val_score
rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

final_svr <- SVR(C = most_common_c, kernel = 'linear')

mse_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model9b_results <- rbind(model9b_results, data.frame(
  Model = "9b",
  Method = "baseline-plus",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model9b_results_normalized <- rbind(model9b_results_normalized, data.frame(
  Model = "9b",
  Method = "baseline-plus",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


final_svr$fit(X_scaledSVM, y)
final_y_pred <- final_svr$predict(X_scaledSVM)

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)
```


### model 9c: baseline plus for honos
baseline plus is a better version of the normal baseline. Instead of directly using the baseline as outcome it trains a svr model with the baseline as only predictor 
```{r}
# for model 9c I only use honos_totaal.1 as predictor
# and the following outcome: honos_totaal.2
# I filter the data to only include non-missing values of outcome honos_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel9c <- dataNIEUW %>% select(honos_totaal.1, honos_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(honos_totaal.2))
```

```{r}
# the following is to keep track of the metrics 
model9c_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model9c_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```


#### visualizing missing data 

```{r}
missing_percentageM9c <- colSums(is.na(dataModel9c)) / nrow(dataModel9c) * 100
print(missing_percentageM9c)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel9c))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel9c)
```

#### imputing data 
```{r}
methods <- make.method(dataModel9c)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["honos_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel9c <- mice(dataModel9c, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel9c)
```

```{r}
dataModel9c_complete <- complete(imputed_dataModel9c,1)
```

```{r}
original_data <- dataModel9c$honos_totaal.1
imputed_data <- complete(imputed_dataModel9c, 1)$honos_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X <- dataModel9c_complete$honos_totaal.1
y <- dataModel9c_complete$honos_totaal.2

X_scaledSVM <- scalerSVM$fit_transform(r_to_py(matrix(X, ncol = 1)))
```

#### baseline model that uses honos_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel9c_complete[, c("honos_totaal.1")]  # Use only honos_totaal.1 as predictor
y_baseline <- dataModel9c_complete$honos_totaal.2  # The outcome is honos_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean_baseline)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using honos_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")
```

#### Nested cross validation
```{r}
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results <- data.frame()
best_params_list <- list()
outer_scores <- c()

# parameter grid
c_values <- c(0.01, 0.1, 1, 10, 30)

# outer CV loop
for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]
  
  set.seed(42 + outer_fold)  
  n_train_samples <- length(train_idx)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  
  for (c_val in c_values) {
    cv_scores <- c()

    # inner loop 
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]
      
      if (inner_fold == 1) {  
        cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
            "rows, y:", length(y_inner_train), "elements\n")
        cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
            "rows, y:", length(y_inner_val), "elements\n")
      }
      
      inner_svr <- SVR(C = c_val, kernel = 'linear')
      inner_svr$fit(X_inner_train, y_inner_train)
      
      y_pred_inner <- inner_svr$predict(X_inner_val)
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
      cv_scores <- c(cv_scores, neg_mse)
    }
    
    mean_score <- mean(cv_scores)
    
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
    }
  }
  
  cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
  
  best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
  
  best_svr <- SVR(C = best_c, kernel = 'linear')
  best_svr$fit(X_train_outer, y_train_outer)
  
  y_pred_outer <- best_svr$predict(X_test_outer)
  
  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
  
  outer_scores <- c(outer_scores, r2)
  
  nested_cv_results <- rbind(nested_cv_results, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results:\n")
print(nested_cv_results)
cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")

cat("\nBest Parameters per Fold:\n")
for (i in 1:length(best_params_list)) {
  cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# after the nested CV is complete, we evaluate the final model with cross_val_score
rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

final_svr <- SVR(C = most_common_c, kernel = 'linear')

mse_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model9c_results <- rbind(model9c_results, data.frame(
  Model = "9c",
  Method = "baseline-plus",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model9c_results_normalized <- rbind(model9c_results_normalized, data.frame(
  Model = "9c",
  Method = "baseline-plus",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


final_svr$fit(X_scaledSVM, y)
final_y_pred <- final_svr$predict(X_scaledSVM)

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)
```


### model 9d: baseline plus for FR
baseline plus is a better version of the normal baseline. Instead of directly using the baseline as outcome it trains a svr model with the baseline as only predictor 
```{r}
# for model 9d I only use FR_totaal.1 as predictor
# and the following outcome: FR_totaal.2
# I filter the data to only include non-missing values of outcome FR_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel9d <- dataNIEUW %>% select(FR_totaal.1, FR_totaal.2, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(FR_totaal.2))
```

```{r}
# the following is to keep track of the metrics 
model9d_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model9d_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```


#### visualizing missing data 

```{r}
missing_percentageM9d <- colSums(is.na(dataModel9d)) / nrow(dataModel9d) * 100
print(missing_percentageM9d)
```


```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel9d))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel9d)
```

#### imputing data 
```{r}
methods <- make.method(dataModel9d)
# variables underneath have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["FR_totaal.1"] <- "pmm" # numeric 
methods["month_diff_1_and_2"] <- ""

# doing the imputation
imputed_dataModel9d <- mice(dataModel9d, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel9d)
```

```{r}
dataModel9d_complete <- complete(imputed_dataModel9d,1)
```

```{r}
original_data <- dataModel9d$FR_totaal.1
imputed_data <- complete(imputed_dataModel9d, 1)$FR_totaal.1

ggplot() +
  geom_density(aes(x = original_data), color = "blue", linetype = "dashed") +
  geom_density(aes(x = imputed_data), color = "red") +
  ggtitle("Distribution Before (Blue) and After (Red) Imputation")
```

#### Selecting features and outcome 
```{r}
X <- dataModel9d_complete$FR_totaal.1
y <- dataModel9d_complete$FR_totaal.2

X_scaledSVM <- scalerSVM$fit_transform(r_to_py(matrix(X, ncol = 1)))
```

#### baseline model that uses FR_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel9d_complete[, c("FR_totaal.1")]  # Use only FR_totaal.1 as predictor
y_baseline <- dataModel9d_complete$FR_totaal.2  # The outcome is FR_totaal.2

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean_baseline)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"


cat("Baseline model using FR_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")
```

#### Nested cross validation
```{r}
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results <- data.frame()
best_params_list <- list()
outer_scores <- c()

# parameter grid
c_values <- c(0.01, 0.1, 1, 10, 30)

# outer CV loop
for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]
  
  set.seed(42 + outer_fold)  
  n_train_samples <- length(train_idx)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  
  for (c_val in c_values) {
    cv_scores <- c()

    # inner loop 
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]
      
      if (inner_fold == 1) {  
        cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
            "rows, y:", length(y_inner_train), "elements\n")
        cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
            "rows, y:", length(y_inner_val), "elements\n")
      }
      
      inner_svr <- SVR(C = c_val, kernel = 'linear')
      inner_svr$fit(X_inner_train, y_inner_train)
      
      y_pred_inner <- inner_svr$predict(X_inner_val)
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
      cv_scores <- c(cv_scores, neg_mse)
    }
    
    mean_score <- mean(cv_scores)
    
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
    }
  }
  
  cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
  
  best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
  
  best_svr <- SVR(C = best_c, kernel = 'linear')
  best_svr$fit(X_train_outer, y_train_outer)
  
  y_pred_outer <- best_svr$predict(X_test_outer)
  
  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
  
  outer_scores <- c(outer_scores, r2)
  
  nested_cv_results <- rbind(nested_cv_results, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results:\n")
print(nested_cv_results)
cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")

cat("\nBest Parameters per Fold:\n")
for (i in 1:length(best_params_list)) {
  cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# after the nested CV is complete, we evaluate the final model with cross_val_score
rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

final_svr <- SVR(C = most_common_c, kernel = 'linear')

mse_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model9d_results <- rbind(model9d_results, data.frame(
  Model = "9d",
  Method = "baseline-plus",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model9d_results_normalized <- rbind(model9d_results_normalized, data.frame(
  Model = "9d",
  Method = "baseline-plus",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


final_svr$fit(X_scaledSVM, y)
final_y_pred <- final_svr$predict(X_scaledSVM)

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)
```


### model 10a: outcome Brief Inspire-O with set of predictors + baseline Brief Inspire-O 
```{r}
# for this model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1, honos_totaal.1, FR_totaal.1. I removed Inspire_totaal.1 because it has a large amount of missing values 
# and the following outcome: Inspire_totaal.2
# I filter the data to only include non-missing values of outcome Inspire_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel10a <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, Inspire_totaal.2, opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(Inspire_totaal.2)) 
```

```{r}
# the following is to keep track of the metrics 
model10a_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model10a_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM10a <- colSums(is.na(dataModel10a)) / nrow(dataModel10a) * 100
print(missing_percentageM10a)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel10a))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel10a)
```

#### imputing data 
```{r}
methods <- make.method(dataModel10a)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["opleiding_nieuw"] <- "pmm" # numeric 
methods["leefsituatie_nieuw"] <- "logreg" # binary 
methods["MANSA_PH_7.1"] <- "logreg" # binary 
methods["MANSA_PH_9.1"] <- "logreg" # binary 
methods["MANSA_PH_10.1"] <- "logreg" # binary 
methods["MANSA_PH_11.1"] <- "logreg" # binary 
methods["month_diff_1_and_2"] <- ""


# doing the imputation
imputed_dataModel10a <- mice(dataModel10a, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel10a)
```

```{r}
dataModel10a_complete <- complete(imputed_dataModel10a,1)
```

#### Selecting features and outcome 
```{r}
X <- dataModel10a_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "FR_totaal.1", "Inspire_totaal.1", "opleiding_nieuw", "burgerlijkestaat_nieuw", "leefsituatie_nieuw", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1")]
y <- dataModel10a_complete$Inspire_totaal.2  
# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 

```

#### baseline model that uses Inspire_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel10a_complete[, c("Inspire_totaal.1")] 
y_baseline <- dataModel10a_complete$Inspire_totaal.2  

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"

cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")
```
#### Nested cross validation (5x repeated)
```{r}
X_scaledSVM <- scalerSVM$fit_transform(X) 
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

# Saving coefficients and performances 
all_coefficients <- list()
all_performance <- list()

for (repeat_loop in 1:5) {
  # Setting a seed for reproducability 
  set.seed(42 + repeat_loop)
  cat("\n\n========== REPETITION", repeat_loop, "==========\n")
  
  # Creating a different data split for each repetition
  split_indices <- sample(1:n_samples)
  training_size <- floor(0.8 * n_samples)
  
  X_rep <- X_scaledSVM[split_indices[1:training_size], , drop=FALSE]
  y_rep <- y[split_indices[1:training_size]]
  
  cat("Repetition", repeat_loop, "training data dimensions:", dim(X_rep)[1], "x", dim(X_rep)[2], "\n")
  
  outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = nrow(X_rep)))

  nested_cv_results <- data.frame()
  best_params_list <- list()
  outer_scores <- c()

  c_values <- c(0.01, 0.1, 1, 10, 30)
  
  # Outer CV loop
  for (outer_fold in 1:n_outer_folds) {
    cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
    
    test_idx <- which(outer_fold_indices == outer_fold)
    train_idx <- which(outer_fold_indices != outer_fold)
    
    X_train_outer <- X_rep[train_idx, , drop=FALSE]
    y_train_outer <- y_rep[train_idx]
    X_test_outer <- X_rep[test_idx, , drop=FALSE]
    y_test_outer <- y_rep[test_idx]
    
    n_train_samples <- length(train_idx)
    
    set.seed(100 + (repeat_loop * 10) + outer_fold)
    inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
    
    best_c <- NULL
    best_score <- -Inf
    
    for (c_val in c_values) {
      cv_scores <- c()
  
      # Inner loop 
      for (inner_fold in 1:n_inner_folds) {
        inner_val_idx <- which(inner_fold_indices == inner_fold)
        inner_train_idx <- which(inner_fold_indices != inner_fold)
        
        X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
        y_inner_train <- y_train_outer[inner_train_idx]
        X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
        y_inner_val <- y_train_outer[inner_val_idx]
        
        if (inner_fold == 1) {  
          cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
              "rows, y:", length(y_inner_train), "elements\n")
          cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
              "rows, y:", length(y_inner_val), "elements\n")
        }
        
        inner_svr <- SVR(C = c_val, kernel = 'linear')
        inner_svr$fit(X_inner_train, y_inner_train)
        
        y_pred_inner <- inner_svr$predict(X_inner_val)
        neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
        cv_scores <- c(cv_scores, neg_mse)
      }
      
      mean_score <- mean(cv_scores)
      
      if (mean_score > best_score) {
        best_score <- mean_score
        best_c <- c_val
      }
    }
    
    cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
    
    best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
    
    best_svr <- SVR(C = best_c, kernel = 'linear')
    best_svr$fit(X_train_outer, y_train_outer)
    
    y_pred_outer <- best_svr$predict(X_test_outer)
    
    mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
    rmse <- sqrt(mse)
    mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
    r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
    
    outer_scores <- c(outer_scores, r2)
    
    nested_cv_results <- rbind(nested_cv_results, data.frame(
      Fold = outer_fold,
      Best_C = best_c,
      MSE = mse,
      RMSE = rmse,
      MAE = mae,
      R2 = r2
    ))
    
    cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
  }
  
  cat("\nNested CV Results:\n")
  print(nested_cv_results)
  cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")
  
  cat("\nBest Parameters per Fold:\n")
  for (i in 1:length(best_params_list)) {
    cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
  }
  
  c_values <- sapply(best_params_list, function(x) x$C)
  c_table <- table(c_values)
  most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
  
  rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=repeat_loop)
  
  final_svr <- SVR(C = most_common_c, kernel = 'linear')
  
  mse_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='r2')
  
  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)
  
  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)
  
  cat("SVR model with nested CV, best C =", most_common_c, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  # Saving performance metrics for this run
  all_performance[[repeat_loop]] <- data.frame(
    Run = repeat_loop,
    MSE = mse_mean,
    RMSE = rmse_mean,
    MAE = mae_mean,
    R2 = r2_mean,
    C = most_common_c
  )
  
  model10a_results <- rbind(model10a_results, data.frame(
    Model = "10a",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))
  
  
  model10a_results_normalized <- rbind(model10a_results_normalized, data.frame(
    Model = "10a",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
  
  final_svr$fit(X_rep, y_rep)
  final_y_pred <- final_svr$predict(X_rep)
  
  coefficients <- final_svr$coef_
  intercept <- final_svr$intercept_
  
  coef_vector <- py_to_r(coefficients)[1, ]  # flatten the 2D array to a 1D vector
  intercept <- py_to_r(intercept)[1]
  
  # saving coefficients for this run
  all_coefficients[[repeat_loop]] <- coef_vector
  
  feature_names <- colnames(X)
  coef_df <- data.frame(
    Feature = feature_names,
    Coefficient = coef_vector
  )
  
  coef_df <- coef_df %>% arrange(desc(abs(Coefficient)))
  
  top_n <- 20
  print(
  ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = paste("Predictor weights from model 10a (run", repeat_loop, ")"),
         x = "Predictor", y = "Coefficient") +
    theme_minimal()
  )
  
  cat("\nTop coefficients for run", repeat_loop, ":\n")
  print(head(coef_df, top_n))
  
  results_df <- data.frame(
    Actual = y_rep,
    Predicted = final_y_pred
  )
  
  final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
    labs(
      title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(final_plot)
  
  cat("Most common C for run", repeat_loop, ":", most_common_c, "\n")
}

# Calculating mean and SD of performance metrics
performance_df <- do.call(rbind, all_performance)
mean_performance <- colMeans(performance_df[, c("MSE", "RMSE", "MAE", "R2")])
sd_performance <- apply(performance_df[, c("MSE", "RMSE", "MAE", "R2")], 2, sd)

cat("MSE:", mean_performance["MSE"], "±", sd_performance["MSE"], "\n")
cat("RMSE:", mean_performance["RMSE"], "±", sd_performance["RMSE"], "\n")
cat("MAE:", mean_performance["MAE"], "±", sd_performance["MAE"], "\n")
cat("R²:", mean_performance["R2"], "±", sd_performance["R2"], "\n")
cat("Most common C value:", names(sort(table(performance_df$C), decreasing = TRUE)[1]), "\n")

model10a_results <- rbind(model10a_results, data.frame(
  Model = "10a",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"],
  MSE_std = sd_performance["MSE"],
  RMSE_mean = mean_performance["RMSE"],
  RMSE_std = sd_performance["RMSE"],
  MAE_mean = mean_performance["MAE"],
  MAE_std = sd_performance["MAE"],
  R2_mean = mean_performance["R2"],
  R2_std = sd_performance["R2"]
))


model10a_results_normalized <- rbind(model10a_results_normalized, data.frame(
  Model = "10a",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"] / mse_mean_baseline,
  MSE_std = sd_performance["MSE"] / mse_mean_baseline,
  RMSE_mean = mean_performance["RMSE"] / rmse_mean_baseline,
  RMSE_std = sd_performance["RMSE"] / rmse_mean_baseline,
  MAE_mean = mean_performance["MAE"] / mae_mean_baseline,
  MAE_std =  sd_performance["MAE"] / mae_mean_baseline,
  R2_mean = mean_performance["R2"] / r2_mean_baseline,
  R2_std = sd_performance["R2"] / r2_mean_baseline
))

coef_matrix <- do.call(cbind, all_coefficients)
coef_means <- rowMeans(coef_matrix)
coef_sds <- apply(coef_matrix, 1, sd)

coef_summary <- data.frame(
  Feature = feature_names,
  Mean_Coefficient = coef_means,
  SD_Coefficient = coef_sds
)

coef_summary <- coef_summary %>% arrange(desc(abs(Mean_Coefficient)))

coef_long <- data.frame()
for (i in 1:length(all_coefficients)) {
  temp_df <- data.frame(
    Feature = feature_names,
    Coefficient = all_coefficients[[i]],
    Run = paste("Run", i)
  )
  coef_long <- rbind(coef_long, temp_df)
}

top_n <- 20
top_features <- head(coef_summary, top_n)$Feature

coef_long_top <- coef_long %>% filter(Feature %in% top_features)

# Creating a plot with mean ± SD
mean_sd_plot <- ggplot(head(coef_summary, top_n), 
                     aes(x = reorder(Feature, Mean_Coefficient), y = Mean_Coefficient)) +
  geom_col(fill = "steelblue") +
  geom_errorbar(aes(ymin = Mean_Coefficient - SD_Coefficient, 
                  ymax = Mean_Coefficient + SD_Coefficient), width = 0.2) +
  coord_flip() +
  labs(title = "Mean predictor weights across 5 runs",
       subtitle = paste("Error bars show ± 1 SD"),
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

print(mean_sd_plot)

# Creating a plot showing individual points for each run
individual_plot <- ggplot(coef_long_top, 
                        aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient, color = Run)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Individual predictor weights from all 5 runs",
       x = "Predictor", y = "Coefficient") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(individual_plot)

# Creating a plot showing performance metrics across runs
performance_long <- performance_df %>%
  pivot_longer(cols = c("MSE", "RMSE", "MAE", "R2"), 
               names_to = "Metric", values_to = "Value")

performance_plot <- ggplot(performance_long, 
                         aes(x = as.factor(Run), y = Value, group = Metric, color = Metric)) +
  geom_line() +
  geom_point(size = 3) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(title = "Model performance across 5 runs",
       x = "Run", y = "Value") +
  theme_minimal()

print(performance_plot)

# Summary table of coefficients
top_coef_summary <- head(coef_summary, top_n)
print(top_coef_summary)
```

#### Feature selection for model 10a
```{r}
RFE <- sklearn$feature_selection$RFE

n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results_rfe <- data.frame()
best_params_list_rfe <- list()
outer_scores_rfe <- c()

c_values <- c(0.01, 0.1, 1, 10, 30)

for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]

  n_train_samples <- length(train_idx)
  set.seed(42 + outer_fold)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  best_features_mask <- NULL
  
  for (c_val in c_values) {
    cv_scores <- c()
    
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]

      base_model <- SVR(C = c_val, kernel = 'linear')
      rfe_model <- RFE(estimator = base_model, n_features_to_select = 5L)
      rfe_model$fit(X_inner_train, y_inner_train)

      selected_mask <- rfe_model$support_
      selected_indices <- which(py_to_r(selected_mask))
      
      X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
      X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]

      model_selected <- SVR(C = c_val, kernel = 'linear')
      model_selected$fit(X_inner_train_selected, y_inner_train)
      y_pred_val <- model_selected$predict(X_inner_val_selected)
      
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
      cv_scores <- c(cv_scores, neg_mse)
    }

    mean_score <- mean(cv_scores)
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
      # Store best mask based on entire training data
      final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'), n_features_to_select = 5L)
      final_rfe$fit(X_train_outer, y_train_outer)
      best_features_mask <- final_rfe$support_
    }
  }

  best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
  cat("  Best C for fold", outer_fold, ":", best_c, "\n")

  X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
  X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]

  final_model <- SVR(C = best_c, kernel = 'linear')
  final_model$fit(X_train_outer_selected, y_train_outer)
  y_pred_outer <- final_model$predict(X_test_outer_selected)

  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)

  outer_scores_rfe <- c(outer_scores_rfe, r2)

  nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results with RFE:\n")
print(nested_cv_results_rfe)
cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")

cat("\nBest Parameters and Features per Fold:\n")
for (i in 1:length(best_params_list_rfe)) {
  cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
  print(colnames(X)[best_params_list_rfe[[i]]$mask])
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# Apply RFE to full dataset using best C
final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'), n_features_to_select = 5L)
final_rfe$fit(X_scaledSVM, y)
selected_mask_final <- final_rfe$support_

# Subset data to selected features
X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]

# Train final model with selected features
final_svr <- SVR(C = most_common_c, kernel = 'linear')
final_svr$fit(X_selected_final, y)

# Use X_selected_final (only features chosen by RFE) for cross-validation
mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model10a_results <- rbind(model10a_results, data.frame(
  Model = "10a",
  Method = "Feature_Elimination",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model10a_results_normalized <- rbind(model10a_results_normalized, data.frame(
  Model = "10a",
  Method = "Feature_Elimination",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


# Don't refit - use the already fitted model on selected features
final_y_pred <- final_svr$predict(X_selected_final)

# Get coefficients
coefficients <- final_svr$coef_
intercept <- final_svr$intercept_

# Use feature names of selected features only
feature_names <- colnames(X)[selected_mask_final]
coef_vector <- py_to_r(coefficients)[1, ]

coef_df <- data.frame(
  Feature = feature_names,
  Coefficient = coef_vector
)

print(coef_df)

top_n <- 20
ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Predictor weights from feature model 10a (with feature elimination)",
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)

```





### model 10b: outcome Mansa with set of predictors + baseline Mansa
```{r}
# for this model I choose the following predictors: Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1 opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel10b <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, mansa_totaal.2, opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
model10b_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model10b_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM10b <- colSums(is.na(dataModel10b[ , !(names(dataModel10b) %in% c("month_diff_1_and_2", "mansa_totaal.2"))])) / nrow(dataModel10b) * 100
print(missing_percentageM10b)

library(ggplot2)

# Calculate and sort missing percentages
missing_percentageM10b <- colSums(is.na(dataModel10b[ , !(names(dataModel10b) %in% c("month_diff_1_and_2", "mansa_totaal.2"))])) / nrow(dataModel10b) * 100
missing_percentageM10b <- sort(missing_percentageM10b, decreasing = TRUE)

# Convert to data frame for plotting
missing_df <- data.frame(
  Variable = names(missing_percentageM10b),
  MissingPercent = missing_percentageM10b
)

# Create the bar plot
ggplot(missing_df, aes(x = reorder(Variable, MissingPercent), y = MissingPercent)) +
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +  # Flips axes for readability
  labs(
    title = "Missing Data by Variable",
    x = "Variable",
    y = "Missing (%)"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.text.y = element_text(size = 10)
  )



```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel10b))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel10b)
```

#### imputing data 
```{r}
methods <- make.method(dataModel10b)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["opleiding_nieuw"] <- "pmm" # numeric 
methods["leefsituatie_nieuw"] <- "logreg" # binary 
methods["MANSA_PH_7.1"] <- "logreg" # binary 
methods["MANSA_PH_9.1"] <- "logreg" # binary 
methods["MANSA_PH_10.1"] <- "logreg" # binary 
methods["MANSA_PH_11.1"] <- "logreg" # binary 
methods["month_diff_1_and_2"] <- ""


# doing the imputation
imputed_dataModel10b <- mice(dataModel10b, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel10b)
```

```{r}
dataModel10b_complete <- complete(imputed_dataModel10b,1)
```

#### Selecting features and outcome 
```{r}
X <- dataModel10b_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "FR_totaal.1", "Inspire_totaal.1", "opleiding_nieuw", "burgerlijkestaat_nieuw", "leefsituatie_nieuw", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1")]
y <- dataModel10b_complete$mansa_totaal.2
# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### baseline model that uses mansa_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel10b_complete[, c("mansa_totaal.1")] 
y_baseline <- dataModel10b_complete$mansa_totaal.2  

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"

cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")
```
#### Nested cross validation (5x repeated)
```{r}
X_scaledSVM <- scalerSVM$fit_transform(X) 
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

# Saving coefficients and performances 
all_coefficients <- list()
all_performance <- list()

for (repeat_loop in 1:5) {
  # Setting a seed for reproducability 
  set.seed(42 + repeat_loop)
  cat("\n\n========== REPETITION", repeat_loop, "==========\n")
  
  # Creating a different data split for each repetition
  split_indices <- sample(1:n_samples)
  training_size <- floor(0.8 * n_samples)
  
  X_rep <- X_scaledSVM[split_indices[1:training_size], , drop=FALSE]
  y_rep <- y[split_indices[1:training_size]]
  
  cat("Repetition", repeat_loop, "training data dimensions:", dim(X_rep)[1], "x", dim(X_rep)[2], "\n")
  
  outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = nrow(X_rep)))

  nested_cv_results <- data.frame()
  best_params_list <- list()
  outer_scores <- c()

  c_values <- c(0.01, 0.1, 1, 10, 30)
  
  # Outer CV loop
  for (outer_fold in 1:n_outer_folds) {
    cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
    
    test_idx <- which(outer_fold_indices == outer_fold)
    train_idx <- which(outer_fold_indices != outer_fold)
    
    X_train_outer <- X_rep[train_idx, , drop=FALSE]
    y_train_outer <- y_rep[train_idx]
    X_test_outer <- X_rep[test_idx, , drop=FALSE]
    y_test_outer <- y_rep[test_idx]
    
    n_train_samples <- length(train_idx)
    
    set.seed(100 + (repeat_loop * 10) + outer_fold)
    inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
    
    best_c <- NULL
    best_score <- -Inf
    
    for (c_val in c_values) {
      cv_scores <- c()
  
      # Inner loop 
      for (inner_fold in 1:n_inner_folds) {
        inner_val_idx <- which(inner_fold_indices == inner_fold)
        inner_train_idx <- which(inner_fold_indices != inner_fold)
        
        X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
        y_inner_train <- y_train_outer[inner_train_idx]
        X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
        y_inner_val <- y_train_outer[inner_val_idx]
        
        if (inner_fold == 1) {  
          cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
              "rows, y:", length(y_inner_train), "elements\n")
          cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
              "rows, y:", length(y_inner_val), "elements\n")
        }
        
        inner_svr <- SVR(C = c_val, kernel = 'linear')
        inner_svr$fit(X_inner_train, y_inner_train)
        
        y_pred_inner <- inner_svr$predict(X_inner_val)
        neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
        cv_scores <- c(cv_scores, neg_mse)
      }
      
      mean_score <- mean(cv_scores)
      
      if (mean_score > best_score) {
        best_score <- mean_score
        best_c <- c_val
      }
    }
    
    cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
    
    best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
    
    best_svr <- SVR(C = best_c, kernel = 'linear')
    best_svr$fit(X_train_outer, y_train_outer)
    
    y_pred_outer <- best_svr$predict(X_test_outer)
    
    mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
    rmse <- sqrt(mse)
    mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
    r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
    
    outer_scores <- c(outer_scores, r2)
    
    nested_cv_results <- rbind(nested_cv_results, data.frame(
      Fold = outer_fold,
      Best_C = best_c,
      MSE = mse,
      RMSE = rmse,
      MAE = mae,
      R2 = r2
    ))
    
    cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
  }
  
  cat("\nNested CV Results:\n")
  print(nested_cv_results)
  cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")
  
  cat("\nBest Parameters per Fold:\n")
  for (i in 1:length(best_params_list)) {
    cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
  }
  
  c_values <- sapply(best_params_list, function(x) x$C)
  c_table <- table(c_values)
  most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
  
  rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=repeat_loop)
  
  final_svr <- SVR(C = most_common_c, kernel = 'linear')
  
  mse_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='r2')
  
  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)
  
  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)
  
  cat("SVR model with nested CV, best C =", most_common_c, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  # Saving performance metrics for this run
  all_performance[[repeat_loop]] <- data.frame(
    Run = repeat_loop,
    MSE = mse_mean,
    RMSE = rmse_mean,
    MAE = mae_mean,
    R2 = r2_mean,
    C = most_common_c
  )
  
  model10b_results <- rbind(model10b_results, data.frame(
    Model = "10b",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))
  
  
  model10b_results_normalized <- rbind(model10b_results_normalized, data.frame(
    Model = "10b",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
  
  final_svr$fit(X_rep, y_rep)
  final_y_pred <- final_svr$predict(X_rep)
  
  coefficients <- final_svr$coef_
  intercept <- final_svr$intercept_
  
  coef_vector <- py_to_r(coefficients)[1, ]  # flatten the 2D array to a 1D vector
  intercept <- py_to_r(intercept)[1]
  
  # saving coefficients for this run
  all_coefficients[[repeat_loop]] <- coef_vector
  
  feature_names <- colnames(X)
  coef_df <- data.frame(
    Feature = feature_names,
    Coefficient = coef_vector
  )
  
  coef_df <- coef_df %>% arrange(desc(abs(Coefficient)))
  
  top_n <- 20
  print(
  ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = paste("Predictor weights from model 10b (run", repeat_loop, ")"),
         x = "Predictor", y = "Coefficient") +
    theme_minimal()
  )
  
  cat("\nTop coefficients for run", repeat_loop, ":\n")
  print(head(coef_df, top_n))
  
  results_df <- data.frame(
    Actual = y_rep,
    Predicted = final_y_pred
  )
  
  final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
    labs(
      title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(final_plot)
  
  cat("Most common C for run", repeat_loop, ":", most_common_c, "\n")
}

# Calculating mean and SD of performance metrics
performance_df <- do.call(rbind, all_performance)
mean_performance <- colMeans(performance_df[, c("MSE", "RMSE", "MAE", "R2")])
sd_performance <- apply(performance_df[, c("MSE", "RMSE", "MAE", "R2")], 2, sd)

cat("MSE:", mean_performance["MSE"], "±", sd_performance["MSE"], "\n")
cat("RMSE:", mean_performance["RMSE"], "±", sd_performance["RMSE"], "\n")
cat("MAE:", mean_performance["MAE"], "±", sd_performance["MAE"], "\n")
cat("R²:", mean_performance["R2"], "±", sd_performance["R2"], "\n")
cat("Most common C value:", names(sort(table(performance_df$C), decreasing = TRUE)[1]), "\n")

model10b_results <- rbind(model10b_results, data.frame(
  Model = "10b",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"],
  MSE_std = sd_performance["MSE"],
  RMSE_mean = mean_performance["RMSE"],
  RMSE_std = sd_performance["RMSE"],
  MAE_mean = mean_performance["MAE"],
  MAE_std = sd_performance["MAE"],
  R2_mean = mean_performance["R2"],
  R2_std = sd_performance["R2"]
))


model10b_results_normalized <- rbind(model10b_results_normalized, data.frame(
  Model = "10b",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"] / mse_mean_baseline,
  MSE_std = sd_performance["MSE"] / mse_mean_baseline,
  RMSE_mean = mean_performance["RMSE"] / rmse_mean_baseline,
  RMSE_std = sd_performance["RMSE"] / rmse_mean_baseline,
  MAE_mean = mean_performance["MAE"] / mae_mean_baseline,
  MAE_std =  sd_performance["MAE"] / mae_mean_baseline,
  R2_mean = mean_performance["R2"] / r2_mean_baseline,
  R2_std = sd_performance["R2"] / r2_mean_baseline
))

coef_matrix <- do.call(cbind, all_coefficients)
coef_means <- rowMeans(coef_matrix)
coef_sds <- apply(coef_matrix, 1, sd)

coef_summary <- data.frame(
  Feature = feature_names,
  Mean_Coefficient = coef_means,
  SD_Coefficient = coef_sds
)

coef_summary <- coef_summary %>% arrange(desc(abs(Mean_Coefficient)))

coef_long <- data.frame()
for (i in 1:length(all_coefficients)) {
  temp_df <- data.frame(
    Feature = feature_names,
    Coefficient = all_coefficients[[i]],
    Run = paste("Run", i)
  )
  coef_long <- rbind(coef_long, temp_df)
}

top_n <- 20
top_features <- head(coef_summary, top_n)$Feature

coef_long_top <- coef_long %>% filter(Feature %in% top_features)

# Creating a plot with mean ± SD
mean_sd_plot <- ggplot(head(coef_summary, top_n), 
                     aes(x = reorder(Feature, Mean_Coefficient), y = Mean_Coefficient)) +
  geom_col(fill = "steelblue") +
  geom_errorbar(aes(ymin = Mean_Coefficient - SD_Coefficient, 
                  ymax = Mean_Coefficient + SD_Coefficient), width = 0.2) +
  coord_flip() +
  labs(title = "Mean predictor weights across 5 runs",
       subtitle = paste("Error bars show ± 1 SD"),
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

print(mean_sd_plot)

# Creating a plot showing individual points for each run
individual_plot <- ggplot(coef_long_top, 
                        aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient, color = Run)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Individual predictor weights from all 5 runs",
       x = "Predictor", y = "Coefficient") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(individual_plot)

# Creating a plot showing performance metrics across runs
performance_long <- performance_df %>%
  pivot_longer(cols = c("MSE", "RMSE", "MAE", "R2"), 
               names_to = "Metric", values_to = "Value")

performance_plot <- ggplot(performance_long, 
                         aes(x = as.factor(Run), y = Value, group = Metric, color = Metric)) +
  geom_line() +
  geom_point(size = 3) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(title = "Model performance across 5 runs",
       x = "Run", y = "Value") +
  theme_minimal()

print(performance_plot)

# Summary table of coefficients
top_coef_summary <- head(coef_summary, top_n)
print(top_coef_summary)
```

#### Feature selection for model 10b
```{r}
RFE <- sklearn$feature_selection$RFE

n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results_rfe <- data.frame()
best_params_list_rfe <- list()
outer_scores_rfe <- c()

c_values <- c(0.01, 0.1, 1, 10, 30)

for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]

  n_train_samples <- length(train_idx)
  set.seed(42 + outer_fold)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  best_features_mask <- NULL
  
  for (c_val in c_values) {
    cv_scores <- c()
    
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]

      base_model <- SVR(C = c_val, kernel = 'linear')
      rfe_model <- RFE(estimator = base_model, n_features_to_select = 5L)
      rfe_model$fit(X_inner_train, y_inner_train)

      selected_mask <- rfe_model$support_
      selected_indices <- which(py_to_r(selected_mask))
      
      X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
      X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]

      model_selected <- SVR(C = c_val, kernel = 'linear')
      model_selected$fit(X_inner_train_selected, y_inner_train)
      y_pred_val <- model_selected$predict(X_inner_val_selected)
      
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
      cv_scores <- c(cv_scores, neg_mse)
    }

    mean_score <- mean(cv_scores)
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
      # Store best mask based on entire training data
      final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'), n_features_to_select = 5L)
      final_rfe$fit(X_train_outer, y_train_outer)
      best_features_mask <- final_rfe$support_
    }
  }

  best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
  cat("  Best C for fold", outer_fold, ":", best_c, "\n")

  X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
  X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]

  final_model <- SVR(C = best_c, kernel = 'linear')
  final_model$fit(X_train_outer_selected, y_train_outer)
  y_pred_outer <- final_model$predict(X_test_outer_selected)

  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)

  outer_scores_rfe <- c(outer_scores_rfe, r2)

  nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results with RFE:\n")
print(nested_cv_results_rfe)
cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")

cat("\nBest Parameters and Features per Fold:\n")
for (i in 1:length(best_params_list_rfe)) {
  cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
  print(colnames(X)[best_params_list_rfe[[i]]$mask])
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# Apply RFE to full dataset using best C
final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'), n_features_to_select = 5L)
final_rfe$fit(X_scaledSVM, y)
selected_mask_final <- final_rfe$support_

# Subset data to selected features
X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]

# Train final model with selected features
final_svr <- SVR(C = most_common_c, kernel = 'linear')
final_svr$fit(X_selected_final, y)

# Use X_selected_final (only features chosen by RFE) for cross-validation
mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model10b_results <- rbind(model10b_results, data.frame(
  Model = "10b",
  Method = "Feature_Elimination",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model10b_results_normalized <- rbind(model10b_results_normalized, data.frame(
  Model = "10b",
  Method = "Feature_Elimination",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


final_y_pred <- final_svr$predict(X_selected_final)

# Get coefficients
coefficients <- final_svr$coef_
intercept <- final_svr$intercept_

# Use feature names of selected features only
feature_names <- colnames(X)[selected_mask_final]
coef_vector <- py_to_r(coefficients)[1, ]

coef_df <- data.frame(
  Feature = feature_names,
  Coefficient = coef_vector
)

print(coef_df)

top_n <- 20
ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Predictor weights from feature model 10b (with feature elimination)",
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)

```

#### Feature selection for model 10b (max 1 feature)
```{r}
# RFE <- sklearn$feature_selection$RFE
# 
# n_samples <- nrow(X_scaledSVM)
# n_outer_folds <- 5
# n_inner_folds <- 5
# 
# set.seed(42)
# outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))
# 
# nested_cv_results_rfe <- data.frame()
# best_params_list_rfe <- list()
# outer_scores_rfe <- c()
# 
# c_values <- c(0.01, 0.1, 1, 10, 30)
# 
# for (outer_fold in 1:n_outer_folds) {
#   cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
#   
#   test_idx <- which(outer_fold_indices == outer_fold)
#   train_idx <- which(outer_fold_indices != outer_fold)
#   
#   X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
#   y_train_outer <- y[train_idx]
#   X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
#   y_test_outer <- y[test_idx]
# 
#   n_train_samples <- length(train_idx)
#   set.seed(42 + outer_fold)
#   inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
#   
#   best_c <- NULL
#   best_score <- -Inf
#   best_features_mask <- NULL
#   
#   for (c_val in c_values) {
#     cv_scores <- c()
#     
#     for (inner_fold in 1:n_inner_folds) {
#       inner_val_idx <- which(inner_fold_indices == inner_fold)
#       inner_train_idx <- which(inner_fold_indices != inner_fold)
#       
#       X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
#       y_inner_train <- y_train_outer[inner_train_idx]
#       X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
#       y_inner_val <- y_train_outer[inner_val_idx]
# 
#       base_model <- SVR(C = c_val, kernel = 'linear')
#       rfe_model <- RFE(estimator = base_model, n_features_to_select = 1L)
#       rfe_model$fit(X_inner_train, y_inner_train)
# 
#       selected_mask <- rfe_model$support_
#       selected_indices <- which(py_to_r(selected_mask))
#       
#       X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
#       X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]
# 
#       model_selected <- SVR(C = c_val, kernel = 'linear')
#       model_selected$fit(X_inner_train_selected, y_inner_train)
#       y_pred_val <- model_selected$predict(X_inner_val_selected)
#       
#       neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
#       cv_scores <- c(cv_scores, neg_mse)
#     }
# 
#     mean_score <- mean(cv_scores)
#     if (mean_score > best_score) {
#       best_score <- mean_score
#       best_c <- c_val
#       # Store best mask based on entire training data
#       final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'), n_features_to_select = 1L)
#       final_rfe$fit(X_train_outer, y_train_outer)
#       best_features_mask <- final_rfe$support_
#     }
#   }
# 
#   best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
#   cat("  Best C for fold", outer_fold, ":", best_c, "\n")
# 
#   X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
#   X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]
# 
#   final_model <- SVR(C = best_c, kernel = 'linear')
#   final_model$fit(X_train_outer_selected, y_train_outer)
#   y_pred_outer <- final_model$predict(X_test_outer_selected)
# 
#   mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
#   rmse <- sqrt(mse)
#   mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
#   r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
# 
#   outer_scores_rfe <- c(outer_scores_rfe, r2)
# 
#   nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
#     Fold = outer_fold,
#     Best_C = best_c,
#     MSE = mse,
#     RMSE = rmse,
#     MAE = mae,
#     R2 = r2
#   ))
#   
#   cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
# }
# 
# cat("\nNested CV Results with RFE:\n")
# print(nested_cv_results_rfe)
# cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")
# 
# cat("\nBest Parameters and Features per Fold:\n")
# for (i in 1:length(best_params_list_rfe)) {
#   cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
#   print(colnames(X)[best_params_list_rfe[[i]]$mask])
# }
# 
# c_values <- sapply(best_params_list, function(x) x$C)
# c_table <- table(c_values)
# most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
# 
# # Apply RFE to full dataset using best C
# final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'), n_features_to_select = 1L)
# final_rfe$fit(X_scaledSVM, y)
# selected_mask_final <- final_rfe$support_
# 
# # Subset data to selected features
# X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]
# 
# # Train final model with selected features
# final_svr <- SVR(C = most_common_c, kernel = 'linear')
# final_svr$fit(X_selected_final, y)
# 
# # Use X_selected_final (only features chosen by RFE) for cross-validation
# mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
# mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
# r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')
# 
# mse_scores_r <- py_to_r(mse_scores)
# mae_scores_r <- py_to_r(mae_scores)
# r2_scores_r <- py_to_r(r2_scores)
# 
# mse_mean <- -mean(mse_scores_r) 
# mse_std <- sd(mse_scores_r)
# rmse_mean <- sqrt(mse_mean)  
# rmse_std <- sqrt(mse_std)
# mae_mean <- -mean(mae_scores_r)
# mae_std <- sd(mae_scores_r)
# r2_mean <- mean(r2_scores_r)
# r2_std <- sd(r2_scores_r)
# 
# cat("SVR model with nested CV, best C =", most_common_c, "\n")
# cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
# cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
# cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
# cat("R² Score:", r2_mean, "±", r2_std, "\n")
# cat("\n")
# 
# model10b_results <- rbind(model10b_results, data.frame(
#   Model = "10b",
#   Method = "Feature_Elimination 1 feature max",
#   Outcome = "mansa_totaal.2",
#   Hyperparameter_value = paste0("C = ", most_common_c),
#   MSE_mean = mse_mean,
#   MSE_std = mse_std,
#   RMSE_mean = rmse_mean,
#   RMSE_std = rmse_std,
#   MAE_mean = mae_mean,
#   MAE_std = mae_std,
#   R2_mean = r2_mean,
#   R2_std = r2_std
# ))
# 
# 
# model10b_results_normalized <- rbind(model10b_results_normalized, data.frame(
#   Model = "10b",
#   Method = "Feature_Elimination 1 feature max",
#   Outcome = "mansa_totaal.2",
#   Hyperparameter_value = paste0("C = ", most_common_c),
#   MSE_mean = mse_mean / mse_mean_baseline,
#   MSE_std = mse_std / mse_mean_baseline,
#   RMSE_mean = rmse_mean / rmse_mean_baseline,
#   RMSE_std = rmse_std / rmse_mean_baseline,
#   MAE_mean = mae_mean / mae_mean_baseline,
#   MAE_std =  mae_std / mae_mean_baseline,
#   R2_mean = r2_mean / r2_mean_baseline,
#   R2_std = r2_std / r2_mean_baseline
# ))
# 
# 
# # Don't refit - use the already fitted model on selected features
# final_y_pred <- final_svr$predict(X_selected_final)
# 
# # Get coefficients
# coefficients <- final_svr$coef_
# intercept <- final_svr$intercept_
# 
# # Use feature names of selected features only
# feature_names <- colnames(X)[selected_mask_final]
# coef_vector <- py_to_r(coefficients)[1, ]
# 
# coef_df <- data.frame(
#   Feature = feature_names,
#   Coefficient = coef_vector
# )
# 
# print(coef_df)
# 
# top_n <- 20
# ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
#   geom_col(fill = "steelblue") +
#   coord_flip() +
#   labs(title = "Predictor weights from feature model 10b (with feature elimination)",
#        x = "Predictor", y = "Coefficient") +
#   theme_minimal()
# 
# results_df <- data.frame(
#   Actual = y,
#   Predicted = final_y_pred
# )
# 
# final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
#   geom_point(color = 'blue', alpha = 0.5) +
#   geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
#   labs(
#     title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
#     x = "Actual Outcome",
#     y = "Predicted Outcome"
#   ) +
#   theme_minimal()
# 
# print(final_plot)

```

#### Feature selection for model 10b (RFE chooses how many features)
```{r}
# RFE <- sklearn$feature_selection$RFE
# 
# n_samples <- nrow(X_scaledSVM)
# n_outer_folds <- 5
# n_inner_folds <- 5
# 
# set.seed(42)
# outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))
# 
# nested_cv_results_rfe <- data.frame()
# best_params_list_rfe <- list()
# outer_scores_rfe <- c()
# 
# c_values <- c(0.01, 0.1, 1, 10, 30)
# 
# for (outer_fold in 1:n_outer_folds) {
#   cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
#   
#   test_idx <- which(outer_fold_indices == outer_fold)
#   train_idx <- which(outer_fold_indices != outer_fold)
#   
#   X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
#   y_train_outer <- y[train_idx]
#   X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
#   y_test_outer <- y[test_idx]
# 
#   n_train_samples <- length(train_idx)
#   set.seed(42 + outer_fold)
#   inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
#   
#   best_c <- NULL
#   best_score <- -Inf
#   best_features_mask <- NULL
#   
#   for (c_val in c_values) {
#     cv_scores <- c()
#     
#     for (inner_fold in 1:n_inner_folds) {
#       inner_val_idx <- which(inner_fold_indices == inner_fold)
#       inner_train_idx <- which(inner_fold_indices != inner_fold)
#       
#       X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
#       y_inner_train <- y_train_outer[inner_train_idx]
#       X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
#       y_inner_val <- y_train_outer[inner_val_idx]
# 
#       base_model <- SVR(C = c_val, kernel = 'linear')
#       rfe_model <- RFE(estimator = base_model)
#       rfe_model$fit(X_inner_train, y_inner_train)
# 
#       selected_mask <- rfe_model$support_
#       selected_indices <- which(py_to_r(selected_mask))
#       
#       X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
#       X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]
# 
#       model_selected <- SVR(C = c_val, kernel = 'linear')
#       model_selected$fit(X_inner_train_selected, y_inner_train)
#       y_pred_val <- model_selected$predict(X_inner_val_selected)
#       
#       neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
#       cv_scores <- c(cv_scores, neg_mse)
#     }
# 
#     mean_score <- mean(cv_scores)
#     if (mean_score > best_score) {
#       best_score <- mean_score
#       best_c <- c_val
#       # Store best mask based on entire training data
#       final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'))
#       final_rfe$fit(X_train_outer, y_train_outer)
#       best_features_mask <- final_rfe$support_
#     }
#   }
# 
#   best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
#   cat("  Best C for fold", outer_fold, ":", best_c, "\n")
# 
#   X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
#   X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]
# 
#   final_model <- SVR(C = best_c, kernel = 'linear')
#   final_model$fit(X_train_outer_selected, y_train_outer)
#   y_pred_outer <- final_model$predict(X_test_outer_selected)
# 
#   mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
#   rmse <- sqrt(mse)
#   mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
#   r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
# 
#   outer_scores_rfe <- c(outer_scores_rfe, r2)
# 
#   nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
#     Fold = outer_fold,
#     Best_C = best_c,
#     MSE = mse,
#     RMSE = rmse,
#     MAE = mae,
#     R2 = r2
#   ))
#   
#   cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
# }
# 
# cat("\nNested CV Results with RFE:\n")
# print(nested_cv_results_rfe)
# cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")
# 
# cat("\nBest Parameters and Features per Fold:\n")
# for (i in 1:length(best_params_list_rfe)) {
#   cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
#   print(colnames(X)[best_params_list_rfe[[i]]$mask])
# }
# 
# c_values <- sapply(best_params_list, function(x) x$C)
# c_table <- table(c_values)
# most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
# 
# # Apply RFE to full dataset using best C
# final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'))
# final_rfe$fit(X_scaledSVM, y)
# selected_mask_final <- final_rfe$support_
# 
# # Subset data to selected features
# X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]
# 
# # Train final model with selected features
# final_svr <- SVR(C = most_common_c, kernel = 'linear')
# final_svr$fit(X_selected_final, y)
# 
# # Use X_selected_final (only features chosen by RFE) for cross-validation
# mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
# mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
# r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')
# 
# mse_scores_r <- py_to_r(mse_scores)
# mae_scores_r <- py_to_r(mae_scores)
# r2_scores_r <- py_to_r(r2_scores)
# 
# mse_mean <- -mean(mse_scores_r) 
# mse_std <- sd(mse_scores_r)
# rmse_mean <- sqrt(mse_mean)  
# rmse_std <- sqrt(mse_std)
# mae_mean <- -mean(mae_scores_r)
# mae_std <- sd(mae_scores_r)
# r2_mean <- mean(r2_scores_r)
# r2_std <- sd(r2_scores_r)
# 
# cat("SVR model with nested CV, best C =", most_common_c, "\n")
# cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
# cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
# cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
# cat("R² Score:", r2_mean, "±", r2_std, "\n")
# cat("\n")
# 
# model10b_results <- rbind(model10b_results, data.frame(
#   Model = "10b",
#   Method = "Feature_Elimination 1 feature max",
#   Outcome = "mansa_totaal.2",
#   Hyperparameter_value = paste0("C = ", most_common_c),
#   MSE_mean = mse_mean,
#   MSE_std = mse_std,
#   RMSE_mean = rmse_mean,
#   RMSE_std = rmse_std,
#   MAE_mean = mae_mean,
#   MAE_std = mae_std,
#   R2_mean = r2_mean,
#   R2_std = r2_std
# ))
# 
# 
# model10b_results_normalized <- rbind(model10b_results_normalized, data.frame(
#   Model = "10b",
#   Method = "Feature_Elimination 1 feature max",
#   Outcome = "mansa_totaal.2",
#   Hyperparameter_value = paste0("C = ", most_common_c),
#   MSE_mean = mse_mean / mse_mean_baseline,
#   MSE_std = mse_std / mse_mean_baseline,
#   RMSE_mean = rmse_mean / rmse_mean_baseline,
#   RMSE_std = rmse_std / rmse_mean_baseline,
#   MAE_mean = mae_mean / mae_mean_baseline,
#   MAE_std =  mae_std / mae_mean_baseline,
#   R2_mean = r2_mean / r2_mean_baseline,
#   R2_std = r2_std / r2_mean_baseline
# ))
# 
# 
# # Don't refit - use the already fitted model on selected features
# final_y_pred <- final_svr$predict(X_selected_final)
# 
# # Get coefficients
# coefficients <- final_svr$coef_
# intercept <- final_svr$intercept_
# 
# # Use feature names of selected features only
# feature_names <- colnames(X)[selected_mask_final]
# coef_vector <- py_to_r(coefficients)[1, ]
# 
# coef_df <- data.frame(
#   Feature = feature_names,
#   Coefficient = coef_vector
# )
# 
# print(coef_df)
# 
# top_n <- 20
# ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
#   geom_col(fill = "steelblue") +
#   coord_flip() +
#   labs(title = "Predictor weights from feature model 10b (with feature elimination)",
#        x = "Predictor", y = "Coefficient") +
#   theme_minimal()
# 
# results_df <- data.frame(
#   Actual = y,
#   Predicted = final_y_pred
# )
# 
# final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
#   geom_point(color = 'blue', alpha = 0.5) +
#   geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
#   labs(
#     title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
#     x = "Actual Outcome",
#     y = "Predicted Outcome"
#   ) +
#   theme_minimal()
# 
# print(final_plot)

```

### model 10c: outcome Honos with set of predictors + baseline Honos 
```{r}
# I filter the data to only include non-missing values of outcome honos_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel10c <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, honos_totaal.2, opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(honos_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
model10c_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model10c_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM10c <- colSums(is.na(dataModel10c)) / nrow(dataModel10c) * 100
print(missing_percentageM10c)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel10c))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel10c)
```

#### imputing data 
```{r}
methods <- make.method(dataModel10c)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["opleiding_nieuw"] <- "pmm" # numeric 
methods["leefsituatie_nieuw"] <- "logreg" # binary 
methods["MANSA_PH_7.1"] <- "logreg" # binary 
methods["MANSA_PH_9.1"] <- "logreg" # binary 
methods["MANSA_PH_10.1"] <- "logreg" # binary 
methods["MANSA_PH_11.1"] <- "logreg" # binary 
methods["month_diff_1_and_2"] <- ""


# doing the imputation
imputed_dataModel10c <- mice(dataModel10c, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel10c)
```

```{r}
dataModel10c_complete <- complete(imputed_dataModel10c,1)
```

#### Selecting features and outcome 
```{r}
X <- dataModel10c_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "FR_totaal.1", "Inspire_totaal.1", "opleiding_nieuw", "burgerlijkestaat_nieuw", "leefsituatie_nieuw", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1")]
y <- dataModel10c_complete$honos_totaal.2
# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### check for multicolinearity 
```{r}
vif(lm(honos_totaal.2 ~ Age + geslacht_GegevensAfname + modusmeanGGZ + levenspartner.1 + betaaldwerk.1 + mansa_totaal.1 + honos_totaal.1 + FR_totaal.1 + Inspire_totaal.1, data = dataModel10c_complete))
```

#### baseline model that uses mansa_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel10c_complete[, c("honos_totaal.1")] 
y_baseline <- dataModel10c_complete$honos_totaal.2  

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"

cat("Baseline model using mansa_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

```
#### Nested cross validation (5x repeated)
```{r}
X_scaledSVM <- scalerSVM$fit_transform(X) 
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

# Saving coefficients and performances 
all_coefficients <- list()
all_performance <- list()

for (repeat_loop in 1:5) {
  # Setting a seed for reproducability 
  set.seed(42 + repeat_loop)
  cat("\n\n========== REPETITION", repeat_loop, "==========\n")
  
  # Creating a different data split for each repetition
  split_indices <- sample(1:n_samples)
  training_size <- floor(0.8 * n_samples)
  
  X_rep <- X_scaledSVM[split_indices[1:training_size], , drop=FALSE]
  y_rep <- y[split_indices[1:training_size]]
  
  cat("Repetition", repeat_loop, "training data dimensions:", dim(X_rep)[1], "x", dim(X_rep)[2], "\n")
  
  outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = nrow(X_rep)))

  nested_cv_results <- data.frame()
  best_params_list <- list()
  outer_scores <- c()

  c_values <- c(0.01, 0.1, 1, 10, 30)
  
  # Outer CV loop
  for (outer_fold in 1:n_outer_folds) {
    cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
    
    test_idx <- which(outer_fold_indices == outer_fold)
    train_idx <- which(outer_fold_indices != outer_fold)
    
    X_train_outer <- X_rep[train_idx, , drop=FALSE]
    y_train_outer <- y_rep[train_idx]
    X_test_outer <- X_rep[test_idx, , drop=FALSE]
    y_test_outer <- y_rep[test_idx]
    
    n_train_samples <- length(train_idx)
    
    set.seed(100 + (repeat_loop * 10) + outer_fold)
    inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
    
    best_c <- NULL
    best_score <- -Inf
    
    for (c_val in c_values) {
      cv_scores <- c()
  
      # Inner loop 
      for (inner_fold in 1:n_inner_folds) {
        inner_val_idx <- which(inner_fold_indices == inner_fold)
        inner_train_idx <- which(inner_fold_indices != inner_fold)
        
        X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
        y_inner_train <- y_train_outer[inner_train_idx]
        X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
        y_inner_val <- y_train_outer[inner_val_idx]
        
        if (inner_fold == 1) {  
          cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
              "rows, y:", length(y_inner_train), "elements\n")
          cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
              "rows, y:", length(y_inner_val), "elements\n")
        }
        
        inner_svr <- SVR(C = c_val, kernel = 'linear')
        inner_svr$fit(X_inner_train, y_inner_train)
        
        y_pred_inner <- inner_svr$predict(X_inner_val)
        neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
        cv_scores <- c(cv_scores, neg_mse)
      }
      
      mean_score <- mean(cv_scores)
      
      if (mean_score > best_score) {
        best_score <- mean_score
        best_c <- c_val
      }
    }
    
    cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
    
    best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
    
    best_svr <- SVR(C = best_c, kernel = 'linear')
    best_svr$fit(X_train_outer, y_train_outer)
    
    y_pred_outer <- best_svr$predict(X_test_outer)
    
    mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
    rmse <- sqrt(mse)
    mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
    r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
    
    outer_scores <- c(outer_scores, r2)
    
    nested_cv_results <- rbind(nested_cv_results, data.frame(
      Fold = outer_fold,
      Best_C = best_c,
      MSE = mse,
      RMSE = rmse,
      MAE = mae,
      R2 = r2
    ))
    
    cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
  }
  
  cat("\nNested CV Results:\n")
  print(nested_cv_results)
  cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")
  
  cat("\nBest Parameters per Fold:\n")
  for (i in 1:length(best_params_list)) {
    cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
  }
  
  c_values <- sapply(best_params_list, function(x) x$C)
  c_table <- table(c_values)
  most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
  
  rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=repeat_loop)
  
  final_svr <- SVR(C = most_common_c, kernel = 'linear')
  
  mse_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='r2')
  
  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)
  
  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)
  
  cat("SVR model with nested CV, best C =", most_common_c, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  # Saving performance metrics for this run
  all_performance[[repeat_loop]] <- data.frame(
    Run = repeat_loop,
    MSE = mse_mean,
    RMSE = rmse_mean,
    MAE = mae_mean,
    R2 = r2_mean,
    C = most_common_c
  )
  
  model10c_results <- rbind(model10c_results, data.frame(
    Model = "10c",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))
  
  
  model10c_results_normalized <- rbind(model10c_results_normalized, data.frame(
    Model = "10c",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
  
  final_svr$fit(X_rep, y_rep)
  final_y_pred <- final_svr$predict(X_rep)
  
  coefficients <- final_svr$coef_
  intercept <- final_svr$intercept_
  
  coef_vector <- py_to_r(coefficients)[1, ]  # flatten the 2D array to a 1D vector
  intercept <- py_to_r(intercept)[1]
  
  # saving coefficients for this run
  all_coefficients[[repeat_loop]] <- coef_vector
  
  feature_names <- colnames(X)
  coef_df <- data.frame(
    Feature = feature_names,
    Coefficient = coef_vector
  )
  
  coef_df <- coef_df %>% arrange(desc(abs(Coefficient)))
  
  top_n <- 20
  print(
  ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = paste("Predictor weights from model 10c (run", repeat_loop, ")"),
         x = "Predictor", y = "Coefficient") +
    theme_minimal()
  )
  
  cat("\nTop coefficients for run", repeat_loop, ":\n")
  print(head(coef_df, top_n))
  
  results_df <- data.frame(
    Actual = y_rep,
    Predicted = final_y_pred
  )
  
  final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
    labs(
      title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(final_plot)
  
  cat("Most common C for run", repeat_loop, ":", most_common_c, "\n")
}

# Calculating mean and SD of performance metrics
performance_df <- do.call(rbind, all_performance)
mean_performance <- colMeans(performance_df[, c("MSE", "RMSE", "MAE", "R2")])
sd_performance <- apply(performance_df[, c("MSE", "RMSE", "MAE", "R2")], 2, sd)

cat("MSE:", mean_performance["MSE"], "±", sd_performance["MSE"], "\n")
cat("RMSE:", mean_performance["RMSE"], "±", sd_performance["RMSE"], "\n")
cat("MAE:", mean_performance["MAE"], "±", sd_performance["MAE"], "\n")
cat("R²:", mean_performance["R2"], "±", sd_performance["R2"], "\n")
cat("Most common C value:", names(sort(table(performance_df$C), decreasing = TRUE)[1]), "\n")

model10c_results <- rbind(model10c_results, data.frame(
  Model = "10c",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"],
  MSE_std = sd_performance["MSE"],
  RMSE_mean = mean_performance["RMSE"],
  RMSE_std = sd_performance["RMSE"],
  MAE_mean = mean_performance["MAE"],
  MAE_std = sd_performance["MAE"],
  R2_mean = mean_performance["R2"],
  R2_std = sd_performance["R2"]
))


model10c_results_normalized <- rbind(model10c_results_normalized, data.frame(
  Model = "10c",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"] / mse_mean_baseline,
  MSE_std = sd_performance["MSE"] / mse_mean_baseline,
  RMSE_mean = mean_performance["RMSE"] / rmse_mean_baseline,
  RMSE_std = sd_performance["RMSE"] / rmse_mean_baseline,
  MAE_mean = mean_performance["MAE"] / mae_mean_baseline,
  MAE_std =  sd_performance["MAE"] / mae_mean_baseline,
  R2_mean = mean_performance["R2"] / r2_mean_baseline,
  R2_std = sd_performance["R2"] / r2_mean_baseline
))

coef_matrix <- do.call(cbind, all_coefficients)
coef_means <- rowMeans(coef_matrix)
coef_sds <- apply(coef_matrix, 1, sd)

coef_summary <- data.frame(
  Feature = feature_names,
  Mean_Coefficient = coef_means,
  SD_Coefficient = coef_sds
)

coef_summary <- coef_summary %>% arrange(desc(abs(Mean_Coefficient)))

coef_long <- data.frame()
for (i in 1:length(all_coefficients)) {
  temp_df <- data.frame(
    Feature = feature_names,
    Coefficient = all_coefficients[[i]],
    Run = paste("Run", i)
  )
  coef_long <- rbind(coef_long, temp_df)
}

top_n <- 20
top_features <- head(coef_summary, top_n)$Feature

coef_long_top <- coef_long %>% filter(Feature %in% top_features)

# Creating a plot with mean ± SD
mean_sd_plot <- ggplot(head(coef_summary, top_n), 
                     aes(x = reorder(Feature, Mean_Coefficient), y = Mean_Coefficient)) +
  geom_col(fill = "steelblue") +
  geom_errorbar(aes(ymin = Mean_Coefficient - SD_Coefficient, 
                  ymax = Mean_Coefficient + SD_Coefficient), width = 0.2) +
  coord_flip() +
  labs(title = "Mean predictor weights across 5 runs",
       subtitle = paste("Error bars show ± 1 SD"),
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

print(mean_sd_plot)

# Creating a plot showing individual points for each run
individual_plot <- ggplot(coef_long_top, 
                        aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient, color = Run)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Individual predictor weights from all 5 runs",
       x = "Predictor", y = "Coefficient") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(individual_plot)

# Creating a plot showing performance metrics across runs
performance_long <- performance_df %>%
  pivot_longer(cols = c("MSE", "RMSE", "MAE", "R2"), 
               names_to = "Metric", values_to = "Value")

performance_plot <- ggplot(performance_long, 
                         aes(x = as.factor(Run), y = Value, group = Metric, color = Metric)) +
  geom_line() +
  geom_point(size = 3) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(title = "Model performance across 5 runs",
       x = "Run", y = "Value") +
  theme_minimal()

print(performance_plot)

# Summary table of coefficients
top_coef_summary <- head(coef_summary, top_n)
print(top_coef_summary)
```

#### Feature selection for model 10c
```{r}
RFE <- sklearn$feature_selection$RFE

n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results_rfe <- data.frame()
best_params_list_rfe <- list()
outer_scores_rfe <- c()

c_values <- c(0.01, 0.1, 1, 10, 30)

for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]

  n_train_samples <- length(train_idx)
  set.seed(42 + outer_fold)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  best_features_mask <- NULL
  
  for (c_val in c_values) {
    cv_scores <- c()
    
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]

      base_model <- SVR(C = c_val, kernel = 'linear')
      rfe_model <- RFE(estimator = base_model, n_features_to_select = 5L)
      rfe_model$fit(X_inner_train, y_inner_train)

      selected_mask <- rfe_model$support_
      selected_indices <- which(py_to_r(selected_mask))
      
      X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
      X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]

      model_selected <- SVR(C = c_val, kernel = 'linear')
      model_selected$fit(X_inner_train_selected, y_inner_train)
      y_pred_val <- model_selected$predict(X_inner_val_selected)
      
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
      cv_scores <- c(cv_scores, neg_mse)
    }

    mean_score <- mean(cv_scores)
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
      # Store best mask based on entire training data
      final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'), n_features_to_select = 5L)
      final_rfe$fit(X_train_outer, y_train_outer)
      best_features_mask <- final_rfe$support_
    }
  }

  best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
  cat("  Best C for fold", outer_fold, ":", best_c, "\n")

  X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
  X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]

  final_model <- SVR(C = best_c, kernel = 'linear')
  final_model$fit(X_train_outer_selected, y_train_outer)
  y_pred_outer <- final_model$predict(X_test_outer_selected)

  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)

  outer_scores_rfe <- c(outer_scores_rfe, r2)

  nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results with RFE:\n")
print(nested_cv_results_rfe)
cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")

cat("\nBest Parameters and Features per Fold:\n")
for (i in 1:length(best_params_list_rfe)) {
  cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
  print(colnames(X)[best_params_list_rfe[[i]]$mask])
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# Apply RFE to full dataset using best C
final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'), n_features_to_select = 5L)
final_rfe$fit(X_scaledSVM, y)
selected_mask_final <- final_rfe$support_

# Subset data to selected features
X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]

# Train final model with selected features
final_svr <- SVR(C = most_common_c, kernel = 'linear')
final_svr$fit(X_selected_final, y)

# Use X_selected_final (only features chosen by RFE) for cross-validation
mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model10c_results <- rbind(model10c_results, data.frame(
  Model = "10c",
  Method = "Feature_Elimination",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model10c_results_normalized <- rbind(model10c_results_normalized, data.frame(
  Model = "10c",
  Method = "Feature_Elimination",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


# Don't refit - use the already fitted model on selected features
final_y_pred <- final_svr$predict(X_selected_final)

# Get coefficients
coefficients <- final_svr$coef_
intercept <- final_svr$intercept_

# Use feature names of selected features only
feature_names <- colnames(X)[selected_mask_final]
coef_vector <- py_to_r(coefficients)[1, ]

coef_df <- data.frame(
  Feature = feature_names,
  Coefficient = coef_vector
)

print(coef_df)

top_n <- 20
ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Predictor weights from feature model 10c (with feature elimination)",
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)

```



### model 10d: outcome FR with set of predictors + baseline FR 
```{r}
# for this model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1, honos_totaal.1, FR_totaal.1. I removed Inspire_totaal.1 because it has a large amount of missing values 
# and the following outcome: FR_totaal.2
# I filter the data to only include non-missing values of outcome FR_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel10d <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, FR_totaal.2, opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(FR_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
model10d_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model10d_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM10d <- colSums(is.na(dataModel10d)) / nrow(dataModel10d) * 100
print(missing_percentageM10d)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel10d))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel10d)
```

#### imputing data 
```{r}
methods <- make.method(dataModel10d)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["opleiding_nieuw"] <- "pmm" # numeric 
methods["leefsituatie_nieuw"] <- "logreg" # binary 
methods["MANSA_PH_7.1"] <- "logreg" # binary 
methods["MANSA_PH_9.1"] <- "logreg" # binary 
methods["MANSA_PH_10.1"] <- "logreg" # binary 
methods["MANSA_PH_11.1"] <- "logreg" # binary 
methods["month_diff_1_and_2"] <- ""
methods["FR_totaal.2"] <- "" 


# doing the imputation
imputed_dataModel10d <- mice(dataModel10d, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel10d)
```

```{r}
dataModel10d_complete <- complete(imputed_dataModel10d,1)
```

#### Selecting features and outcome 
```{r}
X <- dataModel10d_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "FR_totaal.1", "Inspire_totaal.1", "opleiding_nieuw", "burgerlijkestaat_nieuw", "leefsituatie_nieuw", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1")]
y <- dataModel10d_complete$FR_totaal.2
# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### baseline model that uses FR_totaal.1 input directly as outcome 
```{r}

X_baseline <- dataModel10d_complete[, c("FR_totaal.1")] 
y_baseline <- dataModel10d_complete$FR_totaal.2  

y_pred_baseline <- X_baseline  

mse_mean_baseline <- metrics$mean_squared_error(y_baseline, y_pred_baseline)
mse_std_baseline <- "-"
rmse_mean_baseline <- sqrt(mse_mean)
rmse_std_baseline <- "-"
mae_mean_baseline <- metrics$mean_absolute_error(y_baseline, y_pred_baseline)
mae_std_baseline <- "-"
r2_mean_baseline <- metrics$r2_score(y_baseline, y_pred_baseline)
r2_std_baseline <- "-"

cat("Baseline model using FR_totaal.1", "\n")
cat("Mean Squared Error (MSE):", mse_mean_baseline, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean_baseline, "\n")
cat("Mean Absolute Error (MAE):", mae_mean_baseline, "\n")
cat("R² Score:", r2_mean_baseline, "\n")
cat("\n")

```
#### Nested cross validation (5x repeated)
```{r}
X_scaledSVM <- scalerSVM$fit_transform(X) 
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

# Saving coefficients and performances 
all_coefficients <- list()
all_performance <- list()

for (repeat_loop in 1:5) {
  # Setting a seed for reproducability 
  set.seed(42 + repeat_loop)
  cat("\n\n========== REPETITION", repeat_loop, "==========\n")
  
  # Creating a different data split for each repetition
  split_indices <- sample(1:n_samples)
  training_size <- floor(0.8 * n_samples)
  
  X_rep <- X_scaledSVM[split_indices[1:training_size], , drop=FALSE]
  y_rep <- y[split_indices[1:training_size]]
  
  cat("Repetition", repeat_loop, "training data dimensions:", dim(X_rep)[1], "x", dim(X_rep)[2], "\n")
  
  outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = nrow(X_rep)))

  nested_cv_results <- data.frame()
  best_params_list <- list()
  outer_scores <- c()

  c_values <- c(0.01, 0.1, 1, 10, 30)
  
  # Outer CV loop
  for (outer_fold in 1:n_outer_folds) {
    cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
    
    test_idx <- which(outer_fold_indices == outer_fold)
    train_idx <- which(outer_fold_indices != outer_fold)
    
    X_train_outer <- X_rep[train_idx, , drop=FALSE]
    y_train_outer <- y_rep[train_idx]
    X_test_outer <- X_rep[test_idx, , drop=FALSE]
    y_test_outer <- y_rep[test_idx]
    
    n_train_samples <- length(train_idx)
    
    set.seed(100 + (repeat_loop * 10) + outer_fold)
    inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
    
    best_c <- NULL
    best_score <- -Inf
    
    for (c_val in c_values) {
      cv_scores <- c()
  
      # Inner loop 
      for (inner_fold in 1:n_inner_folds) {
        inner_val_idx <- which(inner_fold_indices == inner_fold)
        inner_train_idx <- which(inner_fold_indices != inner_fold)
        
        X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
        y_inner_train <- y_train_outer[inner_train_idx]
        X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
        y_inner_val <- y_train_outer[inner_val_idx]
        
        if (inner_fold == 1) {  
          cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
              "rows, y:", length(y_inner_train), "elements\n")
          cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
              "rows, y:", length(y_inner_val), "elements\n")
        }
        
        inner_svr <- SVR(C = c_val, kernel = 'linear')
        inner_svr$fit(X_inner_train, y_inner_train)
        
        y_pred_inner <- inner_svr$predict(X_inner_val)
        neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
        cv_scores <- c(cv_scores, neg_mse)
      }
      
      mean_score <- mean(cv_scores)
      
      if (mean_score > best_score) {
        best_score <- mean_score
        best_c <- c_val
      }
    }
    
    cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
    
    best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
    
    best_svr <- SVR(C = best_c, kernel = 'linear')
    best_svr$fit(X_train_outer, y_train_outer)
    
    y_pred_outer <- best_svr$predict(X_test_outer)
    
    mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
    rmse <- sqrt(mse)
    mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
    r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
    
    outer_scores <- c(outer_scores, r2)
    
    nested_cv_results <- rbind(nested_cv_results, data.frame(
      Fold = outer_fold,
      Best_C = best_c,
      MSE = mse,
      RMSE = rmse,
      MAE = mae,
      R2 = r2
    ))
    
    cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
  }
  
  cat("\nNested CV Results:\n")
  print(nested_cv_results)
  cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")
  
  cat("\nBest Parameters per Fold:\n")
  for (i in 1:length(best_params_list)) {
    cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
  }
  
  c_values <- sapply(best_params_list, function(x) x$C)
  c_table <- table(c_values)
  most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
  
  rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=repeat_loop)
  
  final_svr <- SVR(C = most_common_c, kernel = 'linear')
  
  mse_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='r2')
  
  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)
  
  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)
  
  cat("SVR model with nested CV, best C =", most_common_c, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  # Saving performance metrics for this run
  all_performance[[repeat_loop]] <- data.frame(
    Run = repeat_loop,
    MSE = mse_mean,
    RMSE = rmse_mean,
    MAE = mae_mean,
    R2 = r2_mean,
    C = most_common_c
  )
  
  model10d_results <- rbind(model10d_results, data.frame(
    Model = "10d",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))
  
  
  model10d_results_normalized <- rbind(model10d_results_normalized, data.frame(
    Model = "10d",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
  
  final_svr$fit(X_rep, y_rep)
  final_y_pred <- final_svr$predict(X_rep)
  
  coefficients <- final_svr$coef_
  intercept <- final_svr$intercept_
  
  coef_vector <- py_to_r(coefficients)[1, ]  # flatten the 2D array to a 1D vector
  intercept <- py_to_r(intercept)[1]
  
  # saving coefficients for this run
  all_coefficients[[repeat_loop]] <- coef_vector
  
  feature_names <- colnames(X)
  coef_df <- data.frame(
    Feature = feature_names,
    Coefficient = coef_vector
  )
  
  coef_df <- coef_df %>% arrange(desc(abs(Coefficient)))
  
  top_n <- 20
  print(
  ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = paste("Predictor weights from model 10d (run", repeat_loop, ")"),
         x = "Predictor", y = "Coefficient") +
    theme_minimal()
  )
  
  cat("\nTop coefficients for run", repeat_loop, ":\n")
  print(head(coef_df, top_n))
  
  results_df <- data.frame(
    Actual = y_rep,
    Predicted = final_y_pred
  )
  
  final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
    labs(
      title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(final_plot)
  
  cat("Most common C for run", repeat_loop, ":", most_common_c, "\n")
}

# Calculating mean and SD of performance metrics
performance_df <- do.call(rbind, all_performance)
mean_performance <- colMeans(performance_df[, c("MSE", "RMSE", "MAE", "R2")])
sd_performance <- apply(performance_df[, c("MSE", "RMSE", "MAE", "R2")], 2, sd)

cat("MSE:", mean_performance["MSE"], "±", sd_performance["MSE"], "\n")
cat("RMSE:", mean_performance["RMSE"], "±", sd_performance["RMSE"], "\n")
cat("MAE:", mean_performance["MAE"], "±", sd_performance["MAE"], "\n")
cat("R²:", mean_performance["R2"], "±", sd_performance["R2"], "\n")
cat("Most common C value:", names(sort(table(performance_df$C), decreasing = TRUE)[1]), "\n")

model10d_results <- rbind(model10d_results, data.frame(
  Model = "10d",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"],
  MSE_std = sd_performance["MSE"],
  RMSE_mean = mean_performance["RMSE"],
  RMSE_std = sd_performance["RMSE"],
  MAE_mean = mean_performance["MAE"],
  MAE_std = sd_performance["MAE"],
  R2_mean = mean_performance["R2"],
  R2_std = sd_performance["R2"]
))


model10d_results_normalized <- rbind(model10d_results_normalized, data.frame(
  Model = "10d",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"] / mse_mean_baseline,
  MSE_std = sd_performance["MSE"] / mse_mean_baseline,
  RMSE_mean = mean_performance["RMSE"] / rmse_mean_baseline,
  RMSE_std = sd_performance["RMSE"] / rmse_mean_baseline,
  MAE_mean = mean_performance["MAE"] / mae_mean_baseline,
  MAE_std =  sd_performance["MAE"] / mae_mean_baseline,
  R2_mean = mean_performance["R2"] / r2_mean_baseline,
  R2_std = sd_performance["R2"] / r2_mean_baseline
))

coef_matrix <- do.call(cbind, all_coefficients)
coef_means <- rowMeans(coef_matrix)
coef_sds <- apply(coef_matrix, 1, sd)

coef_summary <- data.frame(
  Feature = feature_names,
  Mean_Coefficient = coef_means,
  SD_Coefficient = coef_sds
)

coef_summary <- coef_summary %>% arrange(desc(abs(Mean_Coefficient)))

coef_long <- data.frame()
for (i in 1:length(all_coefficients)) {
  temp_df <- data.frame(
    Feature = feature_names,
    Coefficient = all_coefficients[[i]],
    Run = paste("Run", i)
  )
  coef_long <- rbind(coef_long, temp_df)
}

top_n <- 20
top_features <- head(coef_summary, top_n)$Feature

coef_long_top <- coef_long %>% filter(Feature %in% top_features)

# Creating a plot with mean ± SD
mean_sd_plot <- ggplot(head(coef_summary, top_n), 
                     aes(x = reorder(Feature, Mean_Coefficient), y = Mean_Coefficient)) +
  geom_col(fill = "steelblue") +
  geom_errorbar(aes(ymin = Mean_Coefficient - SD_Coefficient, 
                  ymax = Mean_Coefficient + SD_Coefficient), width = 0.2) +
  coord_flip() +
  labs(title = "Mean predictor weights across 5 runs",
       subtitle = paste("Error bars show ± 1 SD"),
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

print(mean_sd_plot)

# Creating a plot showing individual points for each run
individual_plot <- ggplot(coef_long_top, 
                        aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient, color = Run)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Individual predictor weights from all 5 runs",
       x = "Predictor", y = "Coefficient") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(individual_plot)

# Creating a plot showing performance metrics across runs
performance_long <- performance_df %>%
  pivot_longer(cols = c("MSE", "RMSE", "MAE", "R2"), 
               names_to = "Metric", values_to = "Value")

performance_plot <- ggplot(performance_long, 
                         aes(x = as.factor(Run), y = Value, group = Metric, color = Metric)) +
  geom_line() +
  geom_point(size = 3) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(title = "Model performance across 5 runs",
       x = "Run", y = "Value") +
  theme_minimal()

print(performance_plot)

# Summary table of coefficients
top_coef_summary <- head(coef_summary, top_n)
print(top_coef_summary)
```

#### Feature selection for model 10d
```{r}
RFE <- sklearn$feature_selection$RFE

n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results_rfe <- data.frame()
best_params_list_rfe <- list()
outer_scores_rfe <- c()

c_values <- c(0.01, 0.1, 1, 10, 30)

for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]

  n_train_samples <- length(train_idx)
  set.seed(42 + outer_fold)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  best_features_mask <- NULL
  
  for (c_val in c_values) {
    cv_scores <- c()
    
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]

      base_model <- SVR(C = c_val, kernel = 'linear')
      rfe_model <- RFE(estimator = base_model, n_features_to_select = 5L)
      rfe_model$fit(X_inner_train, y_inner_train)

      selected_mask <- rfe_model$support_
      selected_indices <- which(py_to_r(selected_mask))
      
      X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
      X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]

      model_selected <- SVR(C = c_val, kernel = 'linear')
      model_selected$fit(X_inner_train_selected, y_inner_train)
      y_pred_val <- model_selected$predict(X_inner_val_selected)
      
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
      cv_scores <- c(cv_scores, neg_mse)
    }

    mean_score <- mean(cv_scores)
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
      # Store best mask based on entire training data
      final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'), n_features_to_select = 5L)
      final_rfe$fit(X_train_outer, y_train_outer)
      best_features_mask <- final_rfe$support_
    }
  }

  best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
  cat("  Best C for fold", outer_fold, ":", best_c, "\n")

  X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
  X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]

  final_model <- SVR(C = best_c, kernel = 'linear')
  final_model$fit(X_train_outer_selected, y_train_outer)
  y_pred_outer <- final_model$predict(X_test_outer_selected)

  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)

  outer_scores_rfe <- c(outer_scores_rfe, r2)

  nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results with RFE:\n")
print(nested_cv_results_rfe)
cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")

cat("\nBest Parameters and Features per Fold:\n")
for (i in 1:length(best_params_list_rfe)) {
  cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
  print(colnames(X)[best_params_list_rfe[[i]]$mask])
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# Apply RFE to full dataset using best C
final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'), n_features_to_select = 5L)
final_rfe$fit(X_scaledSVM, y)
selected_mask_final <- final_rfe$support_

# Subset data to selected features
X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]

# Train final model with selected features
final_svr <- SVR(C = most_common_c, kernel = 'linear')
final_svr$fit(X_selected_final, y)

# Use X_selected_final (only features chosen by RFE) for cross-validation
mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model10d_results <- rbind(model10d_results, data.frame(
  Model = "10d",
  Method = "Feature_Elimination",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model10d_results_normalized <- rbind(model10d_results_normalized, data.frame(
  Model = "10d",
  Method = "Feature_Elimination",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


# Don't refit - use the already fitted model on selected features
final_y_pred <- final_svr$predict(X_selected_final)

# Get coefficients
coefficients <- final_svr$coef_
intercept <- final_svr$intercept_

# Use feature names of selected features only
feature_names <- colnames(X)[selected_mask_final]
coef_vector <- py_to_r(coefficients)[1, ]

coef_df <- data.frame(
  Feature = feature_names,
  Coefficient = coef_vector
)

print(coef_df)

top_n <- 20
ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Predictor weights from feature model 10d (with feature elimination)",
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)

```





### model 11a: outcome Brief Inspire-O with set of predictors without baseline Brief Inspire-O
```{r}
# for this model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1, honos_totaal.1, FR_totaal.1. I left out Inspire_totaal.1 
# and the following outcome: Inspire_totaal.2
# I filter the data to only include non-missing values of outcome Inspire_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel11a <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, FR_totaal.1, Inspire_totaal.2, opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(Inspire_totaal.2)) 
```

```{r}
# the following is to keep track of the metrics 
model11a_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model11a_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM11a <- colSums(is.na(dataModel11a)) / nrow(dataModel11a) * 100
print(missing_percentageM11a)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel11a))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel11a)
```

#### imputing data 
```{r}
methods <- make.method(dataModel11a)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["opleiding_nieuw"] <- "pmm" # numeric 
methods["leefsituatie_nieuw"] <- "logreg" # binary 
methods["MANSA_PH_7.1"] <- "logreg" # binary 
methods["MANSA_PH_9.1"] <- "logreg" # binary 
methods["MANSA_PH_10.1"] <- "logreg" # binary 
methods["MANSA_PH_11.1"] <- "logreg" # binary 
methods["month_diff_1_and_2"] <- ""
methods["Inspire_totaal.2"] <- ""


# doing the imputation
imputed_dataModel11a <- mice(dataModel11a, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel11a)
```

```{r}
dataModel11a_complete <- complete(imputed_dataModel11a,1)
```

#### Selecting features and outcome 
```{r}
X <- dataModel11a_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "FR_totaal.1", "opleiding_nieuw", "burgerlijkestaat_nieuw", "leefsituatie_nieuw", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1")]
y <- dataModel11a_complete$Inspire_totaal.2  
# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 

```


#### Nested cross validation (5x repeated)
```{r}
X_scaledSVM <- scalerSVM$fit_transform(X) 
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

# Saving coefficients and performances 
all_coefficients <- list()
all_performance <- list()

for (repeat_loop in 1:5) {
  # Setting a seed for reproducability 
  set.seed(42 + repeat_loop)
  cat("\n\n========== REPETITION", repeat_loop, "==========\n")
  
  # Creating a different data split for each repetition
  split_indices <- sample(1:n_samples)
  training_size <- floor(0.8 * n_samples)
  
  X_rep <- X_scaledSVM[split_indices[1:training_size], , drop=FALSE]
  y_rep <- y[split_indices[1:training_size]]
  
  cat("Repetition", repeat_loop, "training data dimensions:", dim(X_rep)[1], "x", dim(X_rep)[2], "\n")
  
  outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = nrow(X_rep)))

  nested_cv_results <- data.frame()
  best_params_list <- list()
  outer_scores <- c()

  c_values <- c(0.01, 0.1, 1, 10, 30)
  
  # Outer CV loop
  for (outer_fold in 1:n_outer_folds) {
    cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
    
    test_idx <- which(outer_fold_indices == outer_fold)
    train_idx <- which(outer_fold_indices != outer_fold)
    
    X_train_outer <- X_rep[train_idx, , drop=FALSE]
    y_train_outer <- y_rep[train_idx]
    X_test_outer <- X_rep[test_idx, , drop=FALSE]
    y_test_outer <- y_rep[test_idx]
    
    n_train_samples <- length(train_idx)
    
    set.seed(100 + (repeat_loop * 10) + outer_fold)
    inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
    
    best_c <- NULL
    best_score <- -Inf
    
    for (c_val in c_values) {
      cv_scores <- c()
  
      # Inner loop 
      for (inner_fold in 1:n_inner_folds) {
        inner_val_idx <- which(inner_fold_indices == inner_fold)
        inner_train_idx <- which(inner_fold_indices != inner_fold)
        
        X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
        y_inner_train <- y_train_outer[inner_train_idx]
        X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
        y_inner_val <- y_train_outer[inner_val_idx]
        
        if (inner_fold == 1) {  
          cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
              "rows, y:", length(y_inner_train), "elements\n")
          cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
              "rows, y:", length(y_inner_val), "elements\n")
        }
        
        inner_svr <- SVR(C = c_val, kernel = 'linear')
        inner_svr$fit(X_inner_train, y_inner_train)
        
        y_pred_inner <- inner_svr$predict(X_inner_val)
        neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
        cv_scores <- c(cv_scores, neg_mse)
      }
      
      mean_score <- mean(cv_scores)
      
      if (mean_score > best_score) {
        best_score <- mean_score
        best_c <- c_val
      }
    }
    
    cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
    
    best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
    
    best_svr <- SVR(C = best_c, kernel = 'linear')
    best_svr$fit(X_train_outer, y_train_outer)
    
    y_pred_outer <- best_svr$predict(X_test_outer)
    
    mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
    rmse <- sqrt(mse)
    mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
    r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
    
    outer_scores <- c(outer_scores, r2)
    
    nested_cv_results <- rbind(nested_cv_results, data.frame(
      Fold = outer_fold,
      Best_C = best_c,
      MSE = mse,
      RMSE = rmse,
      MAE = mae,
      R2 = r2
    ))
    
    cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
  }
  
  cat("\nNested CV Results:\n")
  print(nested_cv_results)
  cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")
  
  cat("\nBest Parameters per Fold:\n")
  for (i in 1:length(best_params_list)) {
    cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
  }
  
  c_values <- sapply(best_params_list, function(x) x$C)
  c_table <- table(c_values)
  most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
  
  rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=repeat_loop)
  
  final_svr <- SVR(C = most_common_c, kernel = 'linear')
  
  mse_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='r2')
  
  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)
  
  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)
  
  cat("SVR model with nested CV, best C =", most_common_c, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  # Saving performance metrics for this run
  all_performance[[repeat_loop]] <- data.frame(
    Run = repeat_loop,
    MSE = mse_mean,
    RMSE = rmse_mean,
    MAE = mae_mean,
    R2 = r2_mean,
    C = most_common_c
  )
  
  model11a_results <- rbind(model11a_results, data.frame(
    Model = "11a",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))
  
  
  model11a_results_normalized <- rbind(model11a_results_normalized, data.frame(
    Model = "11a",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "Inspire_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
  
  final_svr$fit(X_rep, y_rep)
  final_y_pred <- final_svr$predict(X_rep)
  
  coefficients <- final_svr$coef_
  intercept <- final_svr$intercept_
  
  coef_vector <- py_to_r(coefficients)[1, ]  # flatten the 2D array to a 1D vector
  intercept <- py_to_r(intercept)[1]
  
  # saving coefficients for this run
  all_coefficients[[repeat_loop]] <- coef_vector
  
  feature_names <- colnames(X)
  coef_df <- data.frame(
    Feature = feature_names,
    Coefficient = coef_vector
  )
  
  coef_df <- coef_df %>% arrange(desc(abs(Coefficient)))
  
  top_n <- 20
  print(
  ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = paste("Predictor weights from model 11a (run", repeat_loop, ")"),
         x = "Predictor", y = "Coefficient") +
    theme_minimal()
  )
  
  cat("\nTop coefficients for run", repeat_loop, ":\n")
  print(head(coef_df, top_n))
  
  results_df <- data.frame(
    Actual = y_rep,
    Predicted = final_y_pred
  )
  
  final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
    labs(
      title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(final_plot)
  
  cat("Most common C for run", repeat_loop, ":", most_common_c, "\n")
}

# Calculating mean and SD of performance metrics
performance_df <- do.call(rbind, all_performance)
mean_performance <- colMeans(performance_df[, c("MSE", "RMSE", "MAE", "R2")])
sd_performance <- apply(performance_df[, c("MSE", "RMSE", "MAE", "R2")], 2, sd)

cat("MSE:", mean_performance["MSE"], "±", sd_performance["MSE"], "\n")
cat("RMSE:", mean_performance["RMSE"], "±", sd_performance["RMSE"], "\n")
cat("MAE:", mean_performance["MAE"], "±", sd_performance["MAE"], "\n")
cat("R²:", mean_performance["R2"], "±", sd_performance["R2"], "\n")
cat("Most common C value:", names(sort(table(performance_df$C), decreasing = TRUE)[1]), "\n")

model11a_results <- rbind(model11a_results, data.frame(
  Model = "11a",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"],
  MSE_std = sd_performance["MSE"],
  RMSE_mean = mean_performance["RMSE"],
  RMSE_std = sd_performance["RMSE"],
  MAE_mean = mean_performance["MAE"],
  MAE_std = sd_performance["MAE"],
  R2_mean = mean_performance["R2"],
  R2_std = sd_performance["R2"]
))


model11a_results_normalized <- rbind(model11a_results_normalized, data.frame(
  Model = "11a",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"] / mse_mean_baseline,
  MSE_std = sd_performance["MSE"] / mse_mean_baseline,
  RMSE_mean = mean_performance["RMSE"] / rmse_mean_baseline,
  RMSE_std = sd_performance["RMSE"] / rmse_mean_baseline,
  MAE_mean = mean_performance["MAE"] / mae_mean_baseline,
  MAE_std =  sd_performance["MAE"] / mae_mean_baseline,
  R2_mean = mean_performance["R2"] / r2_mean_baseline,
  R2_std = sd_performance["R2"] / r2_mean_baseline
))

coef_matrix <- do.call(cbind, all_coefficients)
coef_means <- rowMeans(coef_matrix)
coef_sds <- apply(coef_matrix, 1, sd)

coef_summary <- data.frame(
  Feature = feature_names,
  Mean_Coefficient = coef_means,
  SD_Coefficient = coef_sds
)

coef_summary <- coef_summary %>% arrange(desc(abs(Mean_Coefficient)))

coef_long <- data.frame()
for (i in 1:length(all_coefficients)) {
  temp_df <- data.frame(
    Feature = feature_names,
    Coefficient = all_coefficients[[i]],
    Run = paste("Run", i)
  )
  coef_long <- rbind(coef_long, temp_df)
}

top_n <- 20
top_features <- head(coef_summary, top_n)$Feature

coef_long_top <- coef_long %>% filter(Feature %in% top_features)

# Creating a plot with mean ± SD
mean_sd_plot <- ggplot(head(coef_summary, top_n), 
                     aes(x = reorder(Feature, Mean_Coefficient), y = Mean_Coefficient)) +
  geom_col(fill = "steelblue") +
  geom_errorbar(aes(ymin = Mean_Coefficient - SD_Coefficient, 
                  ymax = Mean_Coefficient + SD_Coefficient), width = 0.2) +
  coord_flip() +
  labs(title = "Mean predictor weights across 5 runs",
       subtitle = paste("Error bars show ± 1 SD"),
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

print(mean_sd_plot)

# Creating a plot showing individual points for each run
individual_plot <- ggplot(coef_long_top, 
                        aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient, color = Run)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Individual predictor weights from all 5 runs",
       x = "Predictor", y = "Coefficient") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(individual_plot)

# Creating a plot showing performance metrics across runs
performance_long <- performance_df %>%
  pivot_longer(cols = c("MSE", "RMSE", "MAE", "R2"), 
               names_to = "Metric", values_to = "Value")

performance_plot <- ggplot(performance_long, 
                         aes(x = as.factor(Run), y = Value, group = Metric, color = Metric)) +
  geom_line() +
  geom_point(size = 3) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(title = "Model performance across 5 runs",
       x = "Run", y = "Value") +
  theme_minimal()

print(performance_plot)

# Summary table of coefficients
top_coef_summary <- head(coef_summary, top_n)
print(top_coef_summary)
```

#### Feature selection for model 11a
```{r}
RFE <- sklearn$feature_selection$RFE

n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results_rfe <- data.frame()
best_params_list_rfe <- list()
outer_scores_rfe <- c()

c_values <- c(0.01, 0.1, 1, 10, 30)

for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]

  n_train_samples <- length(train_idx)
  set.seed(42 + outer_fold)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  best_features_mask <- NULL
  
  for (c_val in c_values) {
    cv_scores <- c()
    
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]

      base_model <- SVR(C = c_val, kernel = 'linear')
      rfe_model <- RFE(estimator = base_model, n_features_to_select = 5L)
      rfe_model$fit(X_inner_train, y_inner_train)

      selected_mask <- rfe_model$support_
      selected_indices <- which(py_to_r(selected_mask))
      
      X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
      X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]

      model_selected <- SVR(C = c_val, kernel = 'linear')
      model_selected$fit(X_inner_train_selected, y_inner_train)
      y_pred_val <- model_selected$predict(X_inner_val_selected)
      
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
      cv_scores <- c(cv_scores, neg_mse)
    }

    mean_score <- mean(cv_scores)
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
      # Store best mask based on entire training data
      final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'), n_features_to_select = 5L)
      final_rfe$fit(X_train_outer, y_train_outer)
      best_features_mask <- final_rfe$support_
    }
  }

  best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
  cat("  Best C for fold", outer_fold, ":", best_c, "\n")

  X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
  X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]

  final_model <- SVR(C = best_c, kernel = 'linear')
  final_model$fit(X_train_outer_selected, y_train_outer)
  y_pred_outer <- final_model$predict(X_test_outer_selected)

  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)

  outer_scores_rfe <- c(outer_scores_rfe, r2)

  nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results with RFE:\n")
print(nested_cv_results_rfe)
cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")

cat("\nBest Parameters and Features per Fold:\n")
for (i in 1:length(best_params_list_rfe)) {
  cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
  print(colnames(X)[best_params_list_rfe[[i]]$mask])
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# Apply RFE to full dataset using best C
final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'), n_features_to_select = 5L)
final_rfe$fit(X_scaledSVM, y)
selected_mask_final <- final_rfe$support_

# Subset data to selected features
X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]

# Train final model with selected features
final_svr <- SVR(C = most_common_c, kernel = 'linear')
final_svr$fit(X_selected_final, y)

# Use X_selected_final (only features chosen by RFE) for cross-validation
mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model11a_results <- rbind(model11a_results, data.frame(
  Model = "11a",
  Method = "Feature_Elimination",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model11a_results_normalized <- rbind(model11a_results_normalized, data.frame(
  Model = "11a",
  Method = "Feature_Elimination",
  Outcome = "Inspire_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


# Don't refit - use the already fitted model on selected features
final_y_pred <- final_svr$predict(X_selected_final)

# Get coefficients
coefficients <- final_svr$coef_
intercept <- final_svr$intercept_

# Use feature names of selected features only
feature_names <- colnames(X)[selected_mask_final]
coef_vector <- py_to_r(coefficients)[1, ]

coef_df <- data.frame(
  Feature = feature_names,
  Coefficient = coef_vector
)

print(coef_df)

top_n <- 20
ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Predictor weights from feature model 11a (with feature elimination)",
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)

```








### model 11b: outcome Mansa with set of predictors without baseline Mansa
```{r}
# for this model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, Inspire_totaal.1, honos_totaal.1, FR_totaal.1. I left out mansa_totaal.1 
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel11b <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, Inspire_totaal.1, honos_totaal.1, FR_totaal.1, mansa_totaal.2, opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 
```

```{r}
# the following is to keep track of the metrics 
model11b_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model11b_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM11b <- colSums(is.na(dataModel11b)) / nrow(dataModel11b) * 100
print(missing_percentageM11b)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel11b))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel11b)
```

#### imputing data 
```{r}
methods <- make.method(dataModel11b)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["opleiding_nieuw"] <- "pmm" # numeric 
methods["leefsituatie_nieuw"] <- "logreg" # binary 
methods["MANSA_PH_7.1"] <- "logreg" # binary 
methods["MANSA_PH_9.1"] <- "logreg" # binary 
methods["MANSA_PH_10.1"] <- "logreg" # binary 
methods["MANSA_PH_11.1"] <- "logreg" # binary 
methods["month_diff_1_and_2"] <- ""
methods["mansa_totaal.2"] <- ""


# doing the imputation
imputed_dataModel11b <- mice(dataModel11b, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel11b)
```

```{r}
dataModel11b_complete <- complete(imputed_dataModel11b,1)
```

#### Selecting features and outcome 
```{r}
X <- dataModel11b_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "Inspire_totaal.1", "honos_totaal.1", "FR_totaal.1", "opleiding_nieuw", "burgerlijkestaat_nieuw", "leefsituatie_nieuw", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1")]
y <- dataModel11b_complete$mansa_totaal.2  
# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 

```

#### Nested cross validation (5x repeated)
```{r}
X_scaledSVM <- scalerSVM$fit_transform(X) 
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

# Saving coefficients and performances 
all_coefficients <- list()
all_performance <- list()

for (repeat_loop in 1:5) {
  # Setting a seed for reproducability 
  set.seed(42 + repeat_loop)
  cat("\n\n========== REPETITION", repeat_loop, "==========\n")
  
  # Creating a different data split for each repetition
  split_indices <- sample(1:n_samples)
  training_size <- floor(0.8 * n_samples)
  
  X_rep <- X_scaledSVM[split_indices[1:training_size], , drop=FALSE]
  y_rep <- y[split_indices[1:training_size]]
  
  cat("Repetition", repeat_loop, "training data dimensions:", dim(X_rep)[1], "x", dim(X_rep)[2], "\n")
  
  outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = nrow(X_rep)))

  nested_cv_results <- data.frame()
  best_params_list <- list()
  outer_scores <- c()

  c_values <- c(0.01, 0.1, 1, 10, 30)
  
  # Outer CV loop
  for (outer_fold in 1:n_outer_folds) {
    cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
    
    test_idx <- which(outer_fold_indices == outer_fold)
    train_idx <- which(outer_fold_indices != outer_fold)
    
    X_train_outer <- X_rep[train_idx, , drop=FALSE]
    y_train_outer <- y_rep[train_idx]
    X_test_outer <- X_rep[test_idx, , drop=FALSE]
    y_test_outer <- y_rep[test_idx]
    
    n_train_samples <- length(train_idx)
    
    set.seed(100 + (repeat_loop * 10) + outer_fold)
    inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
    
    best_c <- NULL
    best_score <- -Inf
    
    for (c_val in c_values) {
      cv_scores <- c()
  
      # Inner loop 
      for (inner_fold in 1:n_inner_folds) {
        inner_val_idx <- which(inner_fold_indices == inner_fold)
        inner_train_idx <- which(inner_fold_indices != inner_fold)
        
        X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
        y_inner_train <- y_train_outer[inner_train_idx]
        X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
        y_inner_val <- y_train_outer[inner_val_idx]
        
        if (inner_fold == 1) {  
          cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
              "rows, y:", length(y_inner_train), "elements\n")
          cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
              "rows, y:", length(y_inner_val), "elements\n")
        }
        
        inner_svr <- SVR(C = c_val, kernel = 'linear')
        inner_svr$fit(X_inner_train, y_inner_train)
        
        y_pred_inner <- inner_svr$predict(X_inner_val)
        neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
        cv_scores <- c(cv_scores, neg_mse)
      }
      
      mean_score <- mean(cv_scores)
      
      if (mean_score > best_score) {
        best_score <- mean_score
        best_c <- c_val
      }
    }
    
    cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
    
    best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
    
    best_svr <- SVR(C = best_c, kernel = 'linear')
    best_svr$fit(X_train_outer, y_train_outer)
    
    y_pred_outer <- best_svr$predict(X_test_outer)
    
    mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
    rmse <- sqrt(mse)
    mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
    r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
    
    outer_scores <- c(outer_scores, r2)
    
    nested_cv_results <- rbind(nested_cv_results, data.frame(
      Fold = outer_fold,
      Best_C = best_c,
      MSE = mse,
      RMSE = rmse,
      MAE = mae,
      R2 = r2
    ))
    
    cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
  }
  
  cat("\nNested CV Results:\n")
  print(nested_cv_results)
  cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")
  
  cat("\nBest Parameters per Fold:\n")
  for (i in 1:length(best_params_list)) {
    cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
  }
  
  c_values <- sapply(best_params_list, function(x) x$C)
  c_table <- table(c_values)
  most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
  
  rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=repeat_loop)
  
  final_svr <- SVR(C = most_common_c, kernel = 'linear')
  
  mse_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='r2')
  
  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)
  
  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)
  
  cat("SVR model with nested CV, best C =", most_common_c, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  # Saving performance metrics for this run
  all_performance[[repeat_loop]] <- data.frame(
    Run = repeat_loop,
    MSE = mse_mean,
    RMSE = rmse_mean,
    MAE = mae_mean,
    R2 = r2_mean,
    C = most_common_c
  )
  
  model11b_results <- rbind(model11b_results, data.frame(
    Model = "11b",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))
  
  
  model11b_results_normalized <- rbind(model11b_results_normalized, data.frame(
    Model = "11b",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "mansa_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
  
  final_svr$fit(X_rep, y_rep)
  final_y_pred <- final_svr$predict(X_rep)
  
  coefficients <- final_svr$coef_
  intercept <- final_svr$intercept_
  
  coef_vector <- py_to_r(coefficients)[1, ]  # flatten the 2D array to a 1D vector
  intercept <- py_to_r(intercept)[1]
  
  # saving coefficients for this run
  all_coefficients[[repeat_loop]] <- coef_vector
  
  feature_names <- colnames(X)
  coef_df <- data.frame(
    Feature = feature_names,
    Coefficient = coef_vector
  )
  
  coef_df <- coef_df %>% arrange(desc(abs(Coefficient)))
  
  top_n <- 20
  print(
  ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = paste("Predictor weights from model 11b (run", repeat_loop, ")"),
         x = "Predictor", y = "Coefficient") +
    theme_minimal()
  )
  
  cat("\nTop coefficients for run", repeat_loop, ":\n")
  print(head(coef_df, top_n))
  
  results_df <- data.frame(
    Actual = y_rep,
    Predicted = final_y_pred
  )
  
  final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
    labs(
      title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(final_plot)
  
  cat("Most common C for run", repeat_loop, ":", most_common_c, "\n")
}

# Calculating mean and SD of performance metrics
performance_df <- do.call(rbind, all_performance)
mean_performance <- colMeans(performance_df[, c("MSE", "RMSE", "MAE", "R2")])
sd_performance <- apply(performance_df[, c("MSE", "RMSE", "MAE", "R2")], 2, sd)

cat("MSE:", mean_performance["MSE"], "±", sd_performance["MSE"], "\n")
cat("RMSE:", mean_performance["RMSE"], "±", sd_performance["RMSE"], "\n")
cat("MAE:", mean_performance["MAE"], "±", sd_performance["MAE"], "\n")
cat("R²:", mean_performance["R2"], "±", sd_performance["R2"], "\n")
cat("Most common C value:", names(sort(table(performance_df$C), decreasing = TRUE)[1]), "\n")

model11b_results <- rbind(model11b_results, data.frame(
  Model = "11b",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"],
  MSE_std = sd_performance["MSE"],
  RMSE_mean = mean_performance["RMSE"],
  RMSE_std = sd_performance["RMSE"],
  MAE_mean = mean_performance["MAE"],
  MAE_std = sd_performance["MAE"],
  R2_mean = mean_performance["R2"],
  R2_std = sd_performance["R2"]
))


model11b_results_normalized <- rbind(model11b_results_normalized, data.frame(
  Model = "11b",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"] / mse_mean_baseline,
  MSE_std = sd_performance["MSE"] / mse_mean_baseline,
  RMSE_mean = mean_performance["RMSE"] / rmse_mean_baseline,
  RMSE_std = sd_performance["RMSE"] / rmse_mean_baseline,
  MAE_mean = mean_performance["MAE"] / mae_mean_baseline,
  MAE_std =  sd_performance["MAE"] / mae_mean_baseline,
  R2_mean = mean_performance["R2"] / r2_mean_baseline,
  R2_std = sd_performance["R2"] / r2_mean_baseline
))

coef_matrix <- do.call(cbind, all_coefficients)
coef_means <- rowMeans(coef_matrix)
coef_sds <- apply(coef_matrix, 1, sd)

coef_summary <- data.frame(
  Feature = feature_names,
  Mean_Coefficient = coef_means,
  SD_Coefficient = coef_sds
)

coef_summary <- coef_summary %>% arrange(desc(abs(Mean_Coefficient)))

coef_long <- data.frame()
for (i in 1:length(all_coefficients)) {
  temp_df <- data.frame(
    Feature = feature_names,
    Coefficient = all_coefficients[[i]],
    Run = paste("Run", i)
  )
  coef_long <- rbind(coef_long, temp_df)
}

top_n <- 20
top_features <- head(coef_summary, top_n)$Feature

coef_long_top <- coef_long %>% filter(Feature %in% top_features)

# Creating a plot with mean ± SD
mean_sd_plot <- ggplot(head(coef_summary, top_n), 
                     aes(x = reorder(Feature, Mean_Coefficient), y = Mean_Coefficient)) +
  geom_col(fill = "steelblue") +
  geom_errorbar(aes(ymin = Mean_Coefficient - SD_Coefficient, 
                  ymax = Mean_Coefficient + SD_Coefficient), width = 0.2) +
  coord_flip() +
  labs(title = "Mean predictor weights across 5 runs",
       subtitle = paste("Error bars show ± 1 SD"),
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

print(mean_sd_plot)

# Creating a plot showing individual points for each run
individual_plot <- ggplot(coef_long_top, 
                        aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient, color = Run)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Individual predictor weights from all 5 runs",
       x = "Predictor", y = "Coefficient") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(individual_plot)

# Creating a plot showing performance metrics across runs
performance_long <- performance_df %>%
  pivot_longer(cols = c("MSE", "RMSE", "MAE", "R2"), 
               names_to = "Metric", values_to = "Value")

performance_plot <- ggplot(performance_long, 
                         aes(x = as.factor(Run), y = Value, group = Metric, color = Metric)) +
  geom_line() +
  geom_point(size = 3) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(title = "Model performance across 5 runs",
       x = "Run", y = "Value") +
  theme_minimal()

print(performance_plot)

# Summary table of coefficients
top_coef_summary <- head(coef_summary, top_n)
print(top_coef_summary)
```

#### Feature selection for model 11b
```{r}
RFE <- sklearn$feature_selection$RFE

n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results_rfe <- data.frame()
best_params_list_rfe <- list()
outer_scores_rfe <- c()

c_values <- c(0.01, 0.1, 1, 10, 30)

for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]

  n_train_samples <- length(train_idx)
  set.seed(42 + outer_fold)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  best_features_mask <- NULL
  
  for (c_val in c_values) {
    cv_scores <- c()
    
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]

      base_model <- SVR(C = c_val, kernel = 'linear')
      rfe_model <- RFE(estimator = base_model, n_features_to_select = 5L)
      rfe_model$fit(X_inner_train, y_inner_train)

      selected_mask <- rfe_model$support_
      selected_indices <- which(py_to_r(selected_mask))
      
      X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
      X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]

      model_selected <- SVR(C = c_val, kernel = 'linear')
      model_selected$fit(X_inner_train_selected, y_inner_train)
      y_pred_val <- model_selected$predict(X_inner_val_selected)
      
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
      cv_scores <- c(cv_scores, neg_mse)
    }

    mean_score <- mean(cv_scores)
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
      # Store best mask based on entire training data
      final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'), n_features_to_select = 5L)
      final_rfe$fit(X_train_outer, y_train_outer)
      best_features_mask <- final_rfe$support_
    }
  }

  best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
  cat("  Best C for fold", outer_fold, ":", best_c, "\n")

  X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
  X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]

  final_model <- SVR(C = best_c, kernel = 'linear')
  final_model$fit(X_train_outer_selected, y_train_outer)
  y_pred_outer <- final_model$predict(X_test_outer_selected)

  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)

  outer_scores_rfe <- c(outer_scores_rfe, r2)

  nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results with RFE:\n")
print(nested_cv_results_rfe)
cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")

cat("\nBest Parameters and Features per Fold:\n")
for (i in 1:length(best_params_list_rfe)) {
  cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
  print(colnames(X)[best_params_list_rfe[[i]]$mask])
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# Apply RFE to full dataset using best C
final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'), n_features_to_select = 5L)
final_rfe$fit(X_scaledSVM, y)
selected_mask_final <- final_rfe$support_

# Subset data to selected features
X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]

# Train final model with selected features
final_svr <- SVR(C = most_common_c, kernel = 'linear')
final_svr$fit(X_selected_final, y)

# Use X_selected_final (only features chosen by RFE) for cross-validation
mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model11b_results <- rbind(model11b_results, data.frame(
  Model = "11b",
  Method = "Feature_Elimination",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model11b_results_normalized <- rbind(model11b_results_normalized, data.frame(
  Model = "11b",
  Method = "Feature_Elimination",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


# Don't refit - use the already fitted model on selected features
final_y_pred <- final_svr$predict(X_selected_final)

# Get coefficients
coefficients <- final_svr$coef_
intercept <- final_svr$intercept_

# Use feature names of selected features only
feature_names <- colnames(X)[selected_mask_final]
coef_vector <- py_to_r(coefficients)[1, ]

coef_df <- data.frame(
  Feature = feature_names,
  Coefficient = coef_vector
)

print(coef_df)

top_n <- 20
ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Predictor weights from feature model 11b (with feature elimination)",
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)

```





### model 11c: outcome honos with set of predictors without baseline honos
```{r}
# for this model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1, Inspire_totaal.1, FR_totaal.1. I left out honos_totaal.1 
# and the following outcome: honos_totaal.2
# I filter the data to only include non-missing values of outcome honos_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel11c <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, Inspire_totaal.1, FR_totaal.1, honos_totaal.2, opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(honos_totaal.2)) 
```

```{r}
# the following is to keep track of the metrics 
model11c_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model11c_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM11c <- colSums(is.na(dataModel11c)) / nrow(dataModel11c) * 100
print(missing_percentageM11c)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel11c))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel11c)
```

#### imputing data 
```{r}
methods <- make.method(dataModel11c)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["opleiding_nieuw"] <- "pmm" # numeric 
methods["leefsituatie_nieuw"] <- "logreg" # binary 
methods["MANSA_PH_7.1"] <- "logreg" # binary 
methods["MANSA_PH_9.1"] <- "logreg" # binary 
methods["MANSA_PH_10.1"] <- "logreg" # binary 
methods["MANSA_PH_11.1"] <- "logreg" # binary 
methods["month_diff_1_and_2"] <- ""
methods["honos_totaal.2"] <- ""


# doing the imputation
imputed_dataModel11c <- mice(dataModel11c, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel11c)
```

```{r}
dataModel11c_complete <- complete(imputed_dataModel11c,1)
```

#### Selecting features and outcome 
```{r}
X <- dataModel11c_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "Inspire_totaal.1", "FR_totaal.1", "opleiding_nieuw", "burgerlijkestaat_nieuw", "leefsituatie_nieuw", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1")]
y <- dataModel11c_complete$honos_totaal.2  
# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 

```

#### Nested cross validation (5x repeated)
```{r}
X_scaledSVM <- scalerSVM$fit_transform(X) 
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

# Saving coefficients and performances 
all_coefficients <- list()
all_performance <- list()

for (repeat_loop in 1:5) {
  # Setting a seed for reproducability 
  set.seed(42 + repeat_loop)
  cat("\n\n========== REPETITION", repeat_loop, "==========\n")
  
  # Creating a different data split for each repetition
  split_indices <- sample(1:n_samples)
  training_size <- floor(0.8 * n_samples)
  
  X_rep <- X_scaledSVM[split_indices[1:training_size], , drop=FALSE]
  y_rep <- y[split_indices[1:training_size]]
  
  cat("Repetition", repeat_loop, "training data dimensions:", dim(X_rep)[1], "x", dim(X_rep)[2], "\n")
  
  outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = nrow(X_rep)))

  nested_cv_results <- data.frame()
  best_params_list <- list()
  outer_scores <- c()

  c_values <- c(0.01, 0.1, 1, 10, 30)
  
  # Outer CV loop
  for (outer_fold in 1:n_outer_folds) {
    cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
    
    test_idx <- which(outer_fold_indices == outer_fold)
    train_idx <- which(outer_fold_indices != outer_fold)
    
    X_train_outer <- X_rep[train_idx, , drop=FALSE]
    y_train_outer <- y_rep[train_idx]
    X_test_outer <- X_rep[test_idx, , drop=FALSE]
    y_test_outer <- y_rep[test_idx]
    
    n_train_samples <- length(train_idx)
    
    set.seed(100 + (repeat_loop * 10) + outer_fold)
    inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
    
    best_c <- NULL
    best_score <- -Inf
    
    for (c_val in c_values) {
      cv_scores <- c()
  
      # Inner loop 
      for (inner_fold in 1:n_inner_folds) {
        inner_val_idx <- which(inner_fold_indices == inner_fold)
        inner_train_idx <- which(inner_fold_indices != inner_fold)
        
        X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
        y_inner_train <- y_train_outer[inner_train_idx]
        X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
        y_inner_val <- y_train_outer[inner_val_idx]
        
        if (inner_fold == 1) {  
          cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
              "rows, y:", length(y_inner_train), "elements\n")
          cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
              "rows, y:", length(y_inner_val), "elements\n")
        }
        
        inner_svr <- SVR(C = c_val, kernel = 'linear')
        inner_svr$fit(X_inner_train, y_inner_train)
        
        y_pred_inner <- inner_svr$predict(X_inner_val)
        neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
        cv_scores <- c(cv_scores, neg_mse)
      }
      
      mean_score <- mean(cv_scores)
      
      if (mean_score > best_score) {
        best_score <- mean_score
        best_c <- c_val
      }
    }
    
    cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
    
    best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
    
    best_svr <- SVR(C = best_c, kernel = 'linear')
    best_svr$fit(X_train_outer, y_train_outer)
    
    y_pred_outer <- best_svr$predict(X_test_outer)
    
    mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
    rmse <- sqrt(mse)
    mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
    r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
    
    outer_scores <- c(outer_scores, r2)
    
    nested_cv_results <- rbind(nested_cv_results, data.frame(
      Fold = outer_fold,
      Best_C = best_c,
      MSE = mse,
      RMSE = rmse,
      MAE = mae,
      R2 = r2
    ))
    
    cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
  }
  
  cat("\nNested CV Results:\n")
  print(nested_cv_results)
  cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")
  
  cat("\nBest Parameters per Fold:\n")
  for (i in 1:length(best_params_list)) {
    cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
  }
  
  c_values <- sapply(best_params_list, function(x) x$C)
  c_table <- table(c_values)
  most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
  
  rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=repeat_loop)
  
  final_svr <- SVR(C = most_common_c, kernel = 'linear')
  
  mse_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='r2')
  
  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)
  
  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)
  
  cat("SVR model with nested CV, best C =", most_common_c, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  # Saving performance metrics for this run
  all_performance[[repeat_loop]] <- data.frame(
    Run = repeat_loop,
    MSE = mse_mean,
    RMSE = rmse_mean,
    MAE = mae_mean,
    R2 = r2_mean,
    C = most_common_c
  )
  
  model11c_results <- rbind(model11c_results, data.frame(
    Model = "11c",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))
  
  
  model11c_results_normalized <- rbind(model11c_results_normalized, data.frame(
    Model = "11c",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "honos_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
  
  final_svr$fit(X_rep, y_rep)
  final_y_pred <- final_svr$predict(X_rep)
  
  coefficients <- final_svr$coef_
  intercept <- final_svr$intercept_
  
  coef_vector <- py_to_r(coefficients)[1, ]  # flatten the 2D array to a 1D vector
  intercept <- py_to_r(intercept)[1]
  
  # saving coefficients for this run
  all_coefficients[[repeat_loop]] <- coef_vector
  
  feature_names <- colnames(X)
  coef_df <- data.frame(
    Feature = feature_names,
    Coefficient = coef_vector
  )
  
  coef_df <- coef_df %>% arrange(desc(abs(Coefficient)))
  
  top_n <- 20
  print(
  ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = paste("Predictor weights from model 11c (run", repeat_loop, ")"),
         x = "Predictor", y = "Coefficient") +
    theme_minimal()
  )
  
  cat("\nTop coefficients for run", repeat_loop, ":\n")
  print(head(coef_df, top_n))
  
  results_df <- data.frame(
    Actual = y_rep,
    Predicted = final_y_pred
  )
  
  final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
    labs(
      title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(final_plot)
  
  cat("Most common C for run", repeat_loop, ":", most_common_c, "\n")
}

# Calculating mean and SD of performance metrics
performance_df <- do.call(rbind, all_performance)
mean_performance <- colMeans(performance_df[, c("MSE", "RMSE", "MAE", "R2")])
sd_performance <- apply(performance_df[, c("MSE", "RMSE", "MAE", "R2")], 2, sd)

cat("MSE:", mean_performance["MSE"], "±", sd_performance["MSE"], "\n")
cat("RMSE:", mean_performance["RMSE"], "±", sd_performance["RMSE"], "\n")
cat("MAE:", mean_performance["MAE"], "±", sd_performance["MAE"], "\n")
cat("R²:", mean_performance["R2"], "±", sd_performance["R2"], "\n")
cat("Most common C value:", names(sort(table(performance_df$C), decreasing = TRUE)[1]), "\n")

model11c_results <- rbind(model11c_results, data.frame(
  Model = "11c",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"],
  MSE_std = sd_performance["MSE"],
  RMSE_mean = mean_performance["RMSE"],
  RMSE_std = sd_performance["RMSE"],
  MAE_mean = mean_performance["MAE"],
  MAE_std = sd_performance["MAE"],
  R2_mean = mean_performance["R2"],
  R2_std = sd_performance["R2"]
))


model11c_results_normalized <- rbind(model11c_results_normalized, data.frame(
  Model = "11c",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"] / mse_mean_baseline,
  MSE_std = sd_performance["MSE"] / mse_mean_baseline,
  RMSE_mean = mean_performance["RMSE"] / rmse_mean_baseline,
  RMSE_std = sd_performance["RMSE"] / rmse_mean_baseline,
  MAE_mean = mean_performance["MAE"] / mae_mean_baseline,
  MAE_std =  sd_performance["MAE"] / mae_mean_baseline,
  R2_mean = mean_performance["R2"] / r2_mean_baseline,
  R2_std = sd_performance["R2"] / r2_mean_baseline
))

coef_matrix <- do.call(cbind, all_coefficients)
coef_means <- rowMeans(coef_matrix)
coef_sds <- apply(coef_matrix, 1, sd)

coef_summary <- data.frame(
  Feature = feature_names,
  Mean_Coefficient = coef_means,
  SD_Coefficient = coef_sds
)

coef_summary <- coef_summary %>% arrange(desc(abs(Mean_Coefficient)))

coef_long <- data.frame()
for (i in 1:length(all_coefficients)) {
  temp_df <- data.frame(
    Feature = feature_names,
    Coefficient = all_coefficients[[i]],
    Run = paste("Run", i)
  )
  coef_long <- rbind(coef_long, temp_df)
}

top_n <- 20
top_features <- head(coef_summary, top_n)$Feature

coef_long_top <- coef_long %>% filter(Feature %in% top_features)

# Creating a plot with mean ± SD
mean_sd_plot <- ggplot(head(coef_summary, top_n), 
                     aes(x = reorder(Feature, Mean_Coefficient), y = Mean_Coefficient)) +
  geom_col(fill = "steelblue") +
  geom_errorbar(aes(ymin = Mean_Coefficient - SD_Coefficient, 
                  ymax = Mean_Coefficient + SD_Coefficient), width = 0.2) +
  coord_flip() +
  labs(title = "Mean predictor weights across 5 runs",
       subtitle = paste("Error bars show ± 1 SD"),
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

print(mean_sd_plot)

# Creating a plot showing individual points for each run
individual_plot <- ggplot(coef_long_top, 
                        aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient, color = Run)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Individual predictor weights from all 5 runs",
       x = "Predictor", y = "Coefficient") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(individual_plot)

# Creating a plot showing performance metrics across runs
performance_long <- performance_df %>%
  pivot_longer(cols = c("MSE", "RMSE", "MAE", "R2"), 
               names_to = "Metric", values_to = "Value")

performance_plot <- ggplot(performance_long, 
                         aes(x = as.factor(Run), y = Value, group = Metric, color = Metric)) +
  geom_line() +
  geom_point(size = 3) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(title = "Model performance across 5 runs",
       x = "Run", y = "Value") +
  theme_minimal()

print(performance_plot)

# Summary table of coefficients
top_coef_summary <- head(coef_summary, top_n)
print(top_coef_summary)
```

#### Feature selection for model 11c
```{r}
RFE <- sklearn$feature_selection$RFE

n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results_rfe <- data.frame()
best_params_list_rfe <- list()
outer_scores_rfe <- c()

c_values <- c(0.01, 0.1, 1, 10, 30)

for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]

  n_train_samples <- length(train_idx)
  set.seed(42 + outer_fold)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  best_features_mask <- NULL
  
  for (c_val in c_values) {
    cv_scores <- c()
    
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]

      base_model <- SVR(C = c_val, kernel = 'linear')
      rfe_model <- RFE(estimator = base_model, n_features_to_select = 5L)
      rfe_model$fit(X_inner_train, y_inner_train)

      selected_mask <- rfe_model$support_
      selected_indices <- which(py_to_r(selected_mask))
      
      X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
      X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]

      model_selected <- SVR(C = c_val, kernel = 'linear')
      model_selected$fit(X_inner_train_selected, y_inner_train)
      y_pred_val <- model_selected$predict(X_inner_val_selected)
      
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
      cv_scores <- c(cv_scores, neg_mse)
    }

    mean_score <- mean(cv_scores)
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
      # Store best mask based on entire training data
      final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'), n_features_to_select = 5L)
      final_rfe$fit(X_train_outer, y_train_outer)
      best_features_mask <- final_rfe$support_
    }
  }

  best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
  cat("  Best C for fold", outer_fold, ":", best_c, "\n")

  X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
  X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]

  final_model <- SVR(C = best_c, kernel = 'linear')
  final_model$fit(X_train_outer_selected, y_train_outer)
  y_pred_outer <- final_model$predict(X_test_outer_selected)

  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)

  outer_scores_rfe <- c(outer_scores_rfe, r2)

  nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results with RFE:\n")
print(nested_cv_results_rfe)
cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")

cat("\nBest Parameters and Features per Fold:\n")
for (i in 1:length(best_params_list_rfe)) {
  cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
  print(colnames(X)[best_params_list_rfe[[i]]$mask])
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# Apply RFE to full dataset using best C
final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'), n_features_to_select = 5L)
final_rfe$fit(X_scaledSVM, y)
selected_mask_final <- final_rfe$support_

# Subset data to selected features
X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]

# Train final model with selected features
final_svr <- SVR(C = most_common_c, kernel = 'linear')
final_svr$fit(X_selected_final, y)

# Use X_selected_final (only features chosen by RFE) for cross-validation
mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model11c_results <- rbind(model11c_results, data.frame(
  Model = "11c",
  Method = "Feature_Elimination",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model11c_results_normalized <- rbind(model11c_results_normalized, data.frame(
  Model = "11c",
  Method = "Feature_Elimination",
  Outcome = "honos_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


# Don't refit - use the already fitted model on selected features
final_y_pred <- final_svr$predict(X_selected_final)

# Get coefficients
coefficients <- final_svr$coef_
intercept <- final_svr$intercept_

# Use feature names of selected features only
feature_names <- colnames(X)[selected_mask_final]
coef_vector <- py_to_r(coefficients)[1, ]

coef_df <- data.frame(
  Feature = feature_names,
  Coefficient = coef_vector
)

print(coef_df)

top_n <- 20
ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Predictor weights from feature model 11c (with feature elimination)",
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)

```









### model 11d: outcome FR with set of predictors without baseline FR
```{r}
# for this model I choose the following predictors: Age, geslacht, modusmeanGGZ, levenspartner, betaaldwerk, mansa_totaal.1, honos_totaal.1, FR_totaal.1. I left out FR_totaal.1 
# and the following outcome: FR_totaal.2
# I filter the data to only include non-missing values of outcome FR_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel11d <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.2, opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_7.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(FR_totaal.2)) 
```

```{r}
# the following is to keep track of the metrics 
model11d_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model11d_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM11d <- colSums(is.na(dataModel11d)) / nrow(dataModel11d) * 100
print(missing_percentageM11d)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel11d))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel11d)
```

#### imputing data 
```{r}
methods <- make.method(dataModel11d)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["mansa_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["opleiding_nieuw"] <- "pmm" # numeric 
methods["leefsituatie_nieuw"] <- "logreg" # binary 
methods["MANSA_PH_7.1"] <- "logreg" # binary 
methods["MANSA_PH_9.1"] <- "logreg" # binary 
methods["MANSA_PH_10.1"] <- "logreg" # binary 
methods["MANSA_PH_11.1"] <- "logreg" # binary 
methods["month_diff_1_and_2"] <- ""
methods["FR_totaal.2"] <- ""


# doing the imputation
imputed_dataModel11d <- mice(dataModel11d, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel11d)
```

```{r}
dataModel11d_complete <- complete(imputed_dataModel11d,1)
```

#### Selecting features and outcome 
```{r}
X <- dataModel11d_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "mansa_totaal.1", "honos_totaal.1", "Inspire_totaal.1", "opleiding_nieuw", "burgerlijkestaat_nieuw", "leefsituatie_nieuw", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1")]
y <- dataModel11d_complete$FR_totaal.2  
# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 

```

#### Nested cross validation (5x repeated)
```{r}
X_scaledSVM <- scalerSVM$fit_transform(X) 
n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

# Saving coefficients and performances 
all_coefficients <- list()
all_performance <- list()

for (repeat_loop in 1:5) {
  # Setting a seed for reproducability 
  set.seed(42 + repeat_loop)
  cat("\n\n========== REPETITION", repeat_loop, "==========\n")
  
  # Creating a different data split for each repetition
  split_indices <- sample(1:n_samples)
  training_size <- floor(0.8 * n_samples)
  
  X_rep <- X_scaledSVM[split_indices[1:training_size], , drop=FALSE]
  y_rep <- y[split_indices[1:training_size]]
  
  cat("Repetition", repeat_loop, "training data dimensions:", dim(X_rep)[1], "x", dim(X_rep)[2], "\n")
  
  outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = nrow(X_rep)))

  nested_cv_results <- data.frame()
  best_params_list <- list()
  outer_scores <- c()

  c_values <- c(0.01, 0.1, 1, 10, 30)
  
  # Outer CV loop
  for (outer_fold in 1:n_outer_folds) {
    cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
    
    test_idx <- which(outer_fold_indices == outer_fold)
    train_idx <- which(outer_fold_indices != outer_fold)
    
    X_train_outer <- X_rep[train_idx, , drop=FALSE]
    y_train_outer <- y_rep[train_idx]
    X_test_outer <- X_rep[test_idx, , drop=FALSE]
    y_test_outer <- y_rep[test_idx]
    
    n_train_samples <- length(train_idx)
    
    set.seed(100 + (repeat_loop * 10) + outer_fold)
    inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
    
    best_c <- NULL
    best_score <- -Inf
    
    for (c_val in c_values) {
      cv_scores <- c()
  
      # Inner loop 
      for (inner_fold in 1:n_inner_folds) {
        inner_val_idx <- which(inner_fold_indices == inner_fold)
        inner_train_idx <- which(inner_fold_indices != inner_fold)
        
        X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
        y_inner_train <- y_train_outer[inner_train_idx]
        X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
        y_inner_val <- y_train_outer[inner_val_idx]
        
        if (inner_fold == 1) {  
          cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
              "rows, y:", length(y_inner_train), "elements\n")
          cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
              "rows, y:", length(y_inner_val), "elements\n")
        }
        
        inner_svr <- SVR(C = c_val, kernel = 'linear')
        inner_svr$fit(X_inner_train, y_inner_train)
        
        y_pred_inner <- inner_svr$predict(X_inner_val)
        neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
        cv_scores <- c(cv_scores, neg_mse)
      }
      
      mean_score <- mean(cv_scores)
      
      if (mean_score > best_score) {
        best_score <- mean_score
        best_c <- c_val
      }
    }
    
    cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
    
    best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
    
    best_svr <- SVR(C = best_c, kernel = 'linear')
    best_svr$fit(X_train_outer, y_train_outer)
    
    y_pred_outer <- best_svr$predict(X_test_outer)
    
    mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
    rmse <- sqrt(mse)
    mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
    r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
    
    outer_scores <- c(outer_scores, r2)
    
    nested_cv_results <- rbind(nested_cv_results, data.frame(
      Fold = outer_fold,
      Best_C = best_c,
      MSE = mse,
      RMSE = rmse,
      MAE = mae,
      R2 = r2
    ))
    
    cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
  }
  
  cat("\nNested CV Results:\n")
  print(nested_cv_results)
  cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")
  
  cat("\nBest Parameters per Fold:\n")
  for (i in 1:length(best_params_list)) {
    cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
  }
  
  c_values <- sapply(best_params_list, function(x) x$C)
  c_table <- table(c_values)
  most_common_c <- as.numeric(names(c_table)[which.max(c_table)])
  
  rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=repeat_loop)
  
  final_svr <- SVR(C = most_common_c, kernel = 'linear')
  
  mse_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_squared_error')
  mae_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='neg_mean_absolute_error')
  r2_scores <- cross_val_score(final_svr, X_rep, y_rep, cv=rkf, scoring='r2')
  
  mse_scores_r <- py_to_r(mse_scores)
  mae_scores_r <- py_to_r(mae_scores)
  r2_scores_r <- py_to_r(r2_scores)
  
  mse_mean <- -mean(mse_scores_r) 
  mse_std <- sd(mse_scores_r)
  rmse_mean <- sqrt(mse_mean)  
  rmse_std <- sqrt(mse_std)
  mae_mean <- -mean(mae_scores_r)
  mae_std <- sd(mae_scores_r)
  r2_mean <- mean(r2_scores_r)
  r2_std <- sd(r2_scores_r)
  
  cat("SVR model with nested CV, best C =", most_common_c, "\n")
  cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
  cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
  cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
  cat("R² Score:", r2_mean, "±", r2_std, "\n")
  cat("\n")
  
  # Saving performance metrics for this run
  all_performance[[repeat_loop]] <- data.frame(
    Run = repeat_loop,
    MSE = mse_mean,
    RMSE = rmse_mean,
    MAE = mae_mean,
    R2 = r2_mean,
    C = most_common_c
  )
  
  model11d_results <- rbind(model11d_results, data.frame(
    Model = "11d",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean,
    MSE_std = mse_std,
    RMSE_mean = rmse_mean,
    RMSE_std = rmse_std,
    MAE_mean = mae_mean,
    MAE_std = mae_std,
    R2_mean = r2_mean,
    R2_std = r2_std
  ))
  
  
  model11d_results_normalized <- rbind(model11d_results_normalized, data.frame(
    Model = "11d",
    Method = paste0("SVR_nested_CV repeat: ", repeat_loop),
    Outcome = "FR_totaal.2",
    Hyperparameter_value = paste0("C = ", most_common_c),
    MSE_mean = mse_mean / mse_mean_baseline,
    MSE_std = mse_std / mse_mean_baseline,
    RMSE_mean = rmse_mean / rmse_mean_baseline,
    RMSE_std = rmse_std / rmse_mean_baseline,
    MAE_mean = mae_mean / mae_mean_baseline,
    MAE_std =  mae_std / mae_mean_baseline,
    R2_mean = r2_mean / r2_mean_baseline,
    R2_std = r2_std / r2_mean_baseline
  ))
  
  final_svr$fit(X_rep, y_rep)
  final_y_pred <- final_svr$predict(X_rep)
  
  coefficients <- final_svr$coef_
  intercept <- final_svr$intercept_
  
  coef_vector <- py_to_r(coefficients)[1, ]  # flatten the 2D array to a 1D vector
  intercept <- py_to_r(intercept)[1]
  
  # saving coefficients for this run
  all_coefficients[[repeat_loop]] <- coef_vector
  
  feature_names <- colnames(X)
  coef_df <- data.frame(
    Feature = feature_names,
    Coefficient = coef_vector
  )
  
  coef_df <- coef_df %>% arrange(desc(abs(Coefficient)))
  
  top_n <- 20
  print(
  ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = paste("Predictor weights from model 11d (run", repeat_loop, ")"),
         x = "Predictor", y = "Coefficient") +
    theme_minimal()
  )
  
  cat("\nTop coefficients for run", repeat_loop, ":\n")
  print(head(coef_df, top_n))
  
  results_df <- data.frame(
    Actual = y_rep,
    Predicted = final_y_pred
  )
  
  final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
    geom_point(color = 'blue', alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
    labs(
      title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
      x = "Actual Outcome",
      y = "Predicted Outcome"
    ) +
    theme_minimal()
  
  print(final_plot)
  
  cat("Most common C for run", repeat_loop, ":", most_common_c, "\n")
}

# Calculating mean and SD of performance metrics
performance_df <- do.call(rbind, all_performance)
mean_performance <- colMeans(performance_df[, c("MSE", "RMSE", "MAE", "R2")])
sd_performance <- apply(performance_df[, c("MSE", "RMSE", "MAE", "R2")], 2, sd)

cat("MSE:", mean_performance["MSE"], "±", sd_performance["MSE"], "\n")
cat("RMSE:", mean_performance["RMSE"], "±", sd_performance["RMSE"], "\n")
cat("MAE:", mean_performance["MAE"], "±", sd_performance["MAE"], "\n")
cat("R²:", mean_performance["R2"], "±", sd_performance["R2"], "\n")
cat("Most common C value:", names(sort(table(performance_df$C), decreasing = TRUE)[1]), "\n")

model11d_results <- rbind(model11d_results, data.frame(
  Model = "11d",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"],
  MSE_std = sd_performance["MSE"],
  RMSE_mean = mean_performance["RMSE"],
  RMSE_std = sd_performance["RMSE"],
  MAE_mean = mean_performance["MAE"],
  MAE_std = sd_performance["MAE"],
  R2_mean = mean_performance["R2"],
  R2_std = sd_performance["R2"]
))


model11d_results_normalized <- rbind(model11d_results_normalized, data.frame(
  Model = "11d",
  Method = paste0("SVR_nested_CV final"),
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mean_performance["MSE"] / mse_mean_baseline,
  MSE_std = sd_performance["MSE"] / mse_mean_baseline,
  RMSE_mean = mean_performance["RMSE"] / rmse_mean_baseline,
  RMSE_std = sd_performance["RMSE"] / rmse_mean_baseline,
  MAE_mean = mean_performance["MAE"] / mae_mean_baseline,
  MAE_std =  sd_performance["MAE"] / mae_mean_baseline,
  R2_mean = mean_performance["R2"] / r2_mean_baseline,
  R2_std = sd_performance["R2"] / r2_mean_baseline
))

coef_matrix <- do.call(cbind, all_coefficients)
coef_means <- rowMeans(coef_matrix)
coef_sds <- apply(coef_matrix, 1, sd)

coef_summary <- data.frame(
  Feature = feature_names,
  Mean_Coefficient = coef_means,
  SD_Coefficient = coef_sds
)

coef_summary <- coef_summary %>% arrange(desc(abs(Mean_Coefficient)))

coef_long <- data.frame()
for (i in 1:length(all_coefficients)) {
  temp_df <- data.frame(
    Feature = feature_names,
    Coefficient = all_coefficients[[i]],
    Run = paste("Run", i)
  )
  coef_long <- rbind(coef_long, temp_df)
}

top_n <- 20
top_features <- head(coef_summary, top_n)$Feature

coef_long_top <- coef_long %>% filter(Feature %in% top_features)

# Creating a plot with mean ± SD
mean_sd_plot <- ggplot(head(coef_summary, top_n), 
                     aes(x = reorder(Feature, Mean_Coefficient), y = Mean_Coefficient)) +
  geom_col(fill = "steelblue") +
  geom_errorbar(aes(ymin = Mean_Coefficient - SD_Coefficient, 
                  ymax = Mean_Coefficient + SD_Coefficient), width = 0.2) +
  coord_flip() +
  labs(title = "Mean predictor weights across 5 runs",
       subtitle = paste("Error bars show ± 1 SD"),
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

print(mean_sd_plot)

# Creating a plot showing individual points for each run
individual_plot <- ggplot(coef_long_top, 
                        aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient, color = Run)) +
  geom_point(size = 3, position = position_dodge(width = 0.5)) +
  coord_flip() +
  labs(title = "Individual predictor weights from all 5 runs",
       x = "Predictor", y = "Coefficient") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(individual_plot)

# Creating a plot showing performance metrics across runs
performance_long <- performance_df %>%
  pivot_longer(cols = c("MSE", "RMSE", "MAE", "R2"), 
               names_to = "Metric", values_to = "Value")

performance_plot <- ggplot(performance_long, 
                         aes(x = as.factor(Run), y = Value, group = Metric, color = Metric)) +
  geom_line() +
  geom_point(size = 3) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(title = "Model performance across 5 runs",
       x = "Run", y = "Value") +
  theme_minimal()

print(performance_plot)

# Summary table of coefficients
top_coef_summary <- head(coef_summary, top_n)
print(top_coef_summary)
```

#### Feature selection for model 11d
```{r}
RFE <- sklearn$feature_selection$RFE

n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results_rfe <- data.frame()
best_params_list_rfe <- list()
outer_scores_rfe <- c()

c_values <- c(0.01, 0.1, 1, 10, 30)

for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]

  n_train_samples <- length(train_idx)
  set.seed(42 + outer_fold)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  best_features_mask <- NULL
  
  for (c_val in c_values) {
    cv_scores <- c()
    
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]

      base_model <- SVR(C = c_val, kernel = 'linear')
      rfe_model <- RFE(estimator = base_model, n_features_to_select = 5L)
      rfe_model$fit(X_inner_train, y_inner_train)

      selected_mask <- rfe_model$support_
      selected_indices <- which(py_to_r(selected_mask))
      
      X_inner_train_selected <- X_inner_train[, selected_mask, drop=FALSE]
      X_inner_val_selected <- X_inner_val[, selected_mask, drop=FALSE]

      model_selected <- SVR(C = c_val, kernel = 'linear')
      model_selected$fit(X_inner_train_selected, y_inner_train)
      y_pred_val <- model_selected$predict(X_inner_val_selected)
      
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_val)
      cv_scores <- c(cv_scores, neg_mse)
    }

    mean_score <- mean(cv_scores)
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
      # Store best mask based on entire training data
      final_rfe <- RFE(estimator = SVR(C = c_val, kernel = 'linear'), n_features_to_select = 5L)
      final_rfe$fit(X_train_outer, y_train_outer)
      best_features_mask <- final_rfe$support_
    }
  }

  best_params_list_rfe[[outer_fold]] <- list(C = best_c, mask = py_to_r(best_features_mask))
  cat("  Best C for fold", outer_fold, ":", best_c, "\n")

  X_train_outer_selected <- X_train_outer[, best_features_mask, drop=FALSE]
  X_test_outer_selected <- X_test_outer[, best_features_mask, drop=FALSE]

  final_model <- SVR(C = best_c, kernel = 'linear')
  final_model$fit(X_train_outer_selected, y_train_outer)
  y_pred_outer <- final_model$predict(X_test_outer_selected)

  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)

  outer_scores_rfe <- c(outer_scores_rfe, r2)

  nested_cv_results_rfe <- rbind(nested_cv_results_rfe, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results with RFE:\n")
print(nested_cv_results_rfe)
cat("\nMean R² across folds:", mean(outer_scores_rfe), "±", sd(outer_scores_rfe), "\n")

cat("\nBest Parameters and Features per Fold:\n")
for (i in 1:length(best_params_list_rfe)) {
  cat("Fold", i, ": C =", best_params_list_rfe[[i]]$C, "\n")
  print(colnames(X)[best_params_list_rfe[[i]]$mask])
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# Apply RFE to full dataset using best C
final_rfe <- RFE(estimator = SVR(C = most_common_c, kernel = 'linear'), n_features_to_select = 5L)
final_rfe$fit(X_scaledSVM, y)
selected_mask_final <- final_rfe$support_

# Subset data to selected features
X_selected_final <- X_scaledSVM[, selected_mask_final, drop=FALSE]

# Train final model with selected features
final_svr <- SVR(C = most_common_c, kernel = 'linear')
final_svr$fit(X_selected_final, y)

# Use X_selected_final (only features chosen by RFE) for cross-validation
mse_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_selected_final, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model11d_results <- rbind(model11d_results, data.frame(
  Model = "11d",
  Method = "Feature_Elimination",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model11d_results_normalized <- rbind(model11d_results_normalized, data.frame(
  Model = "11d",
  Method = "Feature_Elimination",
  Outcome = "FR_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


# Don't refit - use the already fitted model on selected features
final_y_pred <- final_svr$predict(X_selected_final)

# Get coefficients
coefficients <- final_svr$coef_
intercept <- final_svr$intercept_

# Use feature names of selected features only
feature_names <- colnames(X)[selected_mask_final]
coef_vector <- py_to_r(coefficients)[1, ]

coef_df <- data.frame(
  Feature = feature_names,
  Coefficient = coef_vector
)

print(coef_df)

top_n <- 20
ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Predictor weights from feature model 11d (with feature elimination)",
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)

```









### model 12: outcome Mansa with set of predictors + subcategories mansa
```{r}
# for this model I choose the following predictors: Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, mansa_totaal.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1 opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, MANSA_PH_1.1, MANSA_PH_2.1, MANSA_PH_3.1, MANSA_PH_4.1, MANSA_PH_5.1, MANSA_PH_6.1, MANSA_PH_7.1, MANSA_PH_8.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, MANSA_PH_12.1, MANSA_PH_12.1, MANSA_PH_13.1, MANSA_PH_14.1, MANSA_PH_15.1. 
# and the following outcome: mansa_totaal.2
# I filter the data to only include non-missing values of outcome mansa_totaal.2 and to only include patients with a follow up timepoint between 9 and 15 months after their first timepoint. For this i make a variable that filters on time from first timepoint:

dataModel12 <- dataNIEUW %>% select(Age, geslacht_GegevensAfname, modusmeanGGZ, levenspartner.1, betaaldwerk.1, honos_totaal.1, Inspire_totaal.1, FR_totaal.1, MANSA_PH_1.1, MANSA_PH_2.1, MANSA_PH_3.1, MANSA_PH_4.1, MANSA_PH_5.1, MANSA_PH_6.1, MANSA_PH_7.1, MANSA_PH_8.1, MANSA_PH_9.1, MANSA_PH_10.1, MANSA_PH_11.1, MANSA_PH_12.1, MANSA_PH_13.1, MANSA_PH_14.1, MANSA_PH_15.1, mansa_totaal.2, opleiding_nieuw, burgerlijkestaat_nieuw, leefsituatie_nieuw, month_diff_1_and_2) %>% filter((month_diff_1_and_2 >= 9 & month_diff_1_and_2 <= 15)) %>% filter(!is.na(mansa_totaal.2)) 

```

```{r}
# the following is to keep track of the metrics 
model12_results <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)

model12_results_normalized <- data.frame(
  Model = character(),
  Method = character(),
  Outcome = character(),
  Hyperparameter_value = character(),
  MSE_mean = numeric(),
  MSE_std = numeric(),
  RMSE_mean = numeric(),
  RMSE_std = numeric(),
  MAE_mean = numeric(),
  MAE_std = numeric(),
  R2_mean = numeric(),
  R2_std = numeric(),
  stringsAsFactors = FALSE
)
```

#### visualizing missing data 

```{r}
missing_percentageM12 <- colSums(is.na(dataModel12)) / nrow(dataModel12) * 100
print(missing_percentageM12)
```

```{r}
# better visualization of missing data 
total_missing <- sum(is.na(dataModel12))
cat("Total missing values: ", total_missing, "\n")

md.pattern(dataModel12)
```

#### imputing data 
```{r}
methods <- make.method(dataModel12)
# variables behind "#" have no missing values here 
# methods["Age"] <- "pmm"  # numeric 
# methods["geslacht_GegevensAfname"] <- "logreg"  # binary 
# methods["geslacht_Socio"] <- "logreg"  # binary 
# methods["Leeftijd1ePsyKl_b.1"] <- "pmm"  # numeric 
# methods["modusmeanPsyKl"] <- "pmm" # numeric 

methods["levenspartner.1"] <- "logreg" # binary 
methods["betaaldwerk.1"] <- "logreg" # binary 
methods["honos_totaal.1"] <- "pmm" # numeric 
methods["FR_totaal.1"] <- "pmm" # numeric 
methods["Inspire_totaal.1"] <- "pmm" # numeric 
methods["opleiding_nieuw"] <- "pmm" # numeric 
methods["leefsituatie_nieuw"] <- "logreg" # binary 
methods["MANSA_PH_7.1"] <- "logreg" # binary 
methods["MANSA_PH_9.1"] <- "logreg" # binary 
methods["MANSA_PH_10.1"] <- "logreg" # binary 
methods["MANSA_PH_11.1"] <- "logreg" # binary 
# methods["MANSA_PH_1.1"] <- "pmm" # numeric
# methods["MANSA_PH_2.1"] <- "pmm" # numeric
methods["MANSA_PH_3.1"] <- "pmm" # numeric
methods["MANSA_PH_4.1"] <- "pmm" # numeric
methods["MANSA_PH_5.1"] <- "pmm" # numeric
methods["MANSA_PH_6.1"] <- "pmm" # numeric
methods["MANSA_PH_8.1"] <- "pmm" # numeric
methods["MANSA_PH_12.1"] <- "pmm" # numeric
methods["MANSA_PH_13.1"] <- "pmm" # numeric
methods["MANSA_PH_14.1"] <- "pmm" # numeric
methods["MANSA_PH_15.1"] <- "pmm" # numeric
methods["month_diff_1_and_2"] <- ""
methods["mansa_totaal.2"] <- ""


# doing the imputation
imputed_dataModel12 <- mice(dataModel12, m=5, maxit=50, meth = methods, seed=42, printFlag = FALSE)
```


```{r}
summary(imputed_dataModel12)
```

```{r}
dataModel12_complete <- complete(imputed_dataModel12,1)
```

#### Selecting features and outcome 
```{r}
X <- dataModel12_complete[, c("Age", "geslacht_GegevensAfname", "modusmeanGGZ", "levenspartner.1", "betaaldwerk.1", "honos_totaal.1", "FR_totaal.1", "Inspire_totaal.1", "opleiding_nieuw", "burgerlijkestaat_nieuw", "leefsituatie_nieuw", "MANSA_PH_7.1", "MANSA_PH_9.1", "MANSA_PH_10.1", "MANSA_PH_11.1", "MANSA_PH_1.1", "MANSA_PH_2.1", "MANSA_PH_3.1", "MANSA_PH_4.1", "MANSA_PH_5.1", "MANSA_PH_6.1", "MANSA_PH_8.1", "MANSA_PH_12.1", "MANSA_PH_13.1", "MANSA_PH_14.1", "MANSA_PH_15.1")]
y <- dataModel12_complete$mansa_totaal.2
# one-hot encoding 
X <- model.matrix(~., data=X)[, -1] 
```

#### Nested cross validation
```{r}
X_scaledSVM <- scalerSVM$fit_transform(X) 

n_samples <- nrow(X_scaledSVM)
n_outer_folds <- 5
n_inner_folds <- 5

set.seed(42)
outer_fold_indices <- sample(rep(1:n_outer_folds, length.out = n_samples))

nested_cv_results <- data.frame()
best_params_list <- list()
outer_scores <- c()

# parameter grid
c_values <- c(0.01, 0.1, 1, 10, 30)

# outer CV loop
for (outer_fold in 1:n_outer_folds) {
  cat("\nProcessing outer fold", outer_fold, "of", n_outer_folds, "\n")
  
  test_idx <- which(outer_fold_indices == outer_fold)
  train_idx <- which(outer_fold_indices != outer_fold)
  
  X_train_outer <- X_scaledSVM[train_idx, , drop=FALSE]
  y_train_outer <- y[train_idx]
  X_test_outer <- X_scaledSVM[test_idx, , drop=FALSE]
  y_test_outer <- y[test_idx]
  
  set.seed(42 + outer_fold)  
  n_train_samples <- length(train_idx)
  inner_fold_indices <- sample(rep(1:n_inner_folds, length.out = n_train_samples))
  
  best_c <- NULL
  best_score <- -Inf
  
  for (c_val in c_values) {
    cv_scores <- c()

    # inner loop 
    for (inner_fold in 1:n_inner_folds) {
      inner_val_idx <- which(inner_fold_indices == inner_fold)
      inner_train_idx <- which(inner_fold_indices != inner_fold)
      
      X_inner_train <- X_train_outer[inner_train_idx, , drop=FALSE]
      y_inner_train <- y_train_outer[inner_train_idx]
      X_inner_val <- X_train_outer[inner_val_idx, , drop=FALSE]
      y_inner_val <- y_train_outer[inner_val_idx]
      
      if (inner_fold == 1) {  
        cat("    Inner fold", inner_fold, "train X:", dim(X_inner_train)[1], 
            "rows, y:", length(y_inner_train), "elements\n")
        cat("    Inner fold", inner_fold, "val X:", dim(X_inner_val)[1], 
            "rows, y:", length(y_inner_val), "elements\n")
      }
      
      inner_svr <- SVR(C = c_val, kernel = 'linear')
      inner_svr$fit(X_inner_train, y_inner_train)
      
      y_pred_inner <- inner_svr$predict(X_inner_val)
      neg_mse <- -metrics$mean_squared_error(y_inner_val, y_pred_inner)
      cv_scores <- c(cv_scores, neg_mse)
    }
    
    mean_score <- mean(cv_scores)
    
    if (mean_score > best_score) {
      best_score <- mean_score
      best_c <- c_val
    }
  }
  
  cat("  Best C value for fold", outer_fold, ":", best_c, "\n")
  
  best_params_list[[outer_fold]] <- list(C = best_c, kernel = 'linear')
  
  best_svr <- SVR(C = best_c, kernel = 'linear')
  best_svr$fit(X_train_outer, y_train_outer)
  
  y_pred_outer <- best_svr$predict(X_test_outer)
  
  mse <- metrics$mean_squared_error(y_test_outer, y_pred_outer)
  rmse <- sqrt(mse)
  mae <- metrics$mean_absolute_error(y_test_outer, y_pred_outer)
  r2 <- metrics$r2_score(y_test_outer, y_pred_outer)
  
  outer_scores <- c(outer_scores, r2)
  
  nested_cv_results <- rbind(nested_cv_results, data.frame(
    Fold = outer_fold,
    Best_C = best_c,
    MSE = mse,
    RMSE = rmse,
    MAE = mae,
    R2 = r2
  ))
  
  cat("  Completed outer fold", outer_fold, "- Best C:", best_c, "- R²:", r2, "\n")
}

cat("\nNested CV Results:\n")
print(nested_cv_results)
cat("\nMean R² across folds:", mean(outer_scores), "±", sd(outer_scores), "\n")

cat("\nBest Parameters per Fold:\n")
for (i in 1:length(best_params_list)) {
  cat("Fold", i, ":", "C =", best_params_list[[i]]$C, "\n")
}

c_values <- sapply(best_params_list, function(x) x$C)
c_table <- table(c_values)
most_common_c <- as.numeric(names(c_table)[which.max(c_table)])

# after the nested CV is complete, we evaluate the final model with cross_val_score
rkf <- RepeatedKFold(n_splits=5L, n_repeats=5L, random_state=42L)

final_svr <- SVR(C = most_common_c, kernel = 'linear')

mse_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_squared_error')
mae_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='neg_mean_absolute_error')
r2_scores <- cross_val_score(final_svr, X_scaledSVM, y, cv=rkf, scoring='r2')

mse_scores_r <- py_to_r(mse_scores)
mae_scores_r <- py_to_r(mae_scores)
r2_scores_r <- py_to_r(r2_scores)

mse_mean <- -mean(mse_scores_r) 
mse_std <- sd(mse_scores_r)
rmse_mean <- sqrt(mse_mean)  
rmse_std <- sqrt(mse_std)
mae_mean <- -mean(mae_scores_r)
mae_std <- sd(mae_scores_r)
r2_mean <- mean(r2_scores_r)
r2_std <- sd(r2_scores_r)

cat("SVR model with nested CV, best C =", most_common_c, "\n")
cat("Mean Squared Error (MSE):", mse_mean, "±", mse_std, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_mean, "±", rmse_std, "\n")
cat("Mean Absolute Error (MAE):", mae_mean, "±", mae_std, "\n")
cat("R² Score:", r2_mean, "±", r2_std, "\n")
cat("\n")

model12_results <- rbind(model12_results, data.frame(
  Model = "12",
  Method = "SVR_nested_CV",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean,
  MSE_std = mse_std,
  RMSE_mean = rmse_mean,
  RMSE_std = rmse_std,
  MAE_mean = mae_mean,
  MAE_std = mae_std,
  R2_mean = r2_mean,
  R2_std = r2_std
))


model12_results_normalized <- rbind(model12_results_normalized, data.frame(
  Model = "12",
  Method = "SVR_nested_CV",
  Outcome = "mansa_totaal.2",
  Hyperparameter_value = paste0("C = ", most_common_c),
  MSE_mean = mse_mean / mse_mean_baseline,
  MSE_std = mse_std / mse_mean_baseline,
  RMSE_mean = rmse_mean / rmse_mean_baseline,
  RMSE_std = rmse_std / rmse_mean_baseline,
  MAE_mean = mae_mean / mae_mean_baseline,
  MAE_std =  mae_std / mae_mean_baseline,
  R2_mean = r2_mean / r2_mean_baseline,
  R2_std = r2_std / r2_mean_baseline
))


final_svr$fit(X_scaledSVM, y)
final_y_pred <- final_svr$predict(X_scaledSVM)

# coefficients
coefficients <- final_svr$coef_
intercept <- final_svr$intercept_

coef_vector <- py_to_r(coefficients)[1, ]  # flatten the 2D array to a 1D vector
intercept <- py_to_r(intercept)[1]

feature_names <- colnames(X)
coef_df <- data.frame(
  Feature = feature_names,
  Coefficient = coef_vector
)

# Sort by absolute coefficient magnitude
coef_df <- coef_df %>% arrange(desc(abs(Coefficient)))

print(coef_df)

top_n <- 35
ggplot(head(coef_df, top_n), aes(x = reorder(Feature, Coefficient), y = Coefficient)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Predictor weights from model 12",
       x = "Predictor", y = "Coefficient") +
  theme_minimal()

results_df <- data.frame(
  Actual = y,
  Predicted = final_y_pred
)

final_plot <- ggplot(results_df, aes(x = Actual, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(
    title = paste("Final SVR Model Predictions vs Actual Outcomes\nC =", most_common_c),
    x = "Actual Outcome",
    y = "Predicted Outcome"
  ) +
  theme_minimal()

print(final_plot)
```








## Results
### Fase 0: Model 1

### Fase 1: Testing with different outcomes (Models 2 - 5)
#### Model 2:
```{r}
# full table 
model2_results %>%
  kable(caption = "Model 2 Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row

model2_results_normalized %>%
  kable(caption = "Model 2 Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row


```


#### Model 3: 
```{r}
# full table 
model3_results %>%
  kable(caption = "Model 3 Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row

model3_results_normalized %>%
  kable(caption = "Model 3 Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row


```

#### Model 4: 
```{r}
# full table 
model4_results %>%
  kable(caption = "Model 4 Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row

model4_results_normalized %>%
  kable(caption = "Model 4 Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row


```


#### Model 5: 
```{r}
# full table 
model5_results %>%
  kable(caption = "Model 5 Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row

model5_results_normalized %>%
  kable(caption = "Model 5 Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row


```

#### Overview of all models 
```{r}
all_model_results <- rbind(
  model2_results,
  model3_results,
  model4_results,
  model5_results
)

all_model_results_normalized <- rbind(
  model2_results_normalized,
  model3_results_normalized,
  model4_results_normalized,
  model5_results_normalized
)

all_model_results %>%
  kable(caption = "all models Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

all_model_results_normalized %>%
  kable(caption = "all models Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

# small table (for each method only hyperparameter with lowest MSE)
best_models <- all_model_results %>%
  group_by(Model, Method, Outcome) %>%
  slice_min(MSE_mean, with_ties = FALSE) %>%
  ungroup()

best_models_normalized <- all_model_results_normalized %>%
  group_by(Model, Method, Outcome) %>%
  slice_min(MSE_mean, with_ties = FALSE) %>%
  ungroup()

best_models %>%
  kable(caption = "best models Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE)

best_models_normalized %>%
  kable(caption = "best models Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

```

### Fase 2: Feature selection 
hierin (of juist fase 3) kan ik misschien nog kijken naar andere kernels voor svr. Dan is de eerste fase om te kijken welke methode het beste is en dan is deze fase om m'n model van svm zo goed mogelijk te maken 

#### Model 6: 
```{r}
# full table 
model6_results %>%
  kable(caption = "Model 6 Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model6_results_normalized %>%
  kable(caption = "Model 6 Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 


```

#### Model 7: 
```{r}
# full table 
model7_results %>%
  kable(caption = "Model 7 Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model7_results_normalized %>%
  kable(caption = "Model 7 Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 


```


#### Model FS
```{r}
# full table 
modelFS_results %>%
  kable(caption = "Model FS Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

modelFS_results_normalized %>%
  kable(caption = "Model FS Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```


#### Overview of all models 
```{r}
all_model_results <- rbind(
  model6_results,
  model7_results
)

all_model_results_normalized <- rbind(
  model6_results_normalized,
  model7_results_normalized
)

all_model_results %>%
  kable(caption = "all models Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row

all_model_results_normalized %>%
  kable(caption = "all models Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row

# small table (for each method only hyperparameter with lowest MSE)
best_models <- all_model_results %>%
  group_by(Model, Method, Outcome) %>%
  slice_min(MSE_mean, with_ties = FALSE) %>%
  ungroup()

best_models_normalized <- all_model_results_normalized %>%
  group_by(Model, Method, Outcome) %>%
  slice_min(MSE_mean, with_ties = FALSE) %>%
  ungroup()

best_models %>%
  kable(caption = "best models Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row

best_models_normalized %>%
  kable(caption = "best models Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) # Bold the header row

```

### Fase 3: 
#### Model 8a: 
```{r}
# full table 
model8a_results %>%
  kable(caption = "Model 8a Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model8a_results_normalized %>%
  kable(caption = "Model 8a Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```



#### Model 8b: 
```{r}
# full table 
model8b_results %>%
  kable(caption = "Model 8b Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model8b_results_normalized %>%
  kable(caption = "Model 8b Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```



#### Model 8c: 
```{r}
# full table 
model8c_results %>%
  kable(caption = "Model 8c Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model8c_results_normalized %>%
  kable(caption = "Model 8c Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```



#### Model 8d: 
```{r}
# full table 
model8d_results %>%
  kable(caption = "Model 8d Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model8d_results_normalized %>%
  kable(caption = "Model 8d Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```



#### Model 9a: 
```{r}
# full table 
model9a_results %>%
  kable(caption = "Model 9a Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model9a_results_normalized %>%
  kable(caption = "Model 9a Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```

#### Model 9b: 
```{r}
# full table 
model9b_results %>%
  kable(caption = "Model 9b Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model9b_results_normalized %>%
  kable(caption = "Model 9b Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```

#### Model 9c: 
```{r}
# full table 
model9c_results %>%
  kable(caption = "Model 9c Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model9c_results_normalized %>%
  kable(caption = "Model 9c Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```

#### Model 9d: 
```{r}
# full table 
model9d_results %>%
  kable(caption = "Model 9d Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model9d_results_normalized %>%
  kable(caption = "Model 9d Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```

#### Model 10a: 
```{r}
# full table 
model10a_results %>%
  kable(caption = "Model 10a Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model10a_results_normalized %>%
  kable(caption = "Model 10a Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```


#### Model 10b: 
```{r}
# full table 
model10b_results %>%
  kable(caption = "Model 10b Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model10b_results_normalized %>%
  kable(caption = "Model 10b Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```


#### Model 10c: 
```{r}
# full table 
model10c_results %>%
  kable(caption = "Model 10c Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model10c_results_normalized %>%
  kable(caption = "Model 10c Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```


#### Model 10d: 
```{r}
# full table 
model10d_results %>%
  kable(caption = "Model 10d Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model10d_results_normalized %>%
  kable(caption = "Model 10d Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```

#### Model 11a: 
```{r}
# full table 
model11a_results %>%
  kable(caption = "Model 10a Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model11a_results_normalized %>%
  kable(caption = "Model 10a Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```


#### Model 11b: 
```{r}
# full table 
model11b_results %>%
  kable(caption = "Model 10a Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model11b_results_normalized %>%
  kable(caption = "Model 10a Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```


#### Model 11c: 
```{r}
# full table 
model11c_results %>%
  kable(caption = "Model 10a Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model11c_results_normalized %>%
  kable(caption = "Model 10a Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```


#### Model 11d: 
```{r}
# full table 
model11d_results %>%
  kable(caption = "Model 10a Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

model11d_results_normalized %>%
  kable(caption = "Model 10a Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```


#### Overview of all models 
```{r}
all_model_results <- rbind(
  model8a_results,
  model8b_results,
  model8c_results,
  model8d_results,
  model9a_results,
  model9b_results,
  model9c_results,
  model9d_results,
  model10a_results,
  model10b_results,
  model10c_results,
  model10d_results, 
  model11a_results, 
  model11b_results, 
  model11c_results, 
  model11d_results,
  model12_results
)

all_model_results_normalized <- rbind(
  model8a_results_normalized,
  model8b_results_normalized,
  model8c_results_normalized,
  model8d_results_normalized,
  model9a_results_normalized,
  model9b_results_normalized,
  model9c_results_normalized,
  model9d_results_normalized,
  model10a_results_normalized,
  model10b_results_normalized,
  model10c_results_normalized,
  model10d_results_normalized,
  model11a_results_normalized,
  model11b_results_normalized,
  model11c_results_normalized,
  model11d_results_normalized,
  model12_results_normalized
)

all_model_results %>%
  kable(caption = "all models Performance Metrics") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 

all_model_results_normalized %>%
  kable(caption = "all models Performance Metrics (normalized)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, bold = TRUE) 
```


#### Outcome specific overview 
```{r}
# Inspire Brief-O:
Inspire_results <- all_model_results %>%
  filter(Outcome == "Inspire_totaal.2") %>%
  select(Model, Method, MSE_mean, RMSE_mean, R2_mean)

Inspire_results %>%
  kable(caption = "all models with Inspire outcome") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(0, bold = TRUE)

# mansa: 
mansa_results <- all_model_results %>%
  filter(Outcome == "mansa_totaal.2") %>%
  select(Model, Method, MSE_mean, RMSE_mean, R2_mean)

mansa_results %>%
  kable(caption = "all models with mansa outcome") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(0, bold = TRUE)

# honos:
honos_results <- all_model_results %>%
  filter(Outcome == "honos_totaal.2") %>%
  select(Model, Method, MSE_mean, RMSE_mean, R2_mean)

honos_results %>%
  kable(caption = "all models with honos outcome") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(0, bold = TRUE)

# FR:
FR_results <- all_model_results %>%
  filter(Outcome == "FR_totaal.2") %>%
  select(Model, Method, MSE_mean, RMSE_mean, R2_mean)

FR_results %>%
  kable(caption = "all models with FR outcome") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  row_spec(0, bold = TRUE)

```

